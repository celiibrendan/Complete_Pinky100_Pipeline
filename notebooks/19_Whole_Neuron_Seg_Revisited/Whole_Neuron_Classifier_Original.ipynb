{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d836d9e00886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatajoint\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bpy'"
     ]
    }
   ],
   "source": [
    "import bpy\n",
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from mathutils import Vector\n",
    "from Auto_Proofreader_Tab_without_fast_label import create_local_colors , set_View, create_bounding_box\n",
    "import time\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "\n",
    "#---------------------------DATAJOINT SOLUTION WAY OF GETTING COLORS---------------------#\n",
    "\n",
    "def get_diffuse_colors_list():\n",
    "    diff_list = ta3.LabelKey2().fetch(\"blender_colors\")\n",
    "    diffuse_colors_list = []\n",
    "    for df in diff_list:\n",
    "        diffuse_colors_list.append(tuple(df))\n",
    "\n",
    "    return diffuse_colors_list\n",
    "\n",
    "def get_Bevel_Weights():\n",
    "    num_list = ta3.LabelKey2().fetch(\"numeric\")\n",
    "    num_list[1] = 0\n",
    "    final_bevel_weights = []\n",
    "    for n in num_list:\n",
    "        final_bevel_weights.append(np.round(float(n)/100,2))\n",
    "    return final_bevel_weights\n",
    "\n",
    "def getLabels():\n",
    "    description_list = ta3.LabelKey2().fetch(\"description\")\n",
    "    description_list[0] = \"None\"\n",
    "    description_list[1] = \"None\"\n",
    "    final_description_list = []\n",
    "    for n in description_list:\n",
    "        final_description_list.append(n)\n",
    "    return final_description_list\n",
    "\n",
    "def getColors():\n",
    "    description_list,color_list = ta3.LabelKey2().fetch(\"description\",\"color\")\n",
    "    description_list = description_list.tolist()\n",
    "    color_list = color_list.tolist()\n",
    "    final_color_list = []\n",
    "    final_color_list.append(color_list.pop(0))\n",
    "    final_color_list.append(color_list.pop(0))\n",
    "    description_list.pop(0)\n",
    "    description_list.pop(0)\n",
    "    for i in range(0,len(description_list)):\n",
    "        final_color_list.append(description_list[i] + \" (\" + color_list[i] +\")\")\n",
    "    \n",
    "    return final_color_list\n",
    "\n",
    "def accepted_color_length():\n",
    "    numeric_list = ta3.LabelKey2().fetch(\"numeric\")\n",
    "    return len(numeric_list)\n",
    "\n",
    "#------------------------------------------------#\n",
    "\n",
    "\n",
    "def create_global_colors():  \n",
    "\n",
    "    \n",
    "    #get the list of colors already available in the bpy.data \n",
    "    list = bpy.data.materials.keys()  \n",
    "    \n",
    "    #adds all of the colors to the master list in blender file if they are not there already\n",
    "    colors = getColors()\n",
    "    #print(colors)\n",
    "    \"\"\"diffuse_colors_list = [(0.800, 0.800, 0.800),(0.800, 0.800, 0.800),\n",
    "    (0.0, 0.0, 0.800),(0.800, 0.800, 0.0), #blue, \"yellow\n",
    "    (0.0, 0.800, 0.0),(0.800, 0.0, 0.0),   #green, red\n",
    "    (0.0, 0.800, 0.527),(0.049, 0.458, 0.800),(0.200, 0.0, 0.800), #aqua, off blue, purple\n",
    "    (0.800, 0.0, 0.400),(0.250, 0.120, 0.059), #pink, brown\n",
    "    (0.800, 0.379, 0.232),(0.144,0.193,0.250),(0.800, 0.019, 0.093), #tan, soft, blue, rose\n",
    "    (0.800, 0.486, 0.459),(0.309,0.689,0.170),(0.800, 0.181, 0.013)] #light_pink, #light green orange\"\"\"\n",
    "    \n",
    "    diffuse_colors_list = get_diffuse_colors_list()\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(colors)):\n",
    "        if not(colors[i] in list):\n",
    "            mat = bpy.data.materials.new(name=colors[i]);\n",
    "            mat.diffuse_color = diffuse_colors_list[i]\n",
    "        else:  #if it already exists make sure to set colors list right\n",
    "            bpy.data.materials[colors[i]].diffuse_color = diffuse_colors_list[i]\n",
    "\n",
    "def create_local_colors(ob=bpy.context.object):              \n",
    "    #make sure all of the global colors are set correctly\n",
    "    create_global_colors()\n",
    "    #get current object\n",
    "    #ob = bpy.context.object\n",
    "    \n",
    "    #get the colors list\n",
    "    colors = getColors()\n",
    "    \n",
    "    #makes sure that length of color list matches the number of labels/colrs needed + 1\n",
    "    if(ob.data != None):\n",
    "        difference = len(ob.data.materials) - len(colors)\n",
    "    else:\n",
    "        print(\"materials was none\")\n",
    "        difference = -len(colors)\n",
    "    \n",
    "    #if less than 6 colors already then add the spots there\n",
    "    if(difference < 0):\n",
    "        for i in range(0,-difference):\n",
    "            ob.data.materials.append(None)\n",
    "            \n",
    "    #print(len(ob.data.materials))\n",
    "    \n",
    "    #make sure the colors are in the correct order for the object\n",
    "    \n",
    "    for i in range(0,len(colors)):\n",
    "        ob.data.materials[i] = bpy.data.materials[colors[i]]\n",
    "\n",
    "\n",
    "\n",
    "def load_Neuron_automatic_spine(neuron_ID,decimation_ratio,clusters,smoothness):\n",
    "    ID = str(neuron_ID)\n",
    "    print(\"inside load Neuron\")\n",
    " \n",
    "    #neuron_data = ((mesh_Table & \"segment_ID=\"+ID).fetch(as_dict=True))[0]\n",
    "    primary_key = dict(segmentation=2,decimation_ratio=decimation_ratio)\n",
    "    neuron_data = ((ta3p100.CleansedMesh & primary_key & \"segment_ID=\"+str(ID)).fetch(as_dict=True))[0]\n",
    "\n",
    "\n",
    "    \n",
    "    verts = neuron_data['vertices'].astype(dtype=np.int32).tolist()\n",
    "    faces = neuron_data['triangles'].astype(dtype=np.uint32).tolist()\n",
    "    \n",
    "    mymesh = bpy.data.meshes.new(\"neuron-\"+ID + \"_\" + str(decimation_ratio) + \"_\" + str(clusters) + \"_\" + str(smoothness))\n",
    "    mymesh.from_pydata(verts, [], faces)\n",
    " \n",
    "    mymesh.update(calc_edges=True)\n",
    "    mymesh.calc_normals()\n",
    "\n",
    "    object = bpy.data.objects.new(\"neuron-\"+ID + \"_\" + str(decimation_ratio) + \"_\" + str(clusters) + \"_\" + str(smoothness), mymesh)\n",
    "    #object.location = bpy.context.scene.cursor_location\n",
    "    object.location = Vector((0,0,0))\n",
    "    bpy.context.scene.objects.link(object)\n",
    "    \n",
    "    object.lock_location[0] = True\n",
    "    object.lock_location[1] = True\n",
    "    object.lock_location[2] = True\n",
    "    object.lock_scale[0] = True\n",
    "    object.lock_scale[1] = True\n",
    "    object.lock_scale[2] = True\n",
    "\n",
    "    object.rotation_euler[0] = 4.7124\n",
    "    object.rotation_euler[1] = 0\n",
    "    object.rotation_euler[2] = 0\n",
    "\n",
    "    bpy.ops.object.select_all(action='SELECT')\n",
    "    bpy.ops.object.transform_apply(rotation=True)\n",
    "    bpy.ops.object.select_all(action='DESELECT')\n",
    "    \n",
    "\n",
    "    object.lock_rotation[0] = True\n",
    "    object.lock_rotation[1] = True\n",
    "    object.lock_rotation[2] = True\n",
    "\n",
    "\n",
    "    #set view back to normal:\n",
    "    set_View()\n",
    "\n",
    "    #run the setup color command\n",
    "    #bpy.ops.object.select_all(action='TOGGLE')\n",
    "    \n",
    "    #create_local_colors(object)\n",
    "\n",
    "    #make sure in solid mode\n",
    "    for area in bpy.context.screen.areas: # iterate through areas in current screen\n",
    "        if area.type == 'VIEW_3D':\n",
    "            for space in area.spaces: # iterate through spaces in current VIEW_3D area\n",
    "                if space.type == 'VIEW_3D': # check if space is a 3D view\n",
    "                    space.viewport_shade = 'SOLID' # set the viewport shading to rendered\n",
    "    \n",
    "    return object.name\n",
    "\n",
    "def select_Neuron():\n",
    "    # deselect all\n",
    "    bpy.ops.object.select_all(action='DESELECT')\n",
    "\n",
    "    # selection\n",
    "    for obj in bpy.data.objects:\n",
    "        if \"neuron\" in obj.name:\n",
    "            obj.select = True\n",
    "            bpy.context.scene.objects.active = obj\n",
    "            print(\"object was found and active\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "import random\n",
    "\n",
    "def get_random_color():\n",
    "    ''' generate rgb using a list comprehension '''\n",
    "    r, g, b = [round(random.random(),3) for i in range(3)]\n",
    "    return (r, g, b)\n",
    "\n",
    "\n",
    "def segment_random_colors(ob,unique_segments,extraFlag):\n",
    "    errorFlag = 0\n",
    "    color_keys = bpy.data.materials.keys()\n",
    "    '''for i in range(0,len(bpy.data.materials.keys())-16):\n",
    "        print(\" material \" + color_keys[i])\n",
    "        bpy.data.materials.remove(bpy.data.materials[color_keys[i]])'''\n",
    "    \n",
    "    #makes sure that nothing but the accepted colors are there    \n",
    "    accepted_colors = getColors()\n",
    "    #print(\"inside segment_random_colors and getColors() = \" + str(getColors()))\n",
    "    \n",
    "    #ob.user_clear()\n",
    "    \n",
    "    for i in range(0,len(bpy.data.materials.keys())):\n",
    "        if(color_keys[i] not in accepted_colors):\n",
    "            #print(\"deleting material \" + color_keys[i])\n",
    "            bpy.data.materials[color_keys[i]].user_clear()\n",
    "            bpy.data.materials.remove(bpy.data.materials[color_keys[i]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    if errorFlag == 1:\n",
    "        for i in range(0,len(unique_segments)-1):\n",
    "            #add the new color\n",
    "            mat = bpy.data.materials.new(name=str(i));\n",
    "            mat.diffuse_color = get_random_color()\n",
    "            #assign it to the object\n",
    "            ob.data.materials.append(mat)\n",
    "    \n",
    "        #add one more label so that the unlabelables get a label\n",
    "        mat = bpy.data.materials.new(\"errors\")\n",
    "        mat.diffuse_color = get_random_color()\n",
    "        ob.data.materials.append(mat)\n",
    "    else:\n",
    "        \"\"\"mat = bpy.data.materials.new(name=str(\"base\"))\n",
    "        mat.diffuse_color = [0.800,0.800,0.800]\n",
    "        if(len(ob.data.materials.keys()) > 0):\n",
    "            ob.data.materials[0] = mat\n",
    "        else:\n",
    "            ob.data.materials.append(mat)\"\"\"\n",
    "            \n",
    "        for i in range(0,len(unique_segments)):\n",
    "            #add the new color\n",
    "            mat = bpy.data.materials.new(name=str(i));\n",
    "            mat.diffuse_color = get_random_color()\n",
    "            #assign it to the object\n",
    "            ob.data.materials.append(mat)\n",
    "        if extraFlag == True:\n",
    "            mat = bpy.data.materials.new(name=str(len(unique_segments)+1));\n",
    "            mat.diffuse_color = get_random_color()\n",
    "            #assign it to the object\n",
    "            ob.data.materials.append(mat)\n",
    "\n",
    "def get_cgal_data_and_label(neuron_ID, decimation_ratio, clusters,smoothness):\n",
    "       \n",
    "    #store the group_segmentation in the traingle labels from datajoint\n",
    "    \n",
    "    \n",
    "    #clusters=clusters,smoothness=smoothness\n",
    "    comp_dict = dict(segmentation=2,\n",
    "                            segment_id=neuron_ID,\n",
    "                            decimation_ratio=decimation_ratio,\n",
    "                            clusters=clusters,\n",
    "                            smoothness=smoothness\n",
    "                            )\n",
    "                            \n",
    "    component_data = (ta3p100.ComponentAutoSegmentWhole() & [comp_dict]).fetch(as_dict=True)[0]\n",
    "    triangles_labels = component_data[\"seg_group\"].tolist()\n",
    "    #activate the current object\n",
    "    select_Neuron()\n",
    "    ob = bpy.context.object\n",
    "    \n",
    "    \n",
    "    me = ob.data\n",
    "    \n",
    "    #print(\"starting to hide everything\")\n",
    "    #iterate through all of the vertices\n",
    "    verts_raw = ob.data.vertices\n",
    "    #print(len(active_verts_raw))\n",
    "    \n",
    "    edges_raw = ob.data.edges\n",
    "    \n",
    "    #print(len(active_edges_raw))\n",
    "    \n",
    "    faces_raw = ob.data.polygons\n",
    "    \n",
    "    #gets a list of the unique labels\n",
    "    unique_segments = list(Counter(triangles_labels).keys())\n",
    "    \n",
    "    \n",
    "    segmentation_length = len(unique_segments) # equals to list(set(words))\n",
    "    #print(segmentation_length)\n",
    "\n",
    "    #makes a dictionary that maps the unique segments to a number from range(0,len(unique_seg))\n",
    "    unique_index_dict = {unique_segments[x]:x for x in range(0,segmentation_length)}\n",
    "    \n",
    "    \n",
    "    #print(\"unique_index_dict = \" + str(len(unique_index_dict)))\n",
    "    #print(\"triangle_labels = \" + str(len(triangles_labels)))\n",
    "    #adds all of the labels to the faces\n",
    "    max_length = len(triangles_labels)\n",
    "    \n",
    "    #just iterate and add them to the faces\n",
    "    #here is where need to get stats for sdf numbers\n",
    "    \n",
    "    \n",
    "    labels_list = []\n",
    "    labels_list_index = []\n",
    "    for i,k in enumerate(faces_raw):\n",
    "        #gives the material id for that face\n",
    "        k.material_index = int(unique_index_dict[triangles_labels[i]]) \n",
    "        #assembles the label list that represents all of the faces\n",
    "        labels_list.append(str(unique_index_dict[triangles_labels[i]])) \n",
    "        #labels_list_index.append(\n",
    "    \n",
    "    select_Neuron()\n",
    "    \n",
    "    \n",
    "    #make sure in solid mode\n",
    "    for area in bpy.context.screen.areas: # iterate through areas in current screen\n",
    "        if area.type == 'VIEW_3D':\n",
    "            for space in area.spaces: # iterate through spaces in current VIEW_3D area\n",
    "                if space.type == 'VIEW_3D': # check if space is a 3D view\n",
    "                    space.viewport_shade = 'SOLID' # set the viewport shading to rendered\n",
    "    \n",
    "    bpy.ops.object.mode_set(mode='OBJECT')\n",
    "    \n",
    "\n",
    "\n",
    "    ######--------setting up the colors -------------###########\n",
    "    ###assign the color to the object\n",
    "    current_Material_length = len(bpy.data.materials)\n",
    "    \n",
    "    #remove all of the previous numbered colors\n",
    "    color_keys = bpy.data.materials.keys()\n",
    "\n",
    "    \n",
    "    #generate new colors for the bpy.data global\n",
    "\n",
    "    segment_random_colors(ob=ob,unique_segments=unique_segments,extraFlag=False)\n",
    "    \n",
    "    #these variables are set in order to keep the functions the same as FINAL_importing_auto_seg.py\n",
    "    newname = ob.name\n",
    "    print(\"done with cgal_segmentation\")\n",
    "    \n",
    "    #----------------------now return a dictionary of the sdf values like in the older function get_sdf_dictionary\n",
    "    #get the sdf values and store in sdf_labels\n",
    "    sdf_labels = component_data[\"sdf\"].tolist()\n",
    "        \n",
    "    sdf_temp_dict = {}\n",
    "    labels_seen = []\n",
    "    #iterate through the labels_list\n",
    "    for i,label in enumerate(labels_list):\n",
    "        if label not in labels_seen:\n",
    "            labels_seen.append(label)\n",
    "            sdf_temp_dict[label] = []\n",
    "        \n",
    "        sdf_temp_dict[label].append(sdf_labels[i])\n",
    "    #print(sdf_temp_dict)\n",
    "    \n",
    "    #now calculate the stats on the sdf values for each label\n",
    "    sdf_final_dict = {}\n",
    "    for key,value in sdf_temp_dict.items():\n",
    "        \"\"\"\n",
    "        #calculate the average\n",
    "        mean = np.mean(value)\n",
    "        #calculate the median\n",
    "        median = np.median(value)\n",
    "        #calculate the max\n",
    "        max = np.amax(value)\n",
    "        #calculate minimum\n",
    "        min = np.amin(value)\n",
    "        \n",
    "        temp_dict = {\"mean\":mean,\"median\":median,\"max\":max,\"min\":min}\n",
    "        \n",
    "        #assign them \n",
    "        sdf_final_dict[key] = temp_dict.copy()\n",
    "        \"\"\"\n",
    "        \n",
    "        #just want to store the median\n",
    "        sdf_final_dict[key] = dict(median=np.median(value),mean=np.mean(value),max=np.amax(value))\n",
    "\n",
    "    return sdf_final_dict, labels_list\n",
    "\n",
    "def get_highest_sdf_part(sdf_final_dict, labels_list,size_threshold=3000,exclude_label=None):\n",
    "    high_median_val = 0\n",
    "    high_median = -1\n",
    "    high_mean_val = 0\n",
    "    high_mean = -1\n",
    "    high_max_val = 0\n",
    "    high_max = -1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    my_list = Counter(labels_list)\n",
    "    my_list_keys = list(my_list.keys())\n",
    "    if exclude_label != None:\n",
    "        my_list_keys.remove(exclude_label)\n",
    "    \n",
    "    for x in my_list_keys:\n",
    "        #print(\"x = \" + str(x))\n",
    "        #print(\"high_median_val = \" + str(high_median_val))\n",
    "        #print('sdf_final_dict[x][\"median\"] = ' + str(sdf_final_dict[x][\"median\"]))\n",
    "        if sdf_final_dict[x][\"median\"] > high_median_val and my_list[x] > size_threshold:\n",
    "            high_median = x\n",
    "            high_median_val = sdf_final_dict[x][\"median\"]\n",
    "        if sdf_final_dict[x][\"mean\"] > high_mean_val  and my_list[x] > size_threshold:\n",
    "            high_mean = x\n",
    "            high_mean_val = sdf_final_dict[x][\"mean\"]\n",
    "        if sdf_final_dict[x][\"max\"] > high_max_val  and my_list[x] > size_threshold:\n",
    "            high_max = x\n",
    "            high_max_val = sdf_final_dict[x][\"max\"]\n",
    "    \n",
    "    \n",
    "    return high_median,high_median_val,high_mean,high_mean_val,high_max,high_max_val\n",
    "\n",
    "#export the neuron\n",
    "def export_neuron(ob_name,destination_folder=\"whole_neuron_testing\"):\n",
    "    \n",
    "    blend_file_path = bpy.data.filepath\n",
    "    directory = os.path.dirname(blend_file_path)\n",
    "    #destination_folder = \"whole_neuron_testing\"\n",
    "    final_directory = Path(directory) / destination_folder / (ob_name + \".obj\")\n",
    "    #target_file = os.path.join(final_directory, ob_name + \".obj\")\n",
    "\n",
    "    bpy.ops.export_scene.obj(filepath=final_directory.as_posix())\n",
    "    return\n",
    "    \n",
    "#def generate_label_list(path_to_check=\"\"):\n",
    "def generate_label_list():\n",
    "    \n",
    "    #have to go into object mode to do some editing\n",
    "    currentMode = bpy.context.object.mode\n",
    "\n",
    "    bpy.ops.object.mode_set(mode='OBJECT')\n",
    "    ob = bpy.context.object\n",
    "    ob.update_from_editmode()\n",
    "    \n",
    "    me = ob.data\n",
    "    faces_raw = me.polygons\n",
    "    verts_raw = me.vertices \n",
    "    \n",
    "    labels_list = [fac.material_index for fac in faces_raw]\n",
    "    return labels_list\n",
    "    \n",
    "    \"\"\"\n",
    "    if path_to_check != \"\":\n",
    "        try:\n",
    "            label_data = np.load(path_to_check + ob.name + \".npz\")\n",
    "        except:\n",
    "            print(\"having to generate the list myself\")\n",
    "            #have to generate the labels list manually\n",
    "            \n",
    "        else:\n",
    "            print(\"loading the labels list from a file\")\n",
    "            labels_list = label_data[\"labels_list\"]\n",
    "            return labels_list\n",
    "    \"\"\"\n",
    "       \n",
    "\n",
    "def rewrite_label(i,x,number_to_replace,verts_to_Face,faces_raw,verts_raw,big_labels,soma_index,cilia_threshold=300):\n",
    "    #won't reassign the same big label to all of those with the same little label\n",
    "    assign_All_Same = True\n",
    "   \n",
    "    #print(\"x[i] =\" + str(x[i]))\n",
    "    #print(len(x))\n",
    "    #print(x)\n",
    "    \n",
    "    remaining_Faces = []\n",
    "    \n",
    "    \"\"\"for i,j in enumerate(x):\n",
    "        if j == x[i]:\n",
    "            print(str(i) + \":\" + str(j))\n",
    "            #print(j)\n",
    "            remaining_Faces.append(i)\"\"\"\n",
    "            \n",
    "    remaining_Faces = [z for z,j in enumerate(x) if j == x[i]]\n",
    "    \n",
    "    \n",
    "    #print(\"remaining_Faces len = \" + str(len(remaining_Faces)))\n",
    "    #print(\"number to replace = \" + str(number_to_replace))\n",
    "    \n",
    "    if number_to_replace != len(remaining_Faces):\n",
    "        raise ValueError(\"ERROR: faces to replace don't match the number of indexes found for that face\")\n",
    "        return []\n",
    "    \n",
    "    #0) Initialize replacement label\n",
    "    #1) Find the face to relabel\n",
    "    #2) ADD THIS FACE TO THE CHECKED FACES\n",
    "    #2) FIND THE VERTICES ASSOCIATED WITH THIS FACE AND ADD TO VERTS TO CHECK\n",
    "    #2) START another while loop that only breaks when there has been a replacement label found\n",
    "    #4) For each vertices IN VERTS TO CHECK, find the faces associated with each, and add it to the FACES TO CHECK (IF IT HAS NOT ALREADY BEEN CHECKED)\n",
    "        #add the vertices to checked verts so we don't end up redoing them\n",
    "    #5) FOR each of the faces to check, find its label, and see if the label is in the big list\n",
    "    #6)     If is in big list --> save the replacement label and break from loop\n",
    "    #7)     If not --> add vertices associated with the face(that have not already been checked) to the verts to check list\n",
    "    #OUTSIDE OF #2 WHILE LOOP\n",
    "    #8) if the assign_All_Same flag\n",
    "    #           is set to true --> assign the label to all of the faces with the same label to be replaced and pop all of them\n",
    "    #           is false --> only assign the new label to the one being checked\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    while remaining_Faces:\n",
    "        counter = 0\n",
    "        replacement_label = \"k\"\n",
    "        face_to_relabel = remaining_Faces[0]\n",
    "        \n",
    "        #print(\"face_to_relabel = \" + str(face_to_relabel))\n",
    "        \n",
    "        checked_Faces = []\n",
    "        checked_Verts = []\n",
    "        verts_to_check = []\n",
    "        faces_to_check = []\n",
    "        \n",
    "        checked_Faces.append(face_to_relabel)\n",
    "        \n",
    "        #get the vertices of the face\n",
    "        face_vertices = faces_raw[face_to_relabel].vertices\n",
    "        for vertex in face_vertices:\n",
    "            verts_to_check.append(vertex)\n",
    "        \n",
    "        #loop that goes until replacement label is found\n",
    "        while replacement_label == \"k\":\n",
    "            counter = counter + 1\n",
    "            if counter > 1000:\n",
    "                print(\"didn't find big face yet, going through pass: \" + str(counter))\n",
    "            #iterates through each vertex to get all of the faces to check\n",
    "            for vertices_checking in verts_to_check:\n",
    "                #gets all of the faces associated with the vertex\n",
    "                faces_with_vertex = verts_to_Face[vertices_checking]\n",
    "                \n",
    "                #puts all the faces to check into a list\n",
    "                for fc in faces_with_vertex:\n",
    "                    if (fc not in faces_to_check) and (fc not in checked_Faces):\n",
    "                        faces_to_check.append(fc)\n",
    "                if(vertices_checking not in checked_Verts):\n",
    "                    checked_Verts.append(vertices_checking)\n",
    "            \n",
    "            verts_to_check = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            #iterate through all faces to find until a label from the big list is found\n",
    "            for face in faces_to_check:\n",
    "                if x[face] in big_labels:\n",
    "                    replacement_label = x[face]\n",
    "                    #print(\"found replacement label = \" + str(replacement_label))\n",
    "                    break\n",
    "                else:\n",
    "                    checked_Faces.append(face)\n",
    "                    #add their vertices to the needs to be checked list if they haven't already been added\n",
    "                    for vertex in faces_raw[face].vertices:\n",
    "                        if (vertex not in checked_Verts) and (vertex not in verts_to_check):\n",
    "                            verts_to_check.append(vertex)\n",
    "            \n",
    "            \n",
    "                    \n",
    "            \n",
    "            faces_to_check = []\n",
    "        #print(\" counter = \" + str(counter))\n",
    "        #print(\"replacement_label = \" + str(replacement_label))\n",
    "        counter = 0\n",
    "        #replace that faces label with the new label\n",
    "        if soma_index == \"-1\":\n",
    "            x[face_to_relabel] = replacement_label\n",
    "            remaining_Faces.pop(0)\n",
    "            \n",
    "            if assign_All_Same != False:\n",
    "                #print(\"assigning all the same\")\n",
    "                for faces in remaining_Faces:\n",
    "                    x[faces] = replacement_label   \n",
    "            \n",
    "                remaining_Faces = []\n",
    "        else:\n",
    "            if replacement_label == soma_index and number_to_replace > cilia_threshold:\n",
    "                #so don't do any replacing\n",
    "                remaining_Faces = []\n",
    "            else:\n",
    "                x[face_to_relabel] = replacement_label\n",
    "                remaining_Faces.pop(0)\n",
    "                \n",
    "                if assign_All_Same != False:\n",
    "                    #print(\"assigning all the same\")\n",
    "                    for faces in remaining_Faces:\n",
    "                        x[faces] = replacement_label   \n",
    "                \n",
    "                    remaining_Faces = []        \n",
    "    \n",
    "    #####need to add in a part that watches out for the soma so we don't smooth away the cilia\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    return x\n",
    "\n",
    "##x is the labels list\n",
    "##threshold is how big in size the labels need to be in order to qualify as a big label\n",
    "##number_Flag: will make it so the number of items left after the smoothing is only a certain number\n",
    "##seg_numbers: if the number_Flag is true, will make sure smoothing only leaves (seg_numbers) amount of colors at the end\n",
    "\n",
    "\n",
    "####can possibly change to make faster\n",
    "def merge_labels(x,ob_name,threshold=50,soma_index=-1,cilia_threshold=300,number_Flag = False, seg_numbers=1):\n",
    "    #make sure to select the correct base object\n",
    "    for obj in bpy.data.objects:\n",
    "        obj.select = False\n",
    "    \n",
    "    ob = bpy.data.objects[ob_name]\n",
    "    ob.select = True\n",
    "    bpy.context.scene.objects.active = ob\n",
    "    \n",
    "    #have to go into object mode to do some editing\n",
    "    currentMode = bpy.context.object.mode\n",
    "\n",
    "    bpy.ops.object.mode_set(mode='OBJECT')\n",
    "    ob = bpy.context.object\n",
    "    ob.update_from_editmode()\n",
    "    \n",
    "    me = ob.data\n",
    "    faces_raw = me.polygons\n",
    "    verts_raw = me.vertices\n",
    "    \n",
    "    #create a list of all the labels and which ones are the biggest ones\n",
    "    from collections import Counter\n",
    "    \n",
    "    myCounter = Counter(x)\n",
    "\n",
    "    big_labels = []\n",
    "    for label,times in myCounter.items():\n",
    "        if(times >= threshold):\n",
    "            #print(str(label) + \":\" + str(times))\n",
    "            big_labels.append([label,times])\n",
    "    \n",
    "    #big labels has the index and the number of times it occurs in a list together\n",
    "    big_labels.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    #At this point have all of the big labels we want to smooth our neuron to\n",
    "    print(\"BIG LABELS = \" + str(big_labels))\n",
    "    \n",
    "    #reduce the number of items in the list if the number_Flag is set\n",
    "    if(number_Flag == True):\n",
    "        big_labels = big_labels[:seg_numbers]\n",
    "        \n",
    "    \n",
    "    \n",
    "    #need to assemble a dictionary that relates vertices to faces\n",
    "    #*****making into a list if the speed is too slow*******#\n",
    "    \n",
    "    verts_to_Face = {}\n",
    "    \n",
    "    #initialize the lookup dictionary as empty lists\n",
    "    for pre_vertex in verts_raw:\n",
    "        verts_to_Face[pre_vertex.index] = []\n",
    "        \n",
    "    #print(len(verts_raw))\n",
    "    #print(len(verts_to_Face))\n",
    "    #print(verts_to_Face[1])\n",
    "    \n",
    "    for face in faces_raw:\n",
    "        #get the vertices\n",
    "        verts = face.vertices\n",
    "        #add the index to the list for each of the vertices\n",
    "        for vertex in verts:\n",
    "            verts_to_Face[vertex].append(face.index)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #just extracts the labels of the big labels (because previously had the labels and number of occurances packaged together)\n",
    "    big_labels_indexes = [x for x,i in big_labels]\n",
    "    print(\"big_labels_indexes=\"+str(big_labels_indexes))\n",
    "    \n",
    "    \n",
    "    #now need to change the labels\n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in big_labels_indexes:\n",
    "            #print(\"working on label \" + str(x[i]))\n",
    "            #print(\"myCounter = \" + str(myCounter))\n",
    "            number_to_replace = myCounter[x[i]]\n",
    "            #print(\"number_to_replace = \" + str(number_to_replace))\n",
    "            #i: the index of the label that we want to replace\n",
    "            #x: the entire list of labels\n",
    "            #number_to_replace: the number of labels with that specific label that we want to replace\n",
    "            #verts_to_Face: the lookup dictionary mapping vertices to faces\n",
    "            #faces_raw/verts_raw: pointers to faces of object\n",
    "            #big_labels_indexes: the list of possible big labels that we want to rewrite the smaller label as \n",
    "            x = rewrite_label(i,x,number_to_replace,verts_to_Face,faces_raw,verts_raw,big_labels_indexes,soma_index,cilia_threshold)\n",
    "    \n",
    "    for jj,fac in enumerate(faces_raw):\n",
    "        fac.material_index = ob.data.materials.keys().index(x[jj])\n",
    "    \n",
    "    #need to redo the colors of the object\n",
    "    from collections import Counter\n",
    "    \n",
    "    myCounter = Counter(x)\n",
    "\n",
    "    \n",
    "    big_labels = []\n",
    "    for label,times in myCounter.items():\n",
    "        if(times >= threshold):\n",
    "            #print(str(label) + \":\" + str(times))\n",
    "            big_labels.append([label,times])\n",
    "    \n",
    "    return x, verts_to_Face\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rewrite_label_vp2(labels_list,small_label,connections, mesh_Number,faces_raw,verts_raw,big_labels,soma_index,cilia_threshold=300):\n",
    "    #won't reassign the same big label to all of those with the same little label\n",
    "    assign_All_Same = True\n",
    "    \n",
    "    \n",
    "    replacement_label = \"k\"\n",
    "    neighbors_seen = [small_label]\n",
    "    neighbors_to_check = []\n",
    "    neighbors_checked = []\n",
    "    current_label = small_label\n",
    "    \n",
    "    #print(\"small_label in rewrite_label = \" + str(small_label))\n",
    "    \n",
    "    while replacement_label == \"k\":\n",
    "        #Get neighbors of current label using new function\n",
    "        current_neighbors = connections[current_label]\n",
    "        \n",
    "        neighbors_seen = list(set(neighbors_seen + current_neighbors))\n",
    "        for cn in current_neighbors:\n",
    "            if cn in big_labels:\n",
    "                replacement_label = cn\n",
    "                break\n",
    "        \n",
    "        if replacement_label != \"k\":\n",
    "            break\n",
    "        \n",
    "        \n",
    "        neighbors_checked.append(current_label)\n",
    "        neighbors_to_check = list(set(neighbors_to_check + [neigh for neigh in current_neighbors if neigh not in neighbors_checked]))\n",
    "        \n",
    "        current_label = neighbors_to_check.pop(0)\n",
    "    \n",
    "    if replacement_label == \"k\":\n",
    "        print(\"ERROR SOMETHING WENT WRONG AND NO REPLACEMENT LABEL FOUND\")\n",
    "    \n",
    "    new_ignore_list = []\n",
    "    #have the replacement label:\n",
    "    if soma_index == \"-1\":\n",
    "        for i,lab in enumerate(labels_list):\n",
    "            if lab in neighbors_seen and lab not in big_labels:\n",
    "                labels_list[i] = replacement_label\n",
    "    else:\n",
    "        #get a list of the neighbors sizes that are not part of big_labels\n",
    "        size_lengths_exceeds = [mesh_Number[labelz]>cilia_threshold for labelz in neighbors_seen if labelz not in big_labels]\n",
    "        \n",
    "        \n",
    "        if replacement_label == soma_index and True in size_lengths_exceeds:\n",
    "            #label them as cilia\n",
    "            print(\"found cilia in \" + str(neighbors_seen))\n",
    "            new_ignore_list = [lb for lb in neighbors_seen if lb not in big_labels]\n",
    "            \"\"\"for i,lab in enumerate(labels_list):\n",
    "                if lab in neighbors_seen and lab not in big_labels:\n",
    "                    labels_list[i] = \"Cilia\"\"\"\n",
    "        else:\n",
    "            for i,lab in enumerate(labels_list):\n",
    "                if lab in neighbors_seen and lab not in big_labels:\n",
    "                    labels_list[i] = replacement_label\n",
    "                    \n",
    "    return labels_list,new_ignore_list\n",
    "\n",
    "####can possibly change to make faster\n",
    "def merge_labels_vp2(labels_list,ob_name,threshold=50,soma_index=-1,cilia_threshold=80):\n",
    "    #make sure to select the correct base object\n",
    "    for obj in bpy.data.objects:\n",
    "        obj.select = False\n",
    "    \n",
    "    ob = bpy.data.objects[ob_name]\n",
    "    ob.select = True\n",
    "    bpy.context.scene.objects.active = ob\n",
    "    \n",
    "    #have to go into object mode to do some editing\n",
    "    bpy.ops.object.mode_set(mode='OBJECT')\n",
    "    ob.update_from_editmode()\n",
    "    \n",
    "    me = ob.data\n",
    "    faces_raw = me.polygons\n",
    "    verts_raw = me.vertices\n",
    "    \n",
    "    #create a list of all the labels and which ones are the biggest ones\n",
    "    from collections import Counter\n",
    "    \n",
    "    myCounter = Counter(labels_list)\n",
    "\n",
    "    big_labels = [label_name for label_name,times in myCounter.items() if times > threshold]\n",
    "    \n",
    "    #At this point have all of the big labels we want to smooth our neuron to\n",
    "    #print(\"BIG LABELS = \" + str(big_labels))\n",
    "   \n",
    "    #need to assemble a dictionary that relates vertices to faces\n",
    "    #*****making into a list if the speed is too slow*******#\n",
    "    verts_to_Face,verts_to_Label = generate_verts_to_face_dictionary(labels_list,faces_raw,verts_raw)\n",
    "    \n",
    "    ####get the list of connections\n",
    "    connections, mesh_Number = get_graph_structure(verts_to_Label,labels_list,faces_raw,verts_raw)\n",
    "    ignore_list = []\n",
    "    print(\"about to start merging\")\n",
    "    #now need to change the labels\n",
    "    for i in range(len(labels_list)):\n",
    "        if labels_list[i] not in big_labels and labels_list[i] not in ignore_list:\n",
    "            labels_list,new_ignore_list = rewrite_label_vp2(labels_list,labels_list[i],connections, mesh_Number,faces_raw,verts_raw,big_labels,soma_index,cilia_threshold)\n",
    "            ignore_list = ignore_list + new_ignore_list\n",
    "            \n",
    "    print(\"DONE merging\")\n",
    "    \"\"\"#need to add cilia to materials\n",
    "    cilia_color = (0.450,0.254,0.800)\n",
    "    mat = bpy.data.materials.new(name=\"Cilia\");\n",
    "    mat.diffuse_color = cilia_color\n",
    "    #assign it to the object\n",
    "    ob.data.materials.append(mat)\"\"\"\n",
    "    \n",
    "    for jj,fac in enumerate(faces_raw):\n",
    "        fac.material_index = ob.data.materials.keys().index(labels_list[jj])\n",
    "    \n",
    "    return labels_list,verts_to_Face\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def generate_verts_to_face_dictionary(labels_list,faces_raw,verts_raw):\n",
    "    verts_to_Face = {pre_vertex.index:[] for pre_vertex in verts_raw}\n",
    "    verts_to_Label = {pre_vertex.index:[] for pre_vertex in verts_raw}\n",
    "    \n",
    "    \n",
    "    for face in faces_raw:\n",
    "        #get the vertices\n",
    "        verts = face.vertices\n",
    "        #add the index to the list for each of the vertices\n",
    "        for vertex in verts:\n",
    "            verts_to_Face[vertex].append(face.index)\n",
    "    \n",
    "    #use the verts to face to create the verts to label dictionary\n",
    "    for vert,face_list in verts_to_Face.items():\n",
    "        diff_labels = [labels_list[fc] for fc in face_list]\n",
    "        #print(list(set(diff_labels)))\n",
    "        verts_to_Label[vert] = list(set(diff_labels))\n",
    "    \n",
    "        \n",
    "            \n",
    "    return verts_to_Face,verts_to_Label\n",
    "\n",
    "\n",
    "def get_graph_structure(verts_to_Label,labels_list,faces_raw,verts_raw):\n",
    "    connections = {label_name:[] for label_name in Counter(labels_list).keys()}\n",
    "    mesh_Number = {label_name:number for label_name,number in Counter(labels_list).items()}\n",
    "    #label_vert_stats = {label_name:[300000,-300000] for label_name in Counter(labels_list).keys()}\n",
    "    \n",
    "    for verts,total_labels in verts_to_Label.items():\n",
    "        if len(total_labels) > 1:\n",
    "            for face in total_labels:\n",
    "                for fc in [v for v in total_labels if v != face]:\n",
    "                    if fc not in connections[face]:\n",
    "                        connections[face].append(fc)\n",
    "        \"\"\"#get the verts stats:\n",
    "        real_vert = verts_raw[verts]\n",
    "        if real_vert.co[2] < label_vert_stats[verts][0]:\n",
    "            label_vert_stats[verts][0] = real_vert.co[2]\n",
    "        if real_vert.co[2] > label_vert_stats[verts][1]:\n",
    "            label_vert_stats[verts][1] = real_vert.co[2]\"\"\"\n",
    "                        \n",
    "    \n",
    "    return connections, mesh_Number#,label_vert_stats\n",
    "    \n",
    "def find_max_min_z_vals(neighbors_list,labels_list,faces_raw,verts_raw):\n",
    "    min_max = {ni:[300000,-300000] for ni in neighbors_list}\n",
    "    \n",
    "    for i,ll in enumerate(labels_list):\n",
    "        if ll in neighbors_list:\n",
    "            verts_from_faces = faces_raw[i].vertices\n",
    "            for vert in verts_from_faces:\n",
    "                real_vert = verts_raw[vert]\n",
    "                if real_vert.co[2] < min_max[ll][0]:\n",
    "                    min_max[ll][0] = real_vert.co[2]\n",
    "                if real_vert.co[2] > min_max[ll][1]:\n",
    "                    min_max[ll][1] = real_vert.co[2]\n",
    "    return min_max\n",
    "\n",
    "\n",
    "\"\"\"#stats found in research:\n",
    "number of faces = 44596\n",
    "min max of vertices = [-15.0, 20876.0]\n",
    "sdf median of Apical = 0.14563350000000003\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"pseudocode for find_Apical\n",
    "1) calculate the height of 70% up the soma\n",
    "2) find all the neighbors of the soma using verts_to_Label\n",
    "3) filter out the neighbors that go below that\n",
    "4) filter away the neighbors that don't meet minimum number of face, vertex change and sdf median\n",
    "5) If multiple, pick the one that has the most number of neighbors\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def find_Apical(min_max,connections,mesh_Number,soma_index,sdf_final_dict):\n",
    "    mesh_Threshold = 2000\n",
    "    height_Threshold =5000\n",
    "    sdf_Threshold = 0.09\n",
    "    #1) calculate the height of 70% up the soma\n",
    "    soma_70_percent = (min_max[soma_index][1] - min_max[soma_index][0])*0.7 +  min_max[soma_index][0]\n",
    "    #2) find all the neighbors of the soma using verts_to_Label\n",
    "    soma_neighbors = connections[soma_index]\n",
    "    #3) filter out the neighbors that go below that\n",
    "     \n",
    "    \n",
    "    possible_Axons_filter_1 = [label for label in soma_neighbors if min_max[label][0] > soma_70_percent]\n",
    "    \n",
    "    #4) filter away the neighbors that don't meet minimum number of face, vertex change and sdf median\n",
    "    print(\"possible_Axons_filter_1 = \" + str(possible_Axons_filter_1))\n",
    "    possible_Axons_filter_2 = [lab for lab in possible_Axons_filter_1 if \n",
    "                                    mesh_Number[lab] > mesh_Threshold and \n",
    "                                    (min_max[lab][1] - min_max[lab][0]) > height_Threshold and\n",
    "                                    sdf_final_dict[lab][\"median\"] > sdf_Threshold]\n",
    "    print(\"possible_Axons_filter_2 = \" + str(possible_Axons_filter_2))\n",
    "    if len(possible_Axons_filter_2) <= 0:\n",
    "        return \"None\"\n",
    "    elif len(possible_Axons_filter_2) == 1:\n",
    "        return possible_Axons_filter_2[0]\n",
    "    else:\n",
    "        #find the one with the most neighbors\n",
    "        current_apical = possible_Axons_filter_2[0]\n",
    "        current_apical_neighbors = len(connections[possible_Axons_filter_2[0]])\n",
    "        for i in range(1,len(possible_Axons_filter_2)):\n",
    "            if len(connections[possible_Axons_filter_2[i]]) > current_apical_neighbors:\n",
    "                current_apical = possible_Axons_filter_2[i]\n",
    "                current_apical_neighbors = len(connections[possible_Axons_filter_2[i]])\n",
    "        \n",
    "        return current_apical\n",
    "\n",
    "####For automatic spine labeling\n",
    "def find_endpoints(G,mesh_number):\n",
    "    #will first calculate all the shortest paths for each of the nodes\n",
    "    \n",
    "    node_list = list(G.nodes)\n",
    "    if(\"backbone\" in node_list):\n",
    "        node_list.remove(\"backbone\")\n",
    "    else:\n",
    "        return [],[] \n",
    "    \n",
    "    shortest_paths = {}\n",
    "    for node in node_list:\n",
    "        shortest_paths[node] = [k for k in nx.all_shortest_paths(G,node,\"backbone\")]\n",
    "    \n",
    "    endpoints = []\n",
    "    #identify the nodes that are not a subset of other nodes\n",
    "    for node in node_list:\n",
    "        other_nodes = [k for k in node_list if k != node ]\n",
    "        not_unique = 0\n",
    "        for path in shortest_paths[node]:\n",
    "            not_unique_Flag = False\n",
    "            for o_node in other_nodes:\n",
    "                for o_shortest_path in shortest_paths[o_node]:\n",
    "                    if set(path) <= set(o_shortest_path):\n",
    "                        not_unique_Flag = True\n",
    "                        \n",
    "            if not_unique_Flag == True:\n",
    "                not_unique = not_unique + 1\n",
    "                \n",
    "        #decide if unique endpoint\n",
    "        if not_unique < len(shortest_paths[node]):   # this means there is a unique path\n",
    "            \n",
    "            #if not_unique != 0:\n",
    "                #print(node + \"-some unique and some non-unique paths for endpoint\")\n",
    "            endpoints.append(node)\n",
    "        \n",
    "    ##print(endpoints)  \n",
    "    longest_paths_list = []\n",
    "    for end_node in endpoints:\n",
    "        longest_path = 0\n",
    "        for path in shortest_paths[end_node]:\n",
    "            path_length = 0\n",
    "            for point in path:\n",
    "                path_length = path_length + mesh_number[point]\n",
    "            if path_length > longest_path:\n",
    "                longest_path = path_length\n",
    "        \n",
    "        longest_paths_list.append((end_node,longest_path))\n",
    "        \n",
    "    #print(longest_paths_list)\n",
    "    longest_paths_list.sort(key=lambda pair: pair[1], reverse=True)\n",
    "    #print(longest_paths_list)\n",
    "    ranked_endpoints = [x for x,i in longest_paths_list]\n",
    "    endpoint_paths_lengths = [i for x,i in longest_paths_list]\n",
    "    \n",
    "    enpoint_path_list = {}\n",
    "    for endpt in ranked_endpoints:\n",
    "        enpoint_path_list[endpt] = shortest_paths[endpt]\n",
    "        \n",
    "    \n",
    "    #ranked_endpoints, longest_paths_list = (list(t) for t in zip(*sorted(zip(endpoints, longest_paths_list))))\n",
    "    \n",
    "    \n",
    "    return ranked_endpoints, enpoint_path_list \n",
    "\n",
    "\n",
    "\n",
    "def classify_whole_neuron(possible_Apical,soma_index,connections,mesh_Number,sdf_final_dict,threshold=700):\n",
    "    cilia_Width_Threshold = 0.05\n",
    "    whole_neuron_labels ={lb:\"unsure\" for lb in connections.keys()}\n",
    "    whole_neuron_labels[soma_index] = \"soma\"\n",
    "    \n",
    "    G=nx.Graph(connections)\n",
    "    \n",
    "    node_list = list(G.nodes)\n",
    "    if(soma_index in node_list):\n",
    "        node_list.remove(soma_index)\n",
    "    else:\n",
    "        return [],[] \n",
    "    \n",
    "    shortest_paths = {}\n",
    "    for node in node_list:\n",
    "        shortest_paths[node] = [k for k in nx.shortest_path(G,node,soma_index)]\n",
    "    \n",
    "    \n",
    "    \"\"\"sdf median of Apical = 0.14563350000000003\n",
    "    sdf median of Axon = 0.07818095\n",
    "    sdf median of Axon segment 2 = 0.0502779\n",
    "    sdf median of Cilia = 0.0102388\n",
    "    sdf median of Cilia 2 = 0.176036\n",
    "    \n",
    "    Rules: if find a segment piece that is less than 0.10 that does not pass through apical and is less than threshold (700) --> everything along path to soma is cilia\n",
    "    --> if doesn't have component less than that sdf value then label as soma\n",
    "    \"\"\"\n",
    "    cilia_Flag = 0\n",
    "    #identify if there is any cilia  0.0102388\n",
    "    for label_key,label_size in mesh_Number.items():\n",
    "        if label_size < threshold:\n",
    "            if sdf_final_dict[label_key][\"median\"] < cilia_Width_Threshold:\n",
    "                #label the label and everything to the soma as cilia\n",
    "                for lb in shortest_paths[label_key]:\n",
    "                    if lb != soma_index:\n",
    "                        if cilia_Flag == 0:\n",
    "                            whole_neuron_labels[lb] = \"cilia\"\n",
    "                            cilia_Flag= 1\n",
    "                        else:\n",
    "                            whole_neuron_labels[lb] = \"error\"\n",
    "            else: #label as part of the soma if too high of width\n",
    "                for lb in shortest_paths[label_key]:\n",
    "                    if lb != soma_index:\n",
    "                        whole_neuron_labels[lb] = \"soma\"\n",
    "    \n",
    "    #if possible apical is None then just return\n",
    "    if possible_Apical == \"None\":\n",
    "        return whole_neuron_labels\n",
    "    \n",
    "    \n",
    "    for label_name, path in shortest_paths.items():\n",
    "        if label_name == possible_Apical:\n",
    "            whole_neuron_labels[label_name] = \"apical\"\n",
    "        else:\n",
    "            if possible_Apical in path:\n",
    "                for jj in path:\n",
    "                    if jj != possible_Apical and jj != soma_index and whole_neuron_labels[jj] == \"unsure\":\n",
    "                       whole_neuron_labels[jj] = \"oblique\" \n",
    "            else:\n",
    "                for jj in path:\n",
    "                    if jj != possible_Apical and jj != soma_index and whole_neuron_labels[jj] == \"unsure\":\n",
    "                       whole_neuron_labels[jj] = \"basal\" \n",
    "    \n",
    "    #return the final list of labels:\n",
    "    return whole_neuron_labels\n",
    "\n",
    "    #CAN'T DETERMINE IF THERE IS AN AXON\n",
    "    \n",
    "    \n",
    "    #endpoint_labels,shortest_paths = find_endpoints(G,mesh_number)\n",
    "def create_whole_neuron_colors(ob):\n",
    "    \n",
    "    color_keys = bpy.data.materials.keys()\n",
    "    #delete all colors that are not part of the basic colors:\n",
    "    #print(\"inside spine color and keys = \" + str(color_keys))\n",
    "    accepted_colors = getColors()\n",
    "    \n",
    "    \"\"\"for i in range(0,len(bpy.data.materials.keys())):\n",
    "        if(color_keys[i] not in accepted_colors):\n",
    "            #print(\"deleting material \" + color_keys[i])\n",
    "            bpy.data.materials.remove(bpy.data.materials[color_keys[i]])\"\"\"\n",
    "        \n",
    "        \n",
    "    create_global_colors()\n",
    "    \n",
    "    #ob.data.materials[0]           \n",
    "    \n",
    "                    \n",
    "    colors_to_add = [\"Apical (blue)\",\n",
    "    \"Basal (yellow)\",\n",
    "    \"Oblique (green)\",\n",
    "    \"Soma (red)\",\n",
    "    \"Cilia (light purple)\",\n",
    "    \"Error (brown)\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    #ob.data.materials[0] = bpy.data.materials[colors_to_add[0]]\n",
    "    \n",
    "    #for i in range(1,len(colors_to_add)):\n",
    "    #    ob.data.materials.append(None)\n",
    "    \n",
    "    current_length = len(ob.data.materials.keys())\n",
    "    print(\"current_length = \" + str(current_length))\n",
    "    \n",
    "    color_indexes = []\n",
    "    for i in range(0,len(colors_to_add)):\n",
    "        ob.data.materials.append(bpy.data.materials[colors_to_add[i]])\n",
    "        color_indexes.append(current_length + i)\n",
    "    \n",
    "    return color_indexes\n",
    "        \n",
    "\n",
    "\n",
    "def label_whole_neuron(labels_list,whole_neuron_labels,ob_name):\n",
    "    ob = bpy.data.objects[ob_name]\n",
    "    \n",
    "    faces_raw = ob.data.polygons\n",
    "    verts_raw = ob.data.vertices\n",
    "    \n",
    "    #delete all the old colors for the object and then add back the local regular colors\n",
    "    color_indexes = create_whole_neuron_colors(ob)\n",
    "    \n",
    "    '''COLORS FOR THE LABELING\n",
    "    colors_to_add = [\"Error (brown)\",\n",
    "    \"Dendrite (purple)\",\n",
    "    \"Spine Head (rose)\",\n",
    "    \"Spine Neck (light green)\",\n",
    "    \"Spine (light pink)\"]'''\n",
    "    \n",
    "    #get the indexes for the labeling from the datajoint table\n",
    "    label_data = ta3.LabelKey2().fetch(\"numeric\",\"description\")\n",
    "    #print(label_data)\n",
    "\n",
    "    label_names = label_data[1].tolist()\n",
    "    label_indexes = label_data[0].tolist()\n",
    "    #print(label_names)\n",
    "\n",
    "    apical_index = label_indexes[label_names.index(\"Apical\")]\n",
    "    basal_index = label_indexes[label_names.index(\"Basal\")]\n",
    "    oblique_index = label_indexes[label_names.index(\"Oblique\")]\n",
    "    soma_index = label_indexes[label_names.index(\"Soma\")]\n",
    "    cilia_index = label_indexes[label_names.index(\"Cilia\")]\n",
    "    error_index = label_indexes[label_names.index(\"Error\")]\n",
    "    print(\"error_index = \" + str(error_index))\n",
    "\n",
    "    \n",
    "    final_faces_labels_list = np.zeros(len(faces_raw))\n",
    "    final_verts_labels_list = np.zeros(len(verts_raw))\n",
    "    \n",
    "    unknown_counter = 0\n",
    "    \n",
    "    for i,lab in enumerate(labels_list):\n",
    "        #get the category according to the dictionary\n",
    "        cat = whole_neuron_labels[lab]\n",
    "        if cat == \"apical\":\n",
    "            faces_raw[i].material_index = color_indexes[0]\n",
    "            final_faces_labels_list[i] = apical_index\n",
    "        elif cat == \"basal\":\n",
    "            faces_raw[i].material_index = color_indexes[1]\n",
    "            final_faces_labels_list[i] = basal_index\n",
    "        elif cat == \"oblique\":\n",
    "            faces_raw[i].material_index = color_indexes[2]\n",
    "            final_faces_labels_list[i] = oblique_index\n",
    "        elif cat == \"soma\":\n",
    "            faces_raw[i].material_index = color_indexes[3]\n",
    "            final_faces_labels_list[i] = soma_index\n",
    "        elif cat == \"cilia\":\n",
    "            faces_raw[i].material_index = color_indexes[4]\n",
    "            final_faces_labels_list[i] = cilia_index\n",
    "        elif cat == \"error\":\n",
    "            faces_raw[i].material_index = color_indexes[5]\n",
    "            final_faces_labels_list[i] = error_index\n",
    "            #print(\"labeling error\")\n",
    "        else:\n",
    "            #faces_raw[i].material_index = color_indexes[4] --don't assign new color\n",
    "            final_faces_labels_list[i] = len(label_indexes) + (int(lab))\n",
    "    \n",
    "    return final_faces_labels_list\n",
    "\n",
    "def generate_output_lists(final_faces_labels_list,ob_name):\n",
    "    face_Counter = Counter(final_faces_labels_list)\n",
    "    #print(face_Counter)\n",
    "    \n",
    "    random_labels = {int(l):int(accepted_color_length()+i) for i,l in enumerate(face_Counter.keys()) if l >= accepted_color_length()}\n",
    "    color_length = accepted_color_length()\n",
    "    for i in range(0,color_length):\n",
    "        random_labels[i] = i\n",
    "        \n",
    "    #print(random_labels)\n",
    "    \n",
    "    #output_faces_list = np.zeros(len(final_faces_labels_list))\n",
    "    #create list for vertices\n",
    "    ob = bpy.data.objects[ob_name]\n",
    "    verts_raw = ob.data.vertices\n",
    "    faces_raw = ob.data.polygons\n",
    "    \n",
    "    start_time_2 = time.time()\n",
    "    #print(\"about to do output faces\")\n",
    "    output_faces_list = [random_labels[int(ll)] for ll in final_faces_labels_list]\n",
    "    #print(\"done output faces\")\n",
    "    #print(\"-----output faces = %s---\"%(time.time()-start_time_2))\n",
    "    start_time_2 = time.time()\n",
    "    \n",
    "    #generate the vertices\n",
    "    #print(\"about to do generate\")\n",
    "    #print(\"output_faces_list = \" + str(output_faces_list))\n",
    "    verts_to_Face,verts_to_Label = generate_verts_to_face_dictionary(output_faces_list,faces_raw,verts_raw)\n",
    "    #print(\"verts_to_Label = \" + str(verts_to_Label))\n",
    "    #print(\"-----verts_to faces = %s---\"%(time.time()-start_time_2))\n",
    "    start_time_2 = time.time()\n",
    "    \n",
    "    output_verts_list = np.zeros(len(verts_raw))\n",
    "    output_verts_list = [int(verts_to_Label[v][0]) for v in verts_to_Label]\n",
    "    \n",
    "    \"\"\"for v in verts_to_Label:\n",
    "        output_verts_list[v] = int(verts_to_Label[v][0])\n",
    "        output_verts_list[v] = int(output_verts_list[v])\"\"\"\n",
    "    #print(\"-----output_verts = %s---\"%(time.time()-start_time_2))\n",
    "    #print(output_verts_list)\n",
    "    #print(\"output_verts_list in function = \" + str(Counter(output_verts_list)))\n",
    "    \n",
    "    return output_faces_list, output_verts_list \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import w2_smooth_whole_neuron \n",
    "import networkx as nx\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \"\"\"bpy.ops.object.mode_set(mode='OBJECT')\n",
    "    # deselect all\n",
    "    bpy.ops.object.select_all(action='DESELECT')\n",
    "\n",
    "    # selection\n",
    "    #for ob in bpy.data.objects\n",
    "    #bpy.data.objects[ob_name].select = True\n",
    "    \n",
    "    for obj in bpy.data.objects:\n",
    "        if \"neuron\" in obj.name or \"bound\" in obj.name:\n",
    "            obj.select = True\n",
    "            \n",
    "   \n",
    "    \n",
    "    # remove it\n",
    "    bpy.ops.object.delete() \n",
    "    #file_loc = \"/Users/brendancelii/Google Drive/Xaq Lab/Datajoint Project/Automatic_Labelers/auto_segmented_big_segments/\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"neuron tab labeler started new\")\n",
    "        #setting the address and the username\n",
    "        print(\"about to connect to database\")\n",
    "        dj.config['database.host'] = '10.28.0.34'\n",
    "        dj.config['database.user'] = 'celiib'\n",
    "        dj.config['database.password'] = 'newceliipass'\n",
    "        #will state whether words are shown or not\n",
    "        dj.config['safemode']=True\n",
    "        print(dj.conn(reset=True))\n",
    "    except:\n",
    "        #Shows a message box with a specific message \n",
    "        print(\"Make sure connected to bcm-wifi!!\")\n",
    "        print(\"ERROR: Make sure connected to bcm-wifi!!\")\n",
    "        raise ValueError(\"ERROR: Make sure connected to bcm-wifi!!\")\n",
    "    \n",
    "    else:\n",
    "        #connect_to_Databases()\n",
    "        #create the database inside the server\n",
    "        schema = dj.schema('microns_ta3p100',create_tables=False)\n",
    "        ta3p100 = dj.create_virtual_module('ta3p100', 'microns_ta3p100')\n",
    "        ta3 = dj.create_virtual_module('ta3', 'microns_ta3')\n",
    "        #reset_Scene_Variables()\n",
    "    \n",
    "        @schema\n",
    "        class Annotation(dj.Computed):\n",
    "            definition = \"\"\"\n",
    "            # creates the labels for the mesh table\n",
    "            -> ta3p100.ComponentAutoSegmentWhole\n",
    "            date_time  : timestamp   #the last time it was edited\n",
    "            ---\n",
    "            vertices   : longblob     # label data for the vertices\n",
    "            triangles  : longblob     # label data for the faces\n",
    "            \"\"\"\n",
    "        \n",
    "\n",
    "            #key_source = ta3.ComponentAutoSegment #& 'n_triangle_indices>100' & [dict(compartment_type=comp) for comp in ['Basal', 'Apical', 'Oblique', 'Dendrite']]\n",
    "            \n",
    "            \n",
    "            def make(self, key):  \n",
    "            \n",
    "                original_start_time = time.time() \n",
    "                #create_bounding_box()\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                \n",
    "                #[331199,421208,481423]\n",
    "                neuron_ID = key[\"segment_id\"]\n",
    "                decimation_ratio = key[\"decimation_ratio\"]\n",
    "                clusters= key[\"clusters\"]\n",
    "                smoothness=key[\"smoothness\"]\n",
    "                \n",
    "                print(\"Neuron ID = \" + str(neuron_ID))\n",
    "                #import the object and create the box\n",
    "                ob_name = load_Neuron_automatic_spine(neuron_ID,decimation_ratio,clusters,smoothness)\n",
    "                #create_bounding_box()\n",
    "                \n",
    "                #assign the colors to the neuron based onthe segmentation data\n",
    "                start_time = time.time()\n",
    "                \n",
    "                #what I will need to get from datajoint acces 1) sdf_final_dict 2) labels_list, might need to make the object active\n",
    "                sdf_final_dict, labels_list = get_cgal_data_and_label(neuron_ID,decimation_ratio, clusters,smoothness)\n",
    "                \n",
    "                #print('df_final_dict[\"1\"][\"median\"] = ' + str(sdf_final_dict[\"1\"][\"median\"]))\n",
    "                \n",
    "                print(\"getting cgal data--- %s seconds ---\" % (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                #find the neuron part that has the highest average sdf value\n",
    "                #high_median, high_median_sdf= get_highest_sdf_part(sdf_final_dict, labels_list )\n",
    "                #need to look for 2nd highest option and see if split soma\n",
    "                #high_median_2nd, high_median_2nd_sdf= get_highest_sdf_part(sdf_final_dict, labels_list,high_median)\n",
    "                \n",
    "                highest_vals= get_highest_sdf_part(sdf_final_dict, labels_list,size_threshold=3000,)\n",
    "                #print(highest_vals)\n",
    "                #highest_vals_2nd= get_highest_sdf_part(sdf_final_dict, labels_list,highest_vals[0])\n",
    "                #print(highest_vals_2nd)\n",
    "                \n",
    "                #print( \"large apical = \" + str(sdf_final_dict[\"76\"][\"max\"]))\n",
    "                \n",
    "                #can do soma merging here\n",
    "                \n",
    "                high_median = highest_vals[0]\n",
    "                #print(\"high_medain = \" + str(high_median ) + \" value = \" + str(high_median_sdf))\n",
    "                #print(\"high_medain 2 = \" + str(high_median_2nd)+ \" value = \" + str(high_median_2nd_sdf))\n",
    "                #print(\"high_mean = \" + str(high_mean ))\n",
    "                #print(\"high_max = \" + str(high_max ))\n",
    "                 \n",
    "                print(\"got highest part--- %s seconds ---\" % (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                \n",
    "                #export the neuron\n",
    "                #export_neuron(ob_name,destination_folder=\"whole_neuron_testing\")\n",
    "                \n",
    "                create_bounding_box()\n",
    "                \n",
    "                \n",
    "                #now do the smoothing\n",
    "                labels_list,verts_to_Face = merge_labels_vp2(labels_list,ob_name,threshold=3000,soma_index=high_median,cilia_threshold=100)\n",
    "                \n",
    "                print(\"done merging labels-- %s seconds ---\" % (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                \n",
    "                #need to find the neighbors of the soma\n",
    "                \n",
    "                ob = bpy.data.objects[ob_name]\n",
    "                #w2_smooth_whole_neuron\n",
    "                faces_raw = ob.data.polygons\n",
    "                verts_raw = ob.data.vertices\n",
    "                \n",
    "                #print(\"labels_list = \" + str(labels_list))\n",
    "                verts_to_Face,verts_to_Label = generate_verts_to_face_dictionary(labels_list,faces_raw,verts_raw)\n",
    "                #pprint(\"done finding verts stats-- %s seconds ---\" % (time.time() - start_time))\n",
    "                print(\"done creating dictionaries -- %s seconds --\" %(time.time() - start_time))\n",
    "                \n",
    "                #print(\"verts_to_Label = \" + str(verts_to_Label))\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                #create a graph structure and stats for the whole neuron\n",
    "                connections, mesh_Number = get_graph_structure(verts_to_Label,labels_list,faces_raw,verts_raw)\n",
    "                print(\"mesh_Number = \" + str(mesh_Number))\n",
    "                print(\"connections = \" + str(connections))\n",
    "                \n",
    "                print(\"done creating connections -- %s seconds --\" %(time.time() - start_time))\n",
    "                \n",
    "                start_time = time.time()\n",
    "                soma_index=high_median\n",
    "                neighbors_list = connections[soma_index].copy()\n",
    "                \n",
    "                neighbors_list.append(soma_index)\n",
    "                min_max = find_max_min_z_vals(neighbors_list,labels_list,faces_raw,verts_raw)\n",
    "                print(\"done FINDING NEIGHBOR STATS -- %s seconds --\" %(time.time() - start_time))\n",
    "                print(\"min_max = \" + str(min_max))\n",
    "                \n",
    "                \"\"\"\n",
    "                print( \"sdf median of Apical = \" + str(sdf_final_dict[\"76\"][\"median\"]))\n",
    "                print( \"sdf median of Axon = \" + str(sdf_final_dict[\"72\"][\"median\"]))\n",
    "                print( \"sdf median of Axon segment 2 = \" + str(sdf_final_dict[\"69\"][\"median\"]))\n",
    "                print( \"sdf median of Cilia = \" + str(sdf_final_dict[\"0\"][\"median\"]))\n",
    "                print( \"sdf median of Cilia 2 = \" + str(sdf_final_dict[\"78\"][\"median\"]))\n",
    "                print( \"Basal Piece = \" + str(sdf_final_dict[\"58\"][\"median\"]))\"\"\"\n",
    "                \n",
    "                \n",
    "                #send data to function that will find the Apical\n",
    "                possible_Apical = find_Apical(min_max,connections,mesh_Number,soma_index,sdf_final_dict)\n",
    "                print(possible_Apical)\n",
    "                \n",
    "                #use the apical label and the soma label to classify the rest as basal or oblique and return a dictionary that has the mapping of label to compartment type\n",
    "                whole_neuron_labels = classify_whole_neuron(possible_Apical,soma_index,connections,mesh_Number,sdf_final_dict,threshold=500)\n",
    "                print(\"whole_neuron_labels = \" + str(whole_neuron_labels))\n",
    "                \n",
    "                #label the neurons according to classification\n",
    "                #############NEED TO ADD STEP THAT CALCULATES THE LABELS OF THE VERTICES ##################\n",
    "                final_faces_labels_list = label_whole_neuron(labels_list,whole_neuron_labels,ob_name)\n",
    "                print(\"done labeling whole neuron\")\n",
    "                \n",
    "                #####need to map the final_faces_labels_list to all successive numbers and get vertices\n",
    "                output_faces_list, output_verts_list = generate_output_lists(final_faces_labels_list,ob_name)\n",
    "                print(\"done generating output list whole neuron\")\n",
    "                \n",
    "                #print(Counter(output_faces_list))\n",
    "                #print(Counter(output_verts_list))\n",
    "                \n",
    "                #Things you have to write to datajoint for primary keys: \n",
    "                \n",
    "                #now write them to the datajoint table  \n",
    "                comp_dict = dict(key,\n",
    "                                    date_time = str(datetime.datetime.now())[0:19],\n",
    "                                    vertices = output_verts_list,\n",
    "                                    triangles = output_faces_list)\n",
    "\n",
    "                #print(\"comp_dict = \" + str(comp_dict))\n",
    "                \n",
    "                print(\"about to write row to datajoint\")\n",
    "                self.insert1(comp_dict,skip_duplicates=True)\n",
    "                print(\"writing label data to datajoint--- %s seconds ---\" % (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                \n",
    "                #delete the object after this\n",
    "                #delete the object\n",
    "                            \n",
    "                # deselect all\n",
    "                bpy.ops.object.select_all(action='DESELECT')\n",
    "\n",
    "                # selection\n",
    "                #for ob in bpy.data.objects\n",
    "                #bpy.data.objects[ob_name].select = True\n",
    "                \n",
    "                for obj in bpy.data.objects:\n",
    "                    if \"neuron\" in obj.name:\n",
    "                        obj.select = True\n",
    "                        \n",
    "                        \n",
    "                # remove it\n",
    "                bpy.ops.object.delete()\n",
    "                \n",
    "                print(\"deleting object--- %s seconds ---\" % (time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # deselect all\n",
    "                bpy.ops.object.select_all(action='DESELECT')\n",
    "\n",
    "                # selection\n",
    "                #for ob in bpy.data.objects\n",
    "                #bpy.data.objects[ob_name].select = True\n",
    "                    \n",
    "                \n",
    "                object_counter = 0\n",
    "                for obj in bpy.data.objects:\n",
    "                    if \"neuron\" in obj.name:\n",
    "                        object_counter += 1\n",
    "                \n",
    "                if object_counter>1:\n",
    "                    raise ValueError(\"THE NUMBER OF OBJECTS ARE MORE THAN 1\")\n",
    "                        \n",
    "                \n",
    "                print(\"finished\")\n",
    "                print(\"--- %s seconds ---\" % (time.time() - original_start_time))\n",
    "                \n",
    "                \n",
    "                \"\"\"neighbors_list,neighbors_shared_vert,number_of_faces = find_neighbors(labels_list,high_median,verts_to_Face,faces_raw,verts_raw)\n",
    "                print(\"neighbors_list = \" + str(neighbors_list))\n",
    "                print(\"done finding soma neighbors-- %s seconds ---\" % (time.time() - start_time))\n",
    "                neighbors_list_max_min = find_max_min_z_vals(neighbors_list + [high_median],labels_list,faces_raw,verts_raw)\n",
    "                print(\"neighbors_list_max_min = \" + str(neighbors_list_max_min))\n",
    "                print(\"done finding verts stats-- %s seconds ---\" % (time.time() - start_time))\n",
    "                \n",
    "                #crawl and make graph structure:\"\"\"\n",
    "                \n",
    "populate_start = time.time()\n",
    "Annotation.populate(reserve_jobs=True)\n",
    "print(\"\\npopulate:\", time.time() - populate_start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

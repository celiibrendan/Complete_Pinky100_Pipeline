{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFirst attempt to combine all functionality in one function\\nthat can be run from top to bottom\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "First attempt to combine all functionality in one function\n",
    "that can be run from top to bottom\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick the mesh to process: \n",
    "file_location = \"./test_meshes/\"\n",
    "file_location = \"./\"\n",
    "file_name =\"seperated_mesh.off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "string is not a file: ./test_meshes/seperated_mesh.off",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e3a415a6341c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#visualize the neuron to make sure it is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_mesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvisualize_mesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trimesh/constants.py\u001b[0m in \u001b[0;36mtimed\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtimed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         log.debug('%s executed in %.4f seconds.',\n\u001b[1;32m    130\u001b[0m                   \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trimesh/exchange/load.py\u001b[0m in \u001b[0;36mload_mesh\u001b[0;34m(file_obj, file_type, resolver, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m      \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_file_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                          \u001b[0mfile_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                          resolver=resolver)\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trimesh/exchange/load.py\u001b[0m in \u001b[0;36mparse_file_args\u001b[0;34m(file_obj, file_type, resolver, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m                     'use load_remote to load URL: {}'.format(file_obj))\n\u001b[1;32m    535\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfile_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'string is not a file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: string is not a file: ./test_meshes/seperated_mesh.off"
     ]
    }
   ],
   "source": [
    "#visualize the neuron to make sure it is correct\n",
    "visualize_mesh = trimesh.load_mesh(file_location + file_name )\n",
    "visualize_mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets parts that are outside of the main mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mesh_significant_outside_pieces(unfiltered_mesh,significance_threshold=2000,n_sample_points=1000):\n",
    "    \"\"\"\n",
    "    Purpose; will take in a full, unfiltered mesh and find the biggest mesh piece, and then return a list of that mesh \n",
    "    with all of the other mesh fragments that are both above the significance_threshold AND outside of the biggest mesh piece\n",
    "\n",
    "    Pseudocode: \n",
    "    1) split the meshes to unconnected pieces\n",
    "    2) Filter the meshes for only those above the significance_threshold\n",
    "    3) find the biggest mesh piece\n",
    "    4) Iterate through all of the remaining pieces:\n",
    "        a. Determine if mesh inside or outside main mesh\n",
    "        b. If outside add to final list to return\n",
    "\n",
    "    Returns: \n",
    "    1) list of significant mesh pieces, including the main one that are not inside of main mesh\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mesh_pieces = unfiltered_mesh.split(only_watertight=False)\n",
    "    \n",
    "    print(f\"There were {len(mesh_pieces)} pieces after mesh split\")\n",
    "\n",
    "    significant_pieces = [m for m in mesh_pieces if len(m.faces) > significance_threshold]\n",
    "\n",
    "    print(f\"There were {len(significant_pieces)} pieces found after size threshold\")\n",
    "    if len(significant_pieces) <=0:\n",
    "        print(\"THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\")\n",
    "        return []\n",
    "\n",
    "    #find piece with largest size\n",
    "    max_index = 0\n",
    "    max_face_len = len(significant_pieces[max_index].faces)\n",
    "\n",
    "    for i in range(1,len(significant_pieces)):\n",
    "        if max_face_len < len(significant_pieces[i].faces):\n",
    "            max_index = i\n",
    "            max_face_len = len(significant_pieces[i].faces)\n",
    "\n",
    "    print(\"max_index = \" + str(max_index))\n",
    "    print(\"max_face_len = \" + str(max_face_len))\n",
    "\n",
    "    final_mesh_pieces = []\n",
    "\n",
    "    main_mesh = significant_pieces[max_index]\n",
    "\n",
    "    #final_mesh_pieces.append(main_mesh)\n",
    "    for i,mesh in enumerate(significant_pieces):\n",
    "        if i != max_index:\n",
    "            #get a random sample of points\n",
    "            # points = np.array(mesh.vertices[:n_sample_points,:]) # OLD WAY OF DOING THIS\n",
    "            idx = np.random.randint(len(mesh.vertices), size=n_sample_points)\n",
    "            points = mesh.vertices[idx,:]\n",
    "            \n",
    "            \n",
    "            start_time = time.time()\n",
    "            signed_distance = trimesh.proximity.signed_distance(main_mesh,points)\n",
    "            print(f\"Total time = {time.time() - start_time}\")\n",
    "\n",
    "            outside_percentage = sum(signed_distance < 0)/n_sample_points\n",
    "            if outside_percentage > 0.9:\n",
    "                final_mesh_pieces.append(mesh)\n",
    "                print(f\"Mesh piece {i} OUTSIDE mesh\")\n",
    "            else:\n",
    "                print(f\"Mesh piece {i} inside mesh :( \")\n",
    "                \n",
    "    return main_mesh,final_mesh_pieces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the global timer\n",
    "global_timer = time.time()\n",
    "\n",
    "\n",
    "unfiltered_mesh = trimesh.load_mesh(file_location + file_name)\n",
    "\n",
    "#setting thresholds\n",
    "significance_threshold=1 #number of faces needed for pieces to be considered to be kept\n",
    "n_sample_points = 5 #number of points sampled on the mesh for determination of inside or outside\n",
    "start_time = time.time()\n",
    "\n",
    "#the main mesh is the first mesh in the piece\n",
    "main_mesh,child_meshes = filter_mesh_significant_outside_pieces(unfiltered_mesh,\n",
    "                            significance_threshold=significance_threshold,\n",
    "                                n_sample_points=n_sample_points)\n",
    "print(f\"Total time for Mesh Cleansing: {time.time() - start_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculates the facets for all the children and the main mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a way of finding the neighbors that have to share adjacent faces\n",
    "def create_neighbors_lookup(mesh):\n",
    "    start_time = time.time()\n",
    "    neighbors_lookup = dict([(i,[]) for i in range(0,len(mesh.faces))])\n",
    "    #print(f\"Creating empty dictionary : {time.time() - start_time}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for adj in mesh.face_adjacency:\n",
    "        neighbors_lookup[adj[0]].append(adj[1])\n",
    "        neighbors_lookup[adj[1]].append(adj[0])\n",
    "    #print(f\"Filling in neighbors lookup : {time.time() - start_time}\")\n",
    "    \n",
    "    return neighbors_lookup\n",
    "\n",
    "def filter_and_expand_facets(new_mesh,\n",
    "                             first_pass_size_threshold=2000,\n",
    "                             normal_closeness=0.985):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #filter the facets that are below a size threshold\n",
    "    new_facets = new_mesh.facets[np.where(new_mesh.facets_area > first_pass_size_threshold)[0]]\n",
    "\n",
    "    neighbors_lookup = create_neighbors_lookup(new_mesh)\n",
    "    \n",
    "    #generate a normals lookup table and see if works faster than regular lookup\n",
    "    normal_lookup = {}\n",
    "\n",
    "    for i in range(0,len(new_mesh.faces)):\n",
    "        normal_lookup[i] = new_mesh.face_normals[i]\n",
    "    \n",
    "    global_start_time = time.time()\n",
    "    final_facets= [0]*len(new_facets)\n",
    "    \n",
    "    for i,facet in enumerate(new_facets):\n",
    "    #     mini_global_start_time = time.time()\n",
    "    #     print(facet)\n",
    "    #     print(type(facet))\n",
    "    #     start_time = time.time()\n",
    "    #     print([find_neighbors(k) for k in facet])\n",
    "        total_neighbors = list(set(np.hstack([neighbors_lookup[k] for k in facet])).difference(set(facet)))\n",
    "    #     print(total_neighbors)\n",
    "    #     print(f\"time initial neighbors: {(time.time() - start_time)}\")\n",
    "        neighbors_to_add = []\n",
    "        neighbors_checked = []\n",
    "        #print(total_neighbors)\n",
    "\n",
    "        #just get the normal from one of the faces already in the facet\n",
    "    #     start_time = time.time()\n",
    "        facet_normal = normal_lookup[facet[0]]\n",
    "\n",
    "    #     total_dot_time = 0\n",
    "        while len(total_neighbors) > 0:\n",
    "            current_neighbor = total_neighbors.pop()\n",
    "            neighbors_checked.append(current_neighbor)\n",
    "\n",
    "    #         print(\"--------------\")\n",
    "    # #         #check to see if neighbor has same normal face\n",
    "    # #         start_dot_time = time.time()\n",
    "    # #         dot_result = np.dot(new_mesh.face_normals[current_neighbor],facet_normal) #> normal_closeness\n",
    "    # #         print(dot_result)\n",
    "    # #         print((time.time() - start_dot_time) * 1000)\n",
    "\n",
    "    #         start_dot_time = time.time()\n",
    "    #         print(current_neighbor)\n",
    "    #         #a = new_mesh.face_normals[current_neighbor]\n",
    "\n",
    "    #         print((time.time() - start_dot_time) * 1000)\n",
    "    #         dot_result = a[0]*facet_normal[0] + a[1]*facet_normal[1] + a[2]*facet_normal[2]  > normal_closeness\n",
    "    #         print(dot_result)\n",
    "    #         print((time.time() - start_dot_time) * 1000)\n",
    "    #         print(\"--------------\")\n",
    "\n",
    "    #         total_dot_time += (time.time() - start_dot_time)*1000\n",
    "\n",
    "            a = normal_lookup[current_neighbor]\n",
    "            if a[0]*facet_normal[0] + a[1]*facet_normal[1] + a[2]*facet_normal[2]  > normal_closeness:\n",
    "\n",
    "                neighbors_to_add.append(current_neighbor)\n",
    "                #get the neighbors of this current face\n",
    "                for neigh in neighbors_lookup[current_neighbor]:\n",
    "                    #only add those neighbors that havent already been checked, in original facet group, or already in list to check\n",
    "                    if neigh not in neighbors_checked and neigh not in facet and neigh not in total_neighbors:\n",
    "                        total_neighbors.append(neigh)\n",
    "    #     print(f\"Total dot time: {total_dot_time}\")\n",
    "    #     print(f\"time loop: {(time.time() - start_time)}\")\n",
    "    #     print(\"neighbors_to_add = \" + str(neighbors_to_add))abs\n",
    "    #     print(\"neighbors_checked = \" + str(neighbors_checked))\n",
    "    #     print(\"adding list = \" + str(list(facet) + neighbors_to_add))\n",
    "    #     start_time = time.time()\n",
    "        final_facets[i] = list(facet) + neighbors_to_add\n",
    "    #     print(f\"Appending to list: {(time.time() - start_time)}\")\n",
    "\n",
    "    #     print(f\"Total time: {(time.time() - mini_global_start_time)}\")\n",
    "    #     print(\"------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    print(f\"Total Facet building time: {(time.time() - global_start_time)}\")\n",
    "    return final_facets\n",
    "\n",
    "def filter_final_facets(gap_mesh):\n",
    "    \"\"\"\n",
    "    Gets the facets faces list and the center points of these facets from mesh\n",
    "    Filters:\n",
    "    1) Only lets facets greater than first_pass_size_threshold exist\n",
    "      **** might need to look at this because area is that of facets before expansion\n",
    "    2) Has to have a high convex border\n",
    "    \n",
    "    Expansions:\n",
    "    1) Expands the facet group to neighbors that are within the normal_closeness\n",
    "    for their normals when doing the dot product\n",
    "    \n",
    "    Order:\n",
    "    1) Size filtering\n",
    "    2) Expansion\n",
    "    3) Convex Border filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    final_facets = filter_and_expand_facets(gap_mesh,\n",
    "                                 first_pass_size_threshold=2000,\n",
    "                                 normal_closeness=0.985)\n",
    "\n",
    "    # after computing final faces, filter for convexity\n",
    "    edges = gap_mesh.edges_sorted.reshape((-1, 6)) #groups all the edges belonging to the corresponding face in one row\n",
    "    final_facets_mean = np.zeros(len(final_facets))\n",
    "    \n",
    "    \n",
    "    #make lookup table for face number to spot in the adjacency edges\n",
    "    face_adjacency_index_lookup = [[] for i in gap_mesh.faces]\n",
    "    for i,faces in enumerate(gap_mesh.face_adjacency):\n",
    "        for f in faces:\n",
    "            face_adjacency_index_lookup[f].append(i)\n",
    "\n",
    "\n",
    "    for j,facet in enumerate(final_facets):\n",
    "        # get the edges for each facet\n",
    "        edges_facet = [edges[i].reshape((-1, 2)) for i in [facet]][0] #stores all the edges belonging to that face\n",
    "\n",
    "        #get the indexes of the boundary edges:\n",
    "        indexes = trimesh.grouping.group_rows(edges_facet, require_count=1)\n",
    "        edge_0 = edges_facet[indexes]\n",
    "\n",
    "        #find the faces that correspond to the boundary edges\n",
    "        edge_0_faces = [facet[int(k/3)] for k in indexes]\n",
    "\n",
    "\n",
    "        #2) Find the indexes of the edges int he face_adajacency_edges and store the projections\n",
    "        adjacency_values = []\n",
    "        for edge,edge_face in zip(edge_0,edge_0_faces):\n",
    "            possible_adj_indexes = face_adjacency_index_lookup[edge_face]\n",
    "\n",
    "            for index in possible_adj_indexes:\n",
    "                if len(set(edge).intersection(set(gap_mesh.face_adjacency_edges[index]))) >= 2:\n",
    "                    #print(f\"adj edge = {e} and boundary edge = {edge}\")\n",
    "                    adjacency_values.append(gap_mesh.face_adjacency_angles[index]) # the metric we actually want to measure\n",
    "                    break\n",
    "\n",
    "\n",
    "        final_facets_mean[j] = np.mean(adjacency_values)\n",
    "        \n",
    "        \n",
    "\n",
    "    #filter the final facets and output them so they can be plotted\n",
    "    adjacency_threshold =  0.8\n",
    "    final_facets_mean_filtered = np.array(final_facets)[final_facets_mean > adjacency_threshold]\n",
    "\n",
    "    #Compute the centers\n",
    "    final_facets_centers = []\n",
    "    \n",
    "    for filt in final_facets_mean_filtered: \n",
    "        #print(\"filt = \" + str(filt))\n",
    "        unique_vertices = gap_mesh.vertices[np.unique(gap_mesh.faces[filt].ravel())].astype(\"float\")\n",
    "        final_facets_centers.append((np.mean(unique_vertices[:,0]),\n",
    "                          np.mean(unique_vertices[:,1]),\n",
    "                          np.mean(unique_vertices[:,2])))\n",
    "    \n",
    "    return final_facets_mean_filtered,final_facets_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_mesh_facets,main_mesh_facets_centers = filter_final_facets(main_mesh)\n",
    "print(\"Finished facets for main mesh\")\n",
    "child_meshes_facets= []\n",
    "for jj,gap_mesh in enumerate(child_meshes):\n",
    "    child_meshes_facets.append(filter_final_facets(gap_mesh))\n",
    "    print(\"Finished facets for child \" + str(jj))\n",
    "\n",
    "#child_meshes_facets = [filter_final_facets(gap_mesh) for gap_mesh in child_meshes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that facet extraction for each piece worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the facets list and see if the process worked:\n",
    "# index = 2\n",
    "# facets_group = np.zeros(len(child_meshes[index].faces)).astype(int)\n",
    "\n",
    "# for i,facet_group in enumerate(child_meshes_facets[index][0]):\n",
    "#     for face in facet_group:\n",
    "#         facets_group[face] = i + 1 #so that you reserve the label 0 for blenders none\n",
    "\n",
    "# np.savez(\"./test_meshes/\" + file_name[:-4] + \"_facet_piece_\" + str(index) + \".npz\",\n",
    "#          facets_group=facets_group,\n",
    "#         facets_group_centers=child_meshes_facets[index][1])\n",
    "\n",
    "# from print_trimesh import print_trimesh\n",
    "# print_trimesh(child_meshes[index],\"./test_meshes/piece_\" + str(index) + \".off\")\n",
    "\n",
    "\n",
    "# len(facets_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START ITERATIVE PROCESS THAT CONNECTS THE MESHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_bbox_filter(child,min_bb_zone,max_bb_zone):\n",
    "    \"\"\"\n",
    "    Determines if child is withing the bounding box zone\n",
    "    designated by the bounding box corners\n",
    "    \n",
    "    \"\"\"\n",
    "    #get the min and max of the bounding box for the mesh\n",
    "    min_bb = np.array(child.bounding_box.vertices).min(0)\n",
    "    max_bb = np.array(child.bounding_box.vertices).max(0)\n",
    "    \n",
    "    #print(min_bb,max_bb)\n",
    "    #print(min_bb_zone,max_bb_zone)\n",
    "    \n",
    "    #if fails any of these checks then return false, else return True\n",
    "    if min(min_bb[0],max_bb[0])>max_bb_zone[0]:\n",
    "        print(\"returning x greater max\")\n",
    "        return False\n",
    "    \n",
    "    if max(min_bb[0],max_bb[0])<min_bb_zone[0]:\n",
    "        print(\"returning x less min\")\n",
    "        return False\n",
    "    \n",
    "    if min(min_bb[1],max_bb[1])>max_bb_zone[1]:\n",
    "        print(\"returning y greater max\")\n",
    "        return False\n",
    "    \n",
    "    if max(min_bb[1],max_bb[1])<min_bb_zone[1]:\n",
    "        print(\"returning y less min\")\n",
    "        return False\n",
    "        \n",
    "    if min(min_bb[2],max_bb[2])>max_bb_zone[2]:\n",
    "        print(\"returning z greater max\")\n",
    "        return False\n",
    "    \n",
    "    if max(min_bb[2],max_bb[2])<min_bb_zone[2]:\n",
    "        print(\"returning z less mim\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "import math\n",
    "def area(vertices):\n",
    "    \"\"\"\n",
    "    Calculates the area of a 3D triangle from it's coordinates\n",
    "    \"\"\"\n",
    "    side_a = np.linalg.norm(vertices[0]-vertices[1])\n",
    "    side_b = np.linalg.norm(vertices[1]-vertices[2])\n",
    "    side_c = np.linalg.norm(vertices[2]-vertices[0])\n",
    "    s = 0.5 * ( side_a + side_b + side_c)\n",
    "    return math.sqrt(s * (s - side_a) * (s - side_b) * (s - side_c))\n",
    "\n",
    "def find_polygon_area(mesh,list_of_faces):\n",
    "    \"Calculates the area of a 3D polygon that is created from connected traingles\"\n",
    "    return(sum([area(mesh.vertices[mesh.faces[r]]) for r in list_of_faces]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restitching functions\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "\n",
    "#gets the projection of point p onto line a\n",
    "def ClosestPointOnLine(a, b, p):\n",
    "    ap = p-a\n",
    "    ab = b-a\n",
    "    #base_vector = ab\n",
    "    result = np.dot(ap,ab)/np.dot(ab,ab) # * ab\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now have the mesh and the facet faces, can send to function\n",
    "def stitch_mesh_piece_vp3(new_mesh,facet_1,facet_2,\n",
    "                          delete_facets=False,\n",
    "                         return_added_mesh = True):\n",
    "    \"\"\"\n",
    "    Changed since last version: \n",
    "    1) parameter for deleting facets at end or not\n",
    "    2) parameter for returning added mesh or not \n",
    "    3) Changed normals check to print statement and not exception\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #how to find the normals of facet groups:\n",
    "    facet_group_1_normal = new_mesh.face_normals[facet_1[0]]\n",
    "    facet_group_2_normal = new_mesh.face_normals[facet_2[0]]\n",
    "\n",
    "\n",
    "    #get the correct version of the normals: (might need to flip them if going in opposite direction)\n",
    "    if np.dot(facet_group_1_normal,facet_group_2_normal) > 0.8:\n",
    "        print(\"same direction normals\")\n",
    "        pass\n",
    "    elif np.dot(facet_group_1_normal,facet_group_2_normal) < -0.8:\n",
    "        print(\"opposite direction normals\")\n",
    "        facet_group_2_normal = facet_group_2_normal*-1\n",
    "    else:\n",
    "        print(\"Not correct normals\")\n",
    "        #raise Exception(\"Not correct normals\")\n",
    "\n",
    "    #print(facet_group_1_normal,facet_group_2_normal)\n",
    "\n",
    "    # make each row correspond to a single face \n",
    "    edges = new_mesh.edges_sorted.reshape((-1, 6))\n",
    "    # get the edges for each facet\n",
    "    edges_facet = [edges[i].reshape((-1, 2)) for i in [facet_1,facet_2]]\n",
    "    edges_boundary = np.array([i[trimesh.grouping.group_rows(i, require_count=1)]\n",
    "                               for i in edges_facet])\n",
    "\n",
    "    #the list of boundary edges and unique points in the boundary edges\n",
    "    edge_0 = edges_boundary[0]\n",
    "    edge_1 = edges_boundary[1]\n",
    "\n",
    "    #gets the unique number of points\n",
    "    edge_0_points = np.unique(np.hstack(edge_0))\n",
    "    edge_1_points = np.unique(np.hstack(edge_1))\n",
    "    print(\"Found boundary edges\")\n",
    "    \"\"\"\n",
    "    get the dot product of all the points\n",
    "    \"\"\"\n",
    "\n",
    "    #get any 2 points on the triangle and make that the reference edge\n",
    "    edge_0_anchor_points = new_mesh.vertices[[edge_0_points[0],edge_0_points[1]]]\n",
    "\n",
    "    #gets the starting index for the 1st facet (so that the start of the stitching is close)\n",
    "    max_index = edge_0_points[0]\n",
    "    max_magnitude = ClosestPointOnLine(edge_0_anchor_points[0],edge_0_anchor_points[1],new_mesh.vertices[max_index])\n",
    "\n",
    "    for i in range(1,len(edge_0_points)):\n",
    "        current_magnitude = ClosestPointOnLine(edge_0_anchor_points[0],edge_0_anchor_points[1],new_mesh.vertices[edge_0_points[i]])\n",
    "\n",
    "        if current_magnitude > max_magnitude:\n",
    "            max_index = i\n",
    "            max_magnitude = current_magnitude\n",
    "\n",
    "    edge_0_starting_point = edge_0_points[max_index]\n",
    "\n",
    "    #gets the starting index for the 2nd facet (so that the start of the stitching is close)\n",
    "    max_index = edge_1_points[0]\n",
    "    max_magnitude = ClosestPointOnLine(edge_0_anchor_points[0],edge_0_anchor_points[1],new_mesh.vertices[max_index])\n",
    "\n",
    "    for i in range(1,len(edge_1_points)):\n",
    "        current_magnitude = ClosestPointOnLine(edge_0_anchor_points[0],edge_0_anchor_points[1],new_mesh.vertices[edge_1_points[i]])\n",
    "        if current_magnitude > max_magnitude:\n",
    "            max_index = i\n",
    "            max_magnitude = current_magnitude\n",
    "\n",
    "    edge_1_starting_point = edge_1_points[max_index]\n",
    "\n",
    "    print(f\"starting edge 1st facet = {edge_0_starting_point}, starting edge 2nd facet= {edge_1_starting_point}, \")\n",
    "    #print(new_mesh.vertices[edge_0_starting_point],new_mesh.vertices[edge_1_starting_point])\n",
    "\n",
    "    \"\"\"\n",
    "    Need to order the points for restitching\n",
    "\n",
    "    Pseudocode: \n",
    "    1) Get starting piont\n",
    "    2) Find the two edges corresponding to that point\n",
    "    3) Need to decide which direction to start....\n",
    "    - go in direction that make the cross of the (1st and last) point in the same direction of the \n",
    "    normal of the first facet\n",
    "    4) loop through and record the orders of the vertices as you traverse along the edges \n",
    "    until you arrive back at the start\n",
    "    5) Error if:\n",
    "        a. You arrive back at the start and haven't processed all the edges\n",
    "        b. Processsed all the edges but haven't arrived back at start\n",
    "\n",
    "    6) Repeat steps 1 through 5 for 2nd facet group\n",
    "    \"\"\"\n",
    "    start_point_list = [edge_0_starting_point,edge_1_starting_point]\n",
    "    edge_list = [edge_0,edge_1]\n",
    "    edge_order_list = []\n",
    "\n",
    "    #loop that organizes the unique boundary points into the correct order\n",
    "    for i,start_point in enumerate(start_point_list):\n",
    "        print(f\"Starting Organizing vertices for side {i}\")\n",
    "        #print(f\"start_point = {start_point}\")\n",
    "        edge_order = [start_point]\n",
    "        processed_edges = []\n",
    "\n",
    "        #find the matching edges\n",
    "        starting_edges_indices = np.where(np.logical_or(edge_list[i][:,0] == start_point,edge_list[i][:,1] == start_point) == True)[0]\n",
    "        print(starting_edges_indices)\n",
    "        starting_edges = edge_list[i][starting_edges_indices]\n",
    "        #print(f\"starting edges = {starting_edges}\") #the list of the two possible edges\n",
    "\n",
    "        if starting_edges.size < 4:\n",
    "            raise Exception(\"Not enough edges for 1st facet start point\")\n",
    "\n",
    "        if starting_edges.size > 4:\n",
    "            raise Exception(\"Too many edges for 1st facet start point\") \n",
    "\n",
    "        #np.where(starting_edges[1,:] != start_point)[0][0]\n",
    "        #gets the vectors that will be used for the cross product\n",
    "        #print(\"np.where(starting_edges[0,:] != start_point)[0][0] = \" + str(np.where(starting_edges[0,:] != start_point)[0][0]))\n",
    "        #print(\"np.where(starting_edges[1,:] != start_point)[0][0] = \" + str(np.where(starting_edges[1,:] != start_point)[0][0]))\n",
    "\n",
    "\n",
    "        #gets the possible starting vectors\n",
    "        possible_starting_vector_1 = new_mesh.vertices[starting_edges[0,:][np.where(starting_edges[0,:] != start_point)[0][0]]] - new_mesh.vertices[start_point]\n",
    "        possible_starting_vector_2 = new_mesh.vertices[starting_edges[1,:][np.where(starting_edges[1,:] != start_point)[0][0]]] - new_mesh.vertices[start_point]\n",
    "        \n",
    "\n",
    "        #find the cross product of the starting vectors\n",
    "        starting_edges_cross = np.cross(possible_starting_vector_1,possible_starting_vector_2)\n",
    "\n",
    "        #make sure that the order of the vectors goes so that the cross product is in line with the starting normal\n",
    "        #this ensures the the circular direction of the stitching will be the same\n",
    "        if np.dot(starting_edges_cross,facet_group_1_normal) > 0:\n",
    "            print(\"Edge 1 picked for direction\")\n",
    "            processed_edges.append(starting_edges_indices[0])\n",
    "            current_vertex = starting_edges[0][np.where(starting_edges[0,:] != start_point)[0][0]]\n",
    "        else:\n",
    "            print(\"Edge 2 picked for direction\")\n",
    "            processed_edges.append(starting_edges_indices[1])\n",
    "            #print(\"np.where(starting_edges[1,:] != start_point) = \" + str(np.where(starting_edges[1,:] != start_point)))\n",
    "            current_vertex = starting_edges[1][np.where(starting_edges[1,:] != start_point)[0][0]]\n",
    "\n",
    "        #print(f\"current_vertex = {current_vertex}\" )\n",
    "        #print(\"edge_list = \" + str(edge_list))\n",
    "\n",
    "        #now iterate through number of \n",
    "        for z in range(1,edge_list[i][:,0].size):\n",
    "            #print(\"edge_order_temp = \" + str(edge_order))\n",
    "            if current_vertex == start_point:\n",
    "                print(\"Start vertex reached before processed all of edges\")\n",
    "                \n",
    "                \"\"\"\n",
    "                \n",
    "                These should be ok because the extra loops are created from holes inside and this process should always get the outside loop\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                break\n",
    "\n",
    "            #get the next edge\n",
    "            counter = 0\n",
    "            next_vertex = -1\n",
    "            for j,edg in enumerate(edge_list[i]):\n",
    "                #print(\"edg = \" + str(edg))\n",
    "                #print(\"processed_edges = \" + str(processed_edges))\n",
    "                if current_vertex in edg and j not in processed_edges:\n",
    "                    current_edge_index = j\n",
    "                    if edg[0] != current_vertex:\n",
    "                        next_vertex = edg[0]\n",
    "                    else:\n",
    "                        next_vertex = edg[1]\n",
    "\n",
    "\n",
    "                    counter += 1\n",
    "                    if counter >= 2:\n",
    "                        raise Exception(f\"More than 2 edges possibilities for {current_vertex}\")\n",
    "\n",
    "            #make sure the next vertex was found\n",
    "            if next_vertex <= -1:\n",
    "                raise Exception(f\"No next vertex was found for {current_vertex} \")\n",
    "\n",
    "            #if found next vertex then add the old vertex and edge index\n",
    "            #to the processed edges lists and the order of vertices\n",
    "            processed_edges.append(current_edge_index)\n",
    "            edge_order.append(current_vertex)\n",
    "\n",
    "            current_vertex = next_vertex\n",
    "\n",
    "\n",
    "        edge_order_list.append(edge_order)\n",
    "        print(f\"edge_{i}_order done, len = {len(edge_order)} \")#\"= {edge_order}\")\n",
    "\n",
    "    lengths_of_boundaries = [len(x) for x in edge_order_list]\n",
    "    bigger = lengths_of_boundaries.index(max(lengths_of_boundaries))\n",
    "    smaller = 1-bigger\n",
    "\n",
    "    #print(f\"index of bigger facets = {bigger}\\nindex of smaller facets = {smaller}\",)\n",
    "\n",
    "    #calculates the number of vertices will be stitched to each vertices on smaller side\n",
    "    dividend = int(lengths_of_boundaries[bigger]/lengths_of_boundaries[smaller])\n",
    "    remainder = lengths_of_boundaries[bigger] - int(lengths_of_boundaries[bigger]/lengths_of_boundaries[smaller])*lengths_of_boundaries[smaller]\n",
    "\n",
    "    print(f\"dividend = {dividend}, remainder = {remainder}\")\n",
    "\n",
    "    #loop that adds the new faces\n",
    "    new_faces = []\n",
    "    current_bigger = 0\n",
    "    for i,current_smaller in enumerate(edge_order_list[smaller]):\n",
    "        #print(\"current_smaller =\" + str(current_smaller))\n",
    "        #print(\"current_bigger=\" + str(current_bigger))\n",
    "        if i == 0:\n",
    "            new_faces.append([current_smaller,edge_order_list[smaller][-1],edge_order_list[bigger][current_bigger]])\n",
    "        else:\n",
    "            new_faces.append([current_smaller,edge_order_list[smaller][i-1],edge_order_list[bigger][current_bigger]])\n",
    "\n",
    "        for j in range(0,dividend + int(i<remainder)):\n",
    "            if current_bigger > len(edge_order_list[bigger]):\n",
    "                raise Exception(\"Somehow rapped around too much\")\n",
    "\n",
    "            if current_bigger >= len(edge_order_list[bigger])-1:\n",
    "                next_bigger = 0\n",
    "            else:\n",
    "                next_bigger = current_bigger+1\n",
    "\n",
    "            new_faces.append([current_smaller,edge_order_list[bigger][current_bigger],\n",
    "                                edge_order_list[bigger][next_bigger]])\n",
    "\n",
    "            current_bigger += 1\n",
    "\n",
    "    #print(\"new_faces = \" + str(new_faces))\n",
    "\n",
    "    stitch_mesh = trimesh.Trimesh()\n",
    "\n",
    "    stitch_mesh.vertices = new_mesh.vertices\n",
    "    stitch_mesh.faces = new_mesh.faces\n",
    "    stitch_mesh.faces = np.vstack([stitch_mesh.faces, new_faces])\n",
    "\n",
    "\n",
    "\n",
    "    if delete_facets == True:\n",
    "        #now take away the original facet faces:\n",
    "        total_faces = np.linspace(0,len(stitch_mesh.faces)-1,len(stitch_mesh.faces)).astype(\"int\")\n",
    "        facet_faces = np.hstack([facet_1 ,facet_2])\n",
    "        faces_to_keep = set(total_faces).difference(set(facet_faces))\n",
    "        faces_to_keep\n",
    "\n",
    "        stitch_mesh = stitch_mesh.submesh([list(faces_to_keep)])[0]\n",
    "\n",
    "    trimesh.repair.fix_inversion(stitch_mesh)\n",
    "    trimesh.repair.fix_winding(stitch_mesh)\n",
    "    trimesh.repair.fix_normals(stitch_mesh)\n",
    "    \n",
    "    #print(\"Finished stitching\")\n",
    "\n",
    "    if return_added_mesh == True:\n",
    "        added_mesh = trimesh.Trimesh()\n",
    "        added_mesh.vertices = new_mesh.vertices\n",
    "        added_mesh.faces = new_faces\n",
    "        trimesh.repair.fix_inversion(added_mesh)\n",
    "        trimesh.repair.fix_winding(added_mesh)\n",
    "        trimesh.repair.fix_normals(added_mesh)\n",
    "\n",
    "        return stitch_mesh,added_mesh\n",
    "    \n",
    "    else:\n",
    "        return stitch_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sets \n",
    "# initial_parameters = dict(bounding_box_threshold=4000,\n",
    "#                          stitch_distance_threshold=3000,\n",
    "#                          size_ratio_threshold=0.30)\n",
    "# main_mesh = main_mesh\n",
    "# bounding_box_threshold = initial_parameters[\"bounding_box_threshold\"]\n",
    "# stitch_distance_threshold = initial_parameters[\"stitch_distance_threshold\"]\n",
    "# size_ratio_threshold = initial_parameters[\"size_ratio_threshold\"]\n",
    "\n",
    "# no_new_children_multiplier = 0\n",
    "# bbox_expansion_percentage = 0.10\n",
    "# stitch_expansion_percentage = 1\n",
    "# size_expansion_percentage = 0.10\n",
    "# no_new_children_limit = 4\n",
    "# consider_same_direction_normals = True\n",
    "\n",
    "# children_processed = []\n",
    "\n",
    "# while len(children_processed) < len(child_meshes):\n",
    "#     if no_new_children_multiplier >= no_new_children_limit:\n",
    "#         print(\"The number of times expanding the thresholds has exceed the limit /n Just returning main mesh\")\n",
    "        \n",
    "    \n",
    "#     #update the thresholds\n",
    "#     bounding_box_threshold = initial_parameters[\"bounding_box_threshold\"]*(1 + bbox_expansion_percentage*no_new_children_multiplier)\n",
    "#     stitch_distance_threshold = initial_parameters[\"stitch_distance_threshold\"]*(1 + stitch_expansion_percentage*no_new_children_multiplier)\n",
    "#     #reduces the \n",
    "#     size_ratio_threshold = initial_parameters[\"size_ratio_threshold\"]*(1 - size_expansion_percentage*no_new_children_multiplier)\n",
    "    \n",
    "#     #print thresholds\n",
    "#     print(f\"bounding_box_threshold = {bounding_box_threshold}\")\n",
    "#     print(f\"stitch_distance_threshold = {stitch_distance_threshold}\" )\n",
    "#     print(f\"size_ratio_threshold = {size_ratio_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Pseudocode for loop at the end that will keep everything going:\n",
    "1) When process a child, will add that index to a list\n",
    "2) Have no_new_children_processed counter set at end of loop \n",
    "    if no children were added to main mesh\n",
    "    --> this will prompt the expansion of the initial parameters\n",
    "3) If this gets too high\n",
    "\n",
    "\n",
    "\n",
    "Things that still need to add:\n",
    "1) Better way of making sure that the normals are good\n",
    "- Can sample one of the neighboring points and flip normals if the dot product is negative\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#sets \n",
    "initial_parameters = dict(bounding_box_threshold=4000,\n",
    "                         stitch_distance_threshold=2000,\n",
    "                         size_ratio_threshold=0.30)\n",
    "main_mesh = main_mesh\n",
    "bounding_box_threshold = initial_parameters[\"bounding_box_threshold\"]\n",
    "stitch_distance_threshold = initial_parameters[\"stitch_distance_threshold\"]\n",
    "size_ratio_threshold = initial_parameters[\"size_ratio_threshold\"]\n",
    "\n",
    "no_new_children_multiplier = 0\n",
    "bbox_expansion_percentage = 0.10\n",
    "stitch_expansion_percentage = 1\n",
    "size_ratio_expansion_percentage = 0.10\n",
    "\n",
    "no_new_children_limit = 4\n",
    "\n",
    "consider_same_direction_normals = True\n",
    "children_processed = []\n",
    "\n",
    "while len(children_processed) < len(child_meshes):\n",
    "    stitch_loop_time = time.time()\n",
    "    if no_new_children_multiplier >= no_new_children_limit:\n",
    "        print(\"The number of times expanding the thresholds has exceed the limit /n Just returning main mesh\")\n",
    "        \n",
    "    \n",
    "    #update the thresholds\n",
    "    bounding_box_threshold = initial_parameters[\"bounding_box_threshold\"]*(1 + bbox_expansion_percentage*no_new_children_multiplier)\n",
    "    stitch_distance_threshold = initial_parameters[\"stitch_distance_threshold\"]*(1 + stitch_expansion_percentage*no_new_children_multiplier)\n",
    "    #reduces the \n",
    "    size_ratio_threshold = initial_parameters[\"size_ratio_threshold\"]*(1 - size_ratio_expansion_percentage*no_new_children_multiplier)\n",
    "    \n",
    "    #print thresholds\n",
    "#     print(f\"bounding_box_threshold = {bounding_box_threshold}\")\n",
    "#     print(f\"stitch_distance_threshold = {stitch_distance_threshold}\" )\n",
    "#     print(f\"size_ratio_threshold = {size_ratio_threshold}\")\n",
    "\n",
    "    #get the main mesh facets normals\n",
    "    main_mesh_normals = [main_mesh.face_normals[fac[0]] for fac in main_mesh_facets]\n",
    "    hit_indexes_list= []\n",
    "    \n",
    "    \n",
    "    #dictionary to save the stitch points\n",
    "    child_meshes_stitch_facets = dict()\n",
    "    child_meshes_stitch_face_ratios = dict()\n",
    "    for i,child in enumerate(child_meshes):\n",
    "        \n",
    "        if i in children_processed:\n",
    "            #print(f\"Child {i} already processed\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Starting Child {i}\")\n",
    "\n",
    "        #initialize the stitch index\n",
    "        #child_meshes_stitch_facets[i] = [-1,-1]\n",
    "\n",
    "        #two highest points for the bounding box\n",
    "        min_bb = np.array(main_mesh.bounding_box.vertices).min(0)\n",
    "        max_bb = np.array(main_mesh.bounding_box.vertices).max(0)\n",
    "\n",
    "        min_bb_zone = min_bb - bounding_box_threshold\n",
    "        max_bb_zone = max_bb + bounding_box_threshold\n",
    "\n",
    "\n",
    "\n",
    "        #then send mesh to function that decides if with\n",
    "        pass_bbox_filter = apply_bbox_filter(child,min_bb_zone,max_bb_zone)\n",
    "\n",
    "        if not pass_bbox_filter:\n",
    "            print(\"skipped by bounding box filter\")\n",
    "            continue\n",
    "\n",
    "        #### used for validation to make sure that bounding box was correctly filtering\n",
    "    #     #add the following to main mesh just to visualize\n",
    "    #     final_mesh = final_mesh + child\n",
    "    #     print(\"just added\")\n",
    "\n",
    "        #\n",
    "        \"\"\"\n",
    "\n",
    "        Do pairwise calculation of distances between centers of facets on child piece and main piece\n",
    "            Filter the pairs for stitch distance threshold\n",
    "            Filter the pairs for matching or opposite normals\n",
    "            Filter for those that are reasonable in size matching (within 50% of each other)\n",
    "\n",
    "        If there are still ties after this point then pitch the best matching size\n",
    "\n",
    "        If None remaining then keep going (might loosen up the parameters later), BUT WITH GLOBAL CHECK SO DOESN’T KEEP ITERATING\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        child_facets,child_facets_centers = child_meshes_facets[i]\n",
    "\n",
    "        facet_distances = np.zeros((len(child_facets_centers),len(main_mesh_facets_centers)))\n",
    "        facet_normals = np.zeros((len(child_facets_centers),len(main_mesh_facets_centers)))\n",
    "\n",
    "        start_time = time.time()\n",
    "        #do the pairwise comparison\n",
    "        for cc,child_center in enumerate(child_facets_centers):\n",
    "\n",
    "            #get the normal of the child\n",
    "            child_normal = child.face_normals[child_facets[cc][0]]\n",
    "            for mm, main_center in enumerate(main_mesh_facets_centers):\n",
    "                #get the euclidean distance\n",
    "    #             facet_distances[cc,mm] = math.sqrt((child_center[0] - main_center[0])**2 + \n",
    "    #                                                  (child_center[1] - main_center[1])**2 + \n",
    "    #                                                  (child_center[2] - main_center[2])**2 )\n",
    "\n",
    "\n",
    "\n",
    "                facet_distances[cc,mm] = np.linalg.norm(np.array(child_center) - np.array(main_center))\n",
    "\n",
    "                main_normal = main_mesh_normals[mm]\n",
    "    #             print(\"main_normal = \" + str(main_normal))\n",
    "    #             print(\"child_normal = \" + str(child_normal))\n",
    "                #get whether the normals are line up (or possibly opposite)\n",
    "                normals_dot = np.dot(child_normal,main_normal)\n",
    "                #print(normals_dot)\n",
    "                if consider_same_direction_normals == True:\n",
    "                    facet_normals[cc,mm] =  normals_dot < -0.95  or normals_dot > 0.95\n",
    "                else:\n",
    "                    facet_normals[cc,mm] =  normals_dot < -0.95 # or normals_dot > 0.95\n",
    "\n",
    "\n",
    "        \n",
    "        print(f\"Total pairwise : {time.time() - start_time}\")\n",
    "\n",
    "    #     if i == 1:\n",
    "    #         4,12\n",
    "    #         print(\"4,12 facet distance = \" + str(facet_distances[4,12]))\n",
    "\n",
    "        #apply the filters\n",
    "        #stitch_distance_booleans = facet_distances < stitch_distance_threshold\n",
    "\n",
    "        \n",
    "        #how to debug certain faces that have been filtered\n",
    "#         main_facet_bebug = 124\n",
    "#         piece_facet_debug = 11\n",
    "        \n",
    "#         if i == 2 and (0 in children_processed):\n",
    "#             print(\"**********\" + str(facet_normals[piece_facet_debug,main_facet_bebug]) + \"**********\")\n",
    "#             print(\"**********\" + str((facet_distances < stitch_distance_threshold)[piece_facet_debug,main_facet_bebug]) + \"**********\")\n",
    "        \n",
    "        \n",
    "        #get the indexes that make it through the first two filters\n",
    "        hit_indexes = np.where(np.logical_and(facet_distances < stitch_distance_threshold,facet_normals)== True) \n",
    "\n",
    "        possible_stitch_pairs = np.vstack(hit_indexes).T\n",
    "        \n",
    "        #print(\"possible_stitch_pairs = \" + str(possible_stitch_pairs))\n",
    "            \n",
    "\n",
    "        face_0_unique_facets = np.unique(hit_indexes[0])\n",
    "        face_1_unique_facets = np.unique(hit_indexes[1])\n",
    "        #get the sizes of all the unique ones\n",
    "\n",
    "        if len(hit_indexes[0]) <= 0:\n",
    "            print(f\"Child {i} There was no possible stitch found after stitch distance and face normal filters\")\n",
    "            continue\n",
    "\n",
    "        if len(hit_indexes[0]) > 0:\n",
    "            face_0_facet_sizes = dict([(u,find_polygon_area(child,child_facets[u])) for u in face_0_unique_facets])\n",
    "            face_1_facet_sizes = dict([(u,find_polygon_area(main_mesh,main_mesh_facets[u])) for u in face_1_unique_facets])\n",
    "\n",
    "    #     print(\"possible_stitch_pairs = \" + str(possible_stitch_pairs))\n",
    "    #     print(\"len(hit_indexes) = \" + str(len(hit_indexes)))\n",
    "    #     print(\"hit_indexes[0].any() = \" + str(hit_indexes[0].any()))\n",
    "    #     print(\"hit_indexes = \" + str(hit_indexes))\n",
    "    #     print(\"hit_indexes[0] = \" + str(hit_indexes[0]))\n",
    "    #     print(\"len(hit_indexes[0]) = \" + str(len(hit_indexes[0])))\n",
    "\n",
    "        #find the sizes of all of them\n",
    "        face_pair_sizes = np.zeros(len(possible_stitch_pairs[:,0]))\n",
    "        face_size_ratios = np.zeros(len(possible_stitch_pairs[:,0]))\n",
    "\n",
    "        #print(\"possible_stitch_pairs = \" + str(possible_stitch_pairs))\n",
    "        for numba,pair in enumerate(possible_stitch_pairs):\n",
    "            #print(\"pair = \" + str(pair))\n",
    "            sizes = [face_0_facet_sizes[pair[0]],face_1_facet_sizes[pair[1]]]\n",
    "            min_area = min(sizes)\n",
    "            max_area = max(sizes)\n",
    "\n",
    "            ratio = min_area/max_area\n",
    "\n",
    "            #print(f\"ratio = {ratio}\")\n",
    "            #print(f\"Total size  = {min_area + max_area}\")\n",
    "            if ratio >= size_ratio_threshold:\n",
    "                face_pair_sizes[numba] = min_area + max_area\n",
    "                face_size_ratios[numba] = ratio\n",
    "            \n",
    "                #print(f\"face_pair_sizes[numba] = {face_pair_sizes[numba]}, face_size_ratios[numba] = {face_size_ratios[numba]}\")\n",
    "\n",
    "\n",
    "        #check that made it past stitch ratio threshold\n",
    "\n",
    "        #best possible stitch pair is just the maximum sized matching ones\n",
    "        best_index = np.where(face_pair_sizes == max(face_pair_sizes))\n",
    "        best_stitch_pair = possible_stitch_pairs[best_index][0]\n",
    "        best_stitch_pair_size = face_pair_sizes[best_index][0]\n",
    "        best_stitch_pair_size_ratio = face_size_ratios[best_index][0]\n",
    "        \n",
    "        print(\"best_stitch_pair = \" + str(best_stitch_pair))\n",
    "        print(\"best_stitch_pair_size = \" + str(best_stitch_pair_size))\n",
    "        print(\"best_stitch_pair_size_ratio = \" + str(best_stitch_pair_size_ratio))\n",
    "\n",
    "        child_meshes_stitch_facets[i] = [best_stitch_pair[0],best_stitch_pair[1]]\n",
    "        child_meshes_stitch_face_ratios[i] = best_stitch_pair_size_ratio\n",
    "\n",
    "    \n",
    "#     if 0 in child_meshes_stitch_facets.keys():\n",
    "#         print(\"Just processed piece 1\")\n",
    "#         break\n",
    "    \n",
    "    \n",
    "    # makes sure that no two child branches try to connect to the same main branch\n",
    "    from collections import Counter\n",
    "    mesh_stitch_counter = Counter(np.array([val for val in child_meshes_stitch_facets.values()])[:,1])\n",
    "\n",
    "    repeat_main_facets = [key for key,val in mesh_stitch_counter.items() if (key != -1 and val > 1)] #gets the main mesh facet with multiples\n",
    "    print(\"repeat_main_facets = \" + str(repeat_main_facets))\n",
    "\n",
    "\n",
    "    #how to fix that some faces are trying to branch to same main facet\n",
    "    if len(repeat_main_facets)>0:\n",
    "\n",
    "        child_mesh_double_indexes = [key for key,val in child_meshes_stitch_facets.items() if val[1] in repeat_main_facets]\n",
    "        print(\"child_mesh_double_indexes = \" + str(child_mesh_double_indexes))\n",
    "\n",
    "\n",
    "        #decide which one to keep and then ditch all the rest --> pick the best matching area:\n",
    "        max_ratio = -1\n",
    "        max_child = -1\n",
    "\n",
    "\n",
    "        for child_index in child_mesh_double_indexes:\n",
    "            current_ratio = child_meshes_stitch_face_ratios[child_index]\n",
    "\n",
    "            if current_ratio > max_ratio:\n",
    "                max_child = child_index\n",
    "                max_ratio = current_ratio\n",
    "\n",
    "        print(f\"max_child = {max_child}, max_ratio = {max_ratio}\")\n",
    "\n",
    "        #remove the others from the stitch facets\n",
    "        for double_index in child_mesh_double_indexes:\n",
    "            if double_index != max_child:\n",
    "                del child_meshes_stitch_facets[double_index]\n",
    "    \n",
    "    \"\"\"\n",
    "    Pseudocode for stitching:\n",
    "    1) For each pair in the child_meshes_stitch_facets:\n",
    "    a. Get the child mesh for that pair\n",
    "    b. Get the list of faces for the child facet (from the facet number)\n",
    "    c. Get the list of faces for the main facet (from the main number)\n",
    "\n",
    "    d. Get the original number of faces and vertices in the main mesh\n",
    "    d2. Use the orignal number of faces and add to list of faces for child facet to offset them correctly\n",
    "        - Save this number list in a dictionary (to use for later and creating the submesh)\n",
    "    e. Add the two meshes together to get big mesh\n",
    "    f. Send the two meshes and the two facet lists to the restitching function to get a main mesh that is stitched up\n",
    "     - but send it to function that doesnt delete the original facet faces \n",
    "         (because this would remove meshes from original and screw up facet number)\n",
    "    g. reassign the main_mesh to this newly stitched up mesh\n",
    "    h. recompute the facets for the main mesh\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #lists to store the faces that need to be removed later\n",
    "    child_faces_to_remove = []\n",
    "    main_faces_to_remove = []\n",
    "    current_main_mesh = main_mesh.copy()\n",
    "\n",
    "    \n",
    "    #if there were no possible stitch points found\n",
    "    if len(child_meshes_stitch_facets.keys()) == 0:\n",
    "        #increment the no children flag multiplier\n",
    "        \n",
    "        no_new_children_multiplier += 1\n",
    "        print(f\"no stitch points found IN ALL CHILDREN --> relaxing the parameters time {no_new_children_multiplier}\")\n",
    "        continue\n",
    "    else: #if there were stitch points found\n",
    "        print(\"child_meshes_stitch_facets = \" + str(child_meshes_stitch_facets))\n",
    "        for child_key,pair in child_meshes_stitch_facets.items():\n",
    "            stitch_time = time.time()\n",
    "            print(f\"---Stitching child {child_key} with pair: {pair}---\")\n",
    "            current_child_mesh = child_meshes[child_key]\n",
    "            current_child_facet_faces = child_meshes_facets[child_key][0][pair[0]]\n",
    "\n",
    "\n",
    "            current_main_mesh_facet_faces = main_mesh_facets[pair[1]]\n",
    "\n",
    "            #Get the original number of faces and vertices in the main mesh\n",
    "            original_mesh_faces_len = len(current_main_mesh.faces)\n",
    "            current_child_facet_faces_adjusted = [k + original_mesh_faces_len \n",
    "                                                              for k in current_child_facet_faces]\n",
    "\n",
    "            #Save the faces number for deletion later\n",
    "            child_faces_to_remove += current_child_facet_faces_adjusted\n",
    "            main_faces_to_remove += current_main_mesh_facet_faces\n",
    "\n",
    "            combined_mesh = main_mesh + current_child_mesh\n",
    "\n",
    "            #how to stitch up the mesh\n",
    "            stitch_mesh,added_mesh = stitch_mesh_piece_vp3(new_mesh=combined_mesh,\n",
    "                                                           facet_1=current_main_mesh_facet_faces,\n",
    "                                                           facet_2=current_child_facet_faces_adjusted,\n",
    "                                                          delete_facets=False,\n",
    "                                                          return_added_mesh=True)\n",
    "\n",
    "            #reassign the main mesh\n",
    "            current_main_mesh = stitch_mesh\n",
    "\n",
    "            #add the child to processed child\n",
    "            children_processed.append(child_key)\n",
    "            print(f\"Finished stitching child {child_key} : {time.time() - stitch_time}\")\n",
    "\n",
    "        #remove all of the processed facets from the main mesh\n",
    "        #now take away the original facet faces:\n",
    "        total_faces = np.linspace(0,len(current_main_mesh.faces)-1,len(current_main_mesh.faces)).astype(\"int\")\n",
    "        facet_faces = np.hstack([child_faces_to_remove ,main_faces_to_remove])\n",
    "        faces_to_keep = set(total_faces).difference(set(facet_faces))\n",
    "\n",
    "\n",
    "        main_mesh = current_main_mesh.submesh([list(faces_to_keep)])[0]\n",
    "\n",
    "\n",
    "        #reset the no_new_children_multiplier because there were successful stitching\n",
    "        no_new_children_multiplier = 0\n",
    "        #recompute the main mesh facets\n",
    "        main_mesh_facets,main_mesh_facets_centers = filter_final_facets(main_mesh)\n",
    "        print(f\"***************Finished big stitching iteration: {time.time() - stitch_loop_time}***************\")\n",
    "        \n",
    "#         if 0 in children_processed and 1 in children_processed:\n",
    "#             print(\"Exiting before does piece 2\")\n",
    "#             break\n",
    "\n",
    "        if len(np.unique(children_processed)) == len(child_meshes):\n",
    "            print(\"All children have been processed\")\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out total time it took: \n",
    "print(f\"Total time for neuron stitching: {time.time() - global_timer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that stitching went well:\n",
    "output_flag = True\n",
    "if output_flag == True:\n",
    "    from print_trimesh import print_trimesh\n",
    "    print_trimesh(main_mesh,\"./test_meshes/debug_6.off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Conclusion: found out that all of stitching was done correctly\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the wrong Facet Picking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

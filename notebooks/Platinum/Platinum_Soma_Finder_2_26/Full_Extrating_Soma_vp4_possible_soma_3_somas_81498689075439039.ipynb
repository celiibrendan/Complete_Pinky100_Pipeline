{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing out the full soma extraction\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "from whole_neuron_classifier_datajoint_adapted import extract_branches_whole_neuron\n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m65 = dj.create_virtual_module('m65', 'microns_minnie65_01')\n",
    "schema = dj.schema(\"microns_minnie65_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_id = 107816118160698192\n",
    "version = 0\n",
    "key = dict(segment_id=segment_id, version = version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Entire Mesh Processed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all the meshes into one mesh\n",
    "def add_mesh_piece(main_mesh_vertices,main_mesh_faces,sub_mesh_vertices,sub_mesh_faces):\n",
    "    \"\"\"\n",
    "    Purpose: Takes in a large mesh piece and an array of other meshes and \n",
    "    returns a large mesh with all meshes appended\n",
    "    \n",
    "    Parameters:\n",
    "    main_mesh_vertices (np.array) : np array store the vertices as rows and the elements as the coordinates\n",
    "    main_mesh_faces (np.array) : np array store the faces as rows and the elements as the referenced vertices\n",
    "    sub_mesh_vertices(list of np.arrays) : list of np arrays with the vertices arrays for all subsegments to be added\n",
    "    sub_mesh_faces(list of np.arrays) : list of np arrays with the faces arrays for all subsegments to be added\n",
    "    \n",
    "    Returns:\n",
    "    mesh_vertices (np.array) : np array store the vertices as rows and the elements as the coordinates for NEW CONCATENATED MESH\n",
    "    mesh_faces (np.array) : np array store the faces as rows and the elements as the referenced vertices for NEW CONCATENATED MESH\n",
    "    \n",
    "    \n",
    "    Pseudocode: \n",
    "    - Checks: \n",
    "    a. Make sure there sub_mesh arrays are greater than 0 and of the same length\n",
    "\n",
    "    1) Count the number of vertices and faces in the main mesh\n",
    "    2) Iterate through the submesh vertices and faces. In loop:\n",
    "    a. Count the number of vertices in the submesh and concate the vertices arrays to the main mesh array\n",
    "    b. Add the vertices_count and add that to every number in the faces array\n",
    "    c. Concatenate the submesh faces onto the larger mesh face\n",
    "    d. Save this new vertices and faces as the main_mesh verts and faces\n",
    "    e. Print out how many new vertices and faces added\n",
    "    3) Print out number of segments added, total faces/vertices for new mesh\n",
    "    4) Return the main mesh vertices and faces\n",
    "    \n",
    "    \"\"\"\n",
    "    #a. Make sure there sub_mesh arrays are greater than 0 and of the same length\n",
    "    if len(sub_mesh_vertices) <= 0:\n",
    "        print(\"There were no vertices in submesh to add, returning main mesh\")\n",
    "        return main_mesh_vertices, main_mesh_faces\n",
    "    if len(sub_mesh_faces) <= 0:\n",
    "        print(\"There were no face in submesh to add, returning main mesh\")\n",
    "        return main_mesh_vertices, main_mesh_faces\n",
    "    if len(sub_mesh_faces) != len(sub_mesh_vertices):\n",
    "        raise Exception(\"The sub_mesh_faces and sub_mesh_vertices length did not match\")\n",
    "        \n",
    "    \n",
    "    #1) Count the number of vertices and faces in the main mesh\n",
    "    n_main_vertices = len(main_mesh_vertices)\n",
    "    n_main_faces = len(main_mesh_faces)\n",
    "    \n",
    "    \n",
    "    #2) Iterate through the submesh vertices and faces. In loop:\n",
    "    for i,(sub_verts, sub_faces) in enumerate(zip(sub_mesh_vertices,sub_mesh_faces)):\n",
    "        #a. Count the number of vertices in the submesh and concate the vertices arrays to the main mesh array\n",
    "        n_sub_verts = len(sub_verts)\n",
    "        n_sub_faces = len(sub_faces)\n",
    "        \n",
    "        main_mesh_vertices = np.vstack([main_mesh_vertices,sub_verts])\n",
    "\n",
    "        \n",
    "        #b. Add the vertices_count of main to every number in the faces array\n",
    "        sub_faces = sub_faces + n_main_vertices\n",
    "        \n",
    "        #c. Concatenate the submesh faces onto the larger mesh face\n",
    "        main_mesh_faces = np.vstack([main_mesh_faces,sub_faces])\n",
    "        \n",
    "        #d. Save this new vertices and faces as the main_mesh verts and faces (DONE)\n",
    "        \n",
    "        #e. Print out how many new vertices and faces added\n",
    "        #print(f\"Added subsegment {i} with {n_sub_verts} vertices and {n_sub_faces} faces\")\n",
    "        \n",
    "        n_main_vertices = len(main_mesh_vertices)\n",
    "        n_main_faces = len(main_mesh_faces)\n",
    "    \n",
    "    #3) Print out number of segments added, total faces/vertices for new mesh  \n",
    "    print(f\"Added {len(sub_mesh_vertices)} subsegements \\n  --> final mesh: {len(main_mesh_vertices)} vertices and {len(main_mesh_faces)} faces\")\n",
    "        \n",
    "    return main_mesh_vertices,main_mesh_faces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = (m65.FromNeuromancer & key).fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---Main Neuron ---\")\n",
    "print(n[\"segment_id\"])\n",
    "print(n[\"n_vertices\"])\n",
    "print(n[\"vertices\"])\n",
    "print(n[\"faces\"])\n",
    "\n",
    "#get all of the segments and their data\n",
    "lookup_key = dict(segment_id=n[\"segment_id\"],version = n[\"version\"])\n",
    "subsegments = (m65.FromNeuromancer.Subsegment & lookup_key).fetch(as_dict=True)\n",
    "\n",
    "subsegment_dicts = dict([(k[\"subsegment_id\"],dict(vertices=k[\"vertices\"],faces=k[\"faces\"])) for k in subsegments])\n",
    "\n",
    "subsegment_ordered_list = np.sort(np.array(list(subsegment_dicts.keys())))\n",
    "subsegments_vertices = [subsegment_dicts[k][\"vertices\"] for k in subsegment_ordered_list]\n",
    "subsegments_faces = [subsegment_dicts[k][\"faces\"] for k in subsegment_ordered_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the entire mesh from the main mesh and all of its sub meshes: \n",
    "new_mesh_vertices, new_mesh_faces = add_mesh_piece(main_mesh_vertices=n[\"vertices\"],\n",
    "                   main_mesh_faces=n[\"faces\"],\n",
    "                   sub_mesh_vertices = subsegments_vertices,\n",
    "                   sub_mesh_faces=subsegments_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Meshlab and CGAL algorithms to extract the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"_orig_inal.off\"\n",
    "indices = [i for i, a in enumerate(s) if a == \"_\"]\n",
    "print(indices)\n",
    "s[:-(len(s)-indices[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "original_main = trimesh.Trimesh(vertices=n[\"vertices\"],faces=n[\"faces\"])\n",
    "output_mesh_name = \"temp/\" + str(n[\"segment_id\"]) + \"_original.off\"\n",
    "original_main.export(\"./\" + output_mesh_name)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# having exporting the mesh then starts to do the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 81498689075439039_multiple_somas.off\n"
     ]
    }
   ],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "from whole_neuron_classifier_datajoint_adapted import extract_branches_whole_neuron\n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datajoint as dj\n",
    "\n",
    "import os\n",
    "# total_test_meshes = [\n",
    "# '103178515351946567_stitched.off',\n",
    "# '104726695973782750_stitched.off',\n",
    "# '106626583548494129_stitched.off',\n",
    "# '107816118160698192_stitched.off',\n",
    "# '110778132960975016_stitched.off',\n",
    "# '96631955273149705_stitched.off',\n",
    "# '81498689075439039_multiple_somas.off']\n",
    "\n",
    "# output_file = total_test_meshes[0]\n",
    "# folder_name = \"test_neurons/\" \n",
    "\n",
    "total_test_meshes = [\n",
    "'81498689075439039_multiple_somas.off']\n",
    "\n",
    "output_file = total_test_meshes[0]\n",
    "folder_name = \"neurons_potential_soma/\" \n",
    "\n",
    "output_mesh_name = folder_name + output_file\n",
    "print(f\"Working on {output_file}\")\n",
    "\n",
    "indices = [i for i, a in enumerate(output_file) if a == \"_\"]\n",
    "indices\n",
    "seg_id_stripped = output_file[:indices[0]]\n",
    "n = dict(segment_id=int(seg_id_stripped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_meshlab_script(mlx_script,input_mesh_file,output_mesh_file):\n",
    "    script_command = (\" -i \" + str(input_mesh_file) + \" -o \" + \n",
    "                                    str(output_mesh_file) + \" -s \" + str(mlx_script))\n",
    "    #return script_command\n",
    "    command_to_run = 'xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@ ' + script_command\n",
    "    #command_to_run = 'meshlabserver ' + script_command\n",
    "    \n",
    "    print(command_to_run)\n",
    "    subprocess_result = subprocess.run(command_to_run,shell=True)\n",
    "    \n",
    "    return subprocess_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess\n",
    "def meshlab_fix_manifold_path_specific_mls(input_path_and_filename,\n",
    "                                           output_path_and_filename=\"\",\n",
    "                                           segment_id=-1,meshlab_script=\"\"):\n",
    "    #fix the path if it comes with the extension\n",
    "    if input_path_and_filename[-4:] == \".off\":\n",
    "        path_and_filename = input_path_and_filename[:-4]\n",
    "        input_mesh = input_path_and_filename\n",
    "    else:\n",
    "        raise Exception(\"Not passed off file\")\n",
    "    \n",
    "    \n",
    "    if output_path_and_filename == \"\":\n",
    "        output_mesh = path_and_filename+\"_mls.off\"\n",
    "    else:\n",
    "        output_mesh = output_path_and_filename\n",
    "    \n",
    "    if meshlab_script == \"\":\n",
    "        meshlab_script = str(pathlib.Path.cwd()) + \"/\" + \"remeshing_remove_non_man_edges.mls\"\n",
    "    \n",
    "    #print(\"meshlab_script = \" + str(meshlab_script))\n",
    "    #print(\"starting meshlabserver fixing non-manifolds\")\n",
    "    subprocess_result_1 = run_meshlab_script(meshlab_script,\n",
    "                      input_mesh,\n",
    "                      output_mesh)\n",
    "    #print(\"Poisson subprocess_result= \"+ str(subprocess_result_1))\n",
    "    \n",
    "    if str(subprocess_result_1)[-13:] != \"returncode=0)\":\n",
    "        raise Exception('neuron' + str(segment_id) + \n",
    "                         ' did not fix the manifold edges')\n",
    "    \n",
    "    return output_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done exporting\n"
     ]
    }
   ],
   "source": [
    "new_mesh = trimesh.load_mesh(output_mesh_name)\n",
    "mesh_splits = new_mesh.split(only_watertight=False)\n",
    "\n",
    "#len(\"Total mesh splits = \" + str(mesh_splits))\n",
    "#get the largest mesh\n",
    "mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# sns.distplot(mesh_lengths)\n",
    "\n",
    "largest_index = np.where(mesh_lengths == np.max(mesh_lengths))\n",
    "largest_mesh = mesh_splits[largest_index][0]\n",
    "\n",
    "\n",
    "stripped_ending = output_mesh_name[:-4]\n",
    "pre_largest_mesh_path = stripped_ending + \"_largest_piece.off\"\n",
    "\n",
    "largest_mesh.export(pre_largest_mesh_path)\n",
    "print(\"done exporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1404104,       4,       2,       4,       4,       2,       4,\n",
       "             2,      30,       2,       4,       2,       4,       4,\n",
       "             4,       4,       4,       4,       4,       4,       4,\n",
       "             4,       2,       4,       4,       2,      18,       4,\n",
       "             4,       4,       4,      20,      12,       4,       6,\n",
       "             4,       4,       4,       2,       4,       4,       4,\n",
       "             2,       2,       4,       4,       4,       4,       2,\n",
       "            64,       4,      28,       4,       4,       4,       6,\n",
       "             4,       4,       2,       2,       2,       2,       2,\n",
       "             2,       2,       2,       2,       2,       2,       2,\n",
       "             2,       2,       2,       2,       4,       2,       2,\n",
       "             4,      20,       2,       4,       2,       4,       2,\n",
       "             4,       4,       4,       4,       4,       4,       4,\n",
       "             4,       2,       6,       4,       4,      34,       2,\n",
       "             4,       2,       2,       2,       2,       2,       4,\n",
       "             4,       2,       4,       2,       2,      10,       2,\n",
       "             4,       4,       2,       4,       4,       4,       4,\n",
       "             8,       4,       2,       4,      16,       2,      12,\n",
       "             4,       4,       6,       4,       4,       4,       4,\n",
       "             2,       4,       2,       4,       4,       4,      24,\n",
       "            24,       4,       4,      20,       4,       4,       4,\n",
       "             4,       4,       4,       4,       4,       4,       4,\n",
       "             4,       4,       2,       4,       4,       4,       2,\n",
       "            20,       2,       2,       4,       4,       8,       4,\n",
       "             4,       4,      22,       4,       4,       4,       4,\n",
       "             4,       4,       2,       4,       4,       4,       2,\n",
       "             2,       2,       4,       2,       2,       2,       4,\n",
       "             8,       2,       4,       4,       2,       4,       4,\n",
       "             4,       2,      10,       4,       4,       4,       4,\n",
       "             2,       6,       4,       4,       4,       4,       4,\n",
       "             6,       4,       4,       4,       4,       4,       2,\n",
       "             2,      14,      20,      32,      22,       4,       4,\n",
       "             4,       6,       4,       4,       4,       2,       4,\n",
       "             4,       4,      28,       4,       4,       2,       4,\n",
       "             4,       4,       2,       4,       6,       4,      10,\n",
       "             4,       4,       2,       4,      60,       6,       4,\n",
       "             2,       4,       4,       4,       6,       6,       4,\n",
       "             4,       2,       2,       4,       4,       4,       4,\n",
       "             4,       4,       4,       4,       4,       4,       4,\n",
       "             4,       4,       4,       4,       4,       8,      14,\n",
       "             4,       6,       4,       4,       4,       4,       4,\n",
       "             4,       4,       4,       4,       2,       4,       4,\n",
       "             4,       4,       4,       2,       2,       2,       4,\n",
       "             4,       2,       4,       4,       4,       4,       4,\n",
       "            68,      10,       4,       4,       4,       4,       4,\n",
       "            18,       4,      10,       2,       2,      26,      14,\n",
       "             4,       4,       4,       4,       4,       4,       4,\n",
       "             4,       4,       4,       4,       4,       4,       2,\n",
       "             4,       4,       2,       2,       4,       2,       2,\n",
       "             2,       2,       4,       4,       4,       4,       4,\n",
       "             4,       8,       4,       4,       4,       4,       4,\n",
       "             2,       4,       4,       4,       4,       4,       2,\n",
       "             4,       2,       4,       4,       4,       4,       4,\n",
       "             8,       4,       4,       2,       2,       2,       4,\n",
       "             4,       4,       4,       2,      18,       6,       4,\n",
       "             4,       2,       2,       2,       2,       4,       2,\n",
       "             2,       4,       4,       4,       4,       2,       2,\n",
       "             2,       4,       2,       4,       4,       2,      10,\n",
       "            10,       4,       2,      24,       4,       4,       4,\n",
       "             4,       2,       2,       4,       4,       4,       2,\n",
       "             2,       2,       2,       2,       4,       2,       4,\n",
       "             4,      56,       2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/poisson_working_meshlab.mls\n",
      "/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece.off\n",
      "/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece_mls.off\n",
      "Running the mls function\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece.off -o /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece_mls.off -s /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/poisson_working_meshlab.mls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece_mls.off'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "# run the meshlab server script\n",
    "script_name = \"poisson_working_meshlab.mls\"\n",
    "meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "input_path =str(pathlib.Path.cwd()) + \"/\" +  pre_largest_mesh_path\n",
    "\n",
    "indices = [i for i, a in enumerate(input_path) if a == \"_\"]\n",
    "stripped_ending = input_path[:-4]\n",
    "\n",
    "output_path = stripped_ending + \"_mls.off\"\n",
    "print(meshlab_script_path_and_name)\n",
    "print(input_path)\n",
    "print(output_path)\n",
    "print(\"Running the mls function\")\n",
    "meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_path,\n",
    "                                           output_path_and_filename=output_path,\n",
    "                                           segment_id=n[\"segment_id\"],\n",
    "                                           meshlab_script=meshlab_script_path_and_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CGAL segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the mesh\n",
    "new_mesh = trimesh.load_mesh(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done exporting\n"
     ]
    }
   ],
   "source": [
    "mesh_splits = new_mesh.split(only_watertight=False)\n",
    "\n",
    "len(\"Total mesh splits = \" + str(mesh_splits))\n",
    "#get the largest mesh\n",
    "mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# sns.distplot(mesh_lengths)\n",
    "\n",
    "largest_index = np.where(mesh_lengths == np.max(mesh_lengths))\n",
    "largest_mesh = mesh_splits[largest_index][0]\n",
    "\n",
    "\n",
    "indices = [i for i, a in enumerate(output_path) if a == \"_\"]\n",
    "stripped_ending = output_path[:-4]\n",
    "largest_mesh_path = stripped_ending + \"_largest_piece.off\"\n",
    "\n",
    "largest_mesh.export(largest_mesh_path)\n",
    "print(\"done exporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+ only.\n",
    "import whole_neuron_classifier_datajoint_adapted as wcda \n",
    "wcda = reload(wcda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0017414093017578125\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2\n",
      "1\n",
      "Finished CGAL segmentation algorithm: 178.86263251304626\n",
      "2) Finished: Generating CGAL segmentation for neuron: 187.7117738723755\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the median value\n",
      "changed the max value\n",
      "changed the mean value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 22\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.18036556243896484\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 0.0003058910369873047\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.4886925220489502\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 2.908301591873169\n",
      "Returning the soma_sdf value AND the classifier\n"
     ]
    }
   ],
   "source": [
    "segment_id = int(n[\"segment_id\"])\n",
    "faces = np.array(largest_mesh.faces)\n",
    "verts = np.array(largest_mesh.vertices)\n",
    "#run the whole algorithm on the neuron to test\n",
    "verts_labels, faces_labels, soma_value,classifier = wcda.extract_branches_whole_neuron(import_Off_Flag=False,segment_id=segment_id,vertices=verts,\n",
    "                     triangles=faces,pymeshfix_Flag=False,\n",
    "                     import_CGAL_Flag=False,\n",
    "                     return_Only_Labels=True,\n",
    "                     clusters=3,\n",
    "                     smoothness=0.2,\n",
    "                    soma_only=True,\n",
    "                    return_classifier = True\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "labels_counter = Counter(faces_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "soma_faces = np.where(faces_labels == 5.0)[0]\n",
    "soma_mesh = largest_mesh.submesh([soma_faces],append=True)\n",
    "soma_mesh.export(folder_name + str(n[\"segment_id\"]) + \"_soma.off\")\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "non_soma_faces = np.where(faces_labels != 5.0)[0]\n",
    "non_soma_mesh = largest_mesh.submesh([non_soma_faces],append=True)\n",
    "non_soma_mesh.export(folder_name + str(n[\"segment_id\"]) + \"_NON_soma.off\")\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the CGAL classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24175"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_counter[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'median': 0.8127675000000001,\n",
       " 'mean': 0.7745000080098222,\n",
       " 'max': 0.890113,\n",
       " 'n_faces': 17104}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.sdf_final_dict[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.8877695 , 0.8127675 , 0.277481  , 0.2248675 , 0.189898  ,\n",
       "        0.1524405 , 0.124139  , 0.113944  , 0.106715  , 0.101004  ,\n",
       "        0.099245  , 0.09496075, 0.0878234 , 0.08633795, 0.0831796 ,\n",
       "        0.0830133 , 0.082234  , 0.0807923 , 0.08037115, 0.0788252 ,\n",
       "        0.0757313 , 0.0753867 , 0.0654246 , 0.0639267 , 0.0628743 ,\n",
       "        0.0626196 , 0.0624109 , 0.0605331 , 0.0594973 , 0.0567058 ,\n",
       "        0.0517397 , 0.05089055, 0.050627  , 0.0498154 , 0.0491784 ,\n",
       "        0.049157  , 0.0485628 , 0.04837385, 0.0482103 , 0.0481955 ,\n",
       "        0.0475172 , 0.0473088 , 0.0471514 , 0.046291  , 0.045387  ,\n",
       "        0.0439093 , 0.0431892 , 0.0431007 , 0.04263235, 0.0407222 ,\n",
       "        0.04041675, 0.0391336 , 0.03849375, 0.0383993 , 0.03797785,\n",
       "        0.03790585, 0.0379037 , 0.03788515, 0.0378785 , 0.03747355,\n",
       "        0.0372941 , 0.03635565, 0.0355824 , 0.0351981 , 0.0346587 ,\n",
       "        0.0336658 , 0.03292505]),\n",
       " array([22, 39, 21, 44, 43, 30, 33, 53,  7, 52, 26, 20, 63, 66, 40, 16, 24,\n",
       "        64, 62, 54, 15, 45, 60, 58, 57, 38, 23, 12, 59, 61,  9, 36, 29, 28,\n",
       "        41,  3, 47, 11,  1,  4, 35, 13, 14,  8, 25,  6, 65, 50, 31,  5, 10,\n",
       "        49,  0, 18, 46,  2, 27, 51, 32, 48, 17, 19, 55, 56, 42, 34, 37]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_values = np.array([v[\"median\"] for k,v in classifier.sdf_final_dict.items()])\n",
    "segmentation = np.array([k for k,v in classifier.sdf_final_dict.items()])\n",
    "\n",
    "#order the compartments by greatest to smallest\n",
    "sorted_medians = np.flip(np.argsort(median_values))\n",
    "median_values[sorted_medians],segmentation[sorted_medians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4e753173f3437f8e645b53fa874d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize the soma in ipyvolume\n",
    "import ipyvolume as ipv\n",
    "import matplotlib.colors as cl\n",
    "fig_2 = ipv.figure(figsize=(15,15))\n",
    "\n",
    "main_mesh = largest_mesh\n",
    "\n",
    "default_color = \"green\"\n",
    "\n",
    "interest_label = [22,39,21]\n",
    "color = [\"red\",\"red\",\"red\"]\n",
    "\n",
    "#interest_label = []\n",
    "#color = []\n",
    "\n",
    "labels_list = classifier.labels_list\n",
    "\n",
    "\n",
    "mesh = ipv.plot_trisurf(main_mesh.vertices[:,0],\n",
    "                   main_mesh.vertices[:,1],\n",
    "                   main_mesh.vertices[:,2],\n",
    "                    triangles=main_mesh.faces\n",
    "                   )\n",
    "mesh.color = cl.to_rgb(default_color)\n",
    "\n",
    "\n",
    "volume_max = np.max(main_mesh.vertices,axis=0)\n",
    "volume_min = np.min(main_mesh.vertices,axis=0)\n",
    "\n",
    "ranges = volume_max - volume_min\n",
    "index = [0,1,2]\n",
    "max_index = np.argmax(ranges)\n",
    "min_limits = [0,0,0]\n",
    "max_limits = [0,0,0]\n",
    "\n",
    "buffer = 10000\n",
    "for i in index:\n",
    "    if i == max_index:\n",
    "        min_limits[i] = volume_min[i] - buffer\n",
    "        max_limits[i] = volume_max[i] + buffer \n",
    "        continue\n",
    "    else:\n",
    "        difference = ranges[max_index] - ranges[i]\n",
    "        min_limits[i] = volume_min[i] - difference/2  - buffer\n",
    "        max_limits[i] = volume_max[i] + difference/2 + buffer\n",
    "\n",
    "#ipv.xyzlim(-2, 2)\n",
    "ipv.xlim(min_limits[0],max_limits[0])\n",
    "ipv.ylim(min_limits[1],max_limits[1])\n",
    "ipv.zlim(min_limits[2],max_limits[2])\n",
    "\n",
    "ipv.style.set_style_light()\n",
    "#ipv.style.box_off()\n",
    "#ipv.style.axes_off()\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_counter = Counter(classifier.labels_list)\n",
    "significant_mesh_pieces = [k for k,v in labels_counter.items() if v > 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 8, 10, 12, 13, 14, 17, 20, 22, 26, 34, 37, 39, 46, 50, 58]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_mesh_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0639267"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_values[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'median': 0.0639267,\n",
       " 'mean': 0.07691820562313163,\n",
       " 'max': 0.2178,\n",
       " 'n_faces': 19402}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.sdf_final_dict[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_soma_mesh = largest_mesh.submesh(np.where(labels_list == 58),append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside\n",
      "current_mesh size = 10099\n",
      "inside\n",
      "current_mesh size = 8614\n",
      "inside\n",
      "current_mesh size = 9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/traittypes/traittypes.py:101: UserWarning: Given trait value dtype \"float64\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  np.dtype(self.dtype).name))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f37490e3d24a36b1c9674417d1b96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize the soma in ipyvolume\n",
    "import ipyvolume as ipv\n",
    "import matplotlib.colors as cl\n",
    "\n",
    "fig1 = ipv.figure(figsize=(15,15))\n",
    "\n",
    "main_mesh = largest_mesh\n",
    "\n",
    "default_color = \"green\"\n",
    "\n",
    "\n",
    "interest_label = [22,39, 58]\n",
    "color = [\"red\"]*len(interest_label)\n",
    "\n",
    "#interest_label = []\n",
    "#color = []\n",
    "\n",
    "labels_list = classifier.labels_list\n",
    "\n",
    "\"\"\"\n",
    "#This method did not work for coloring the faces\n",
    "#compute the colors_list\n",
    "current_color_list = np.tile(np.array(cl.to_rgb(default_color)),(len(main_mesh.faces),1))\n",
    "\n",
    "for lab,c in zip(interest_label,color):\n",
    "    faces_of_interest = np.where(labels_list == lab)[0]\n",
    "    print(\"faces_of_interest = \" + str(faces_of_interest))\n",
    "    current_color_list[faces_of_interest,:] = np.tile(np.array(cl.to_rgb(c)),(len(faces_of_interest),1))\n",
    "\n",
    "#set the figure size\n",
    "ipv.figure(figsize=(15,15))\n",
    "mesh = ipv.plot_trisurf(main_mesh.vertices[:,0],\n",
    "                       main_mesh.vertices[:,1],\n",
    "                       main_mesh.vertices[:,2],\n",
    "                        triangles=main_mesh.faces,\n",
    "                       color=current_color_list\n",
    "                       )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for lab,c in zip(interest_label,color):\n",
    "    print(\"inside\")\n",
    "    current_mesh = largest_mesh.submesh(np.where(labels_list == lab),append=True)\n",
    "    print(\"current_mesh size = \" + str(len(current_mesh.vertices)))\n",
    "    mesh_interested = ipv.plot_trisurf(current_mesh.vertices[:,0],\n",
    "                       current_mesh.vertices[:,1],\n",
    "                       current_mesh.vertices[:,2],\n",
    "                        triangles=current_mesh.faces\n",
    "                       )\n",
    "    mesh_interested.color = cl.to_rgb(c)\n",
    "\n",
    "#assemble mesh that was not in the labels of interest\n",
    "not_interest_labels = [k for k in labels_list if k not in interest_label]\n",
    "current_mesh = largest_mesh.submesh([not_interest_labels],append=True)\n",
    "mesh_not_interested = ipv.plot_trisurf(current_mesh.vertices[:,0],\n",
    "                   current_mesh.vertices[:,1],\n",
    "                   current_mesh.vertices[:,2],\n",
    "                    triangles=current_mesh.faces\n",
    "                   )\n",
    "mesh_not_interested.color = cl.to_rgb(default_color)\n",
    "\n",
    "\n",
    "volume_max = np.max(main_mesh.vertices,axis=0)\n",
    "volume_min = np.min(main_mesh.vertices,axis=0)\n",
    "\n",
    "ranges = volume_max - volume_min\n",
    "index = [0,1,2]\n",
    "max_index = np.argmax(ranges)\n",
    "min_limits = [0,0,0]\n",
    "max_limits = [0,0,0]\n",
    "\n",
    "buffer = 10000\n",
    "for i in index:\n",
    "    if i == max_index:\n",
    "        min_limits[i] = volume_min[i] - buffer\n",
    "        max_limits[i] = volume_max[i] + buffer \n",
    "        continue\n",
    "    else:\n",
    "        difference = ranges[max_index] - ranges[i]\n",
    "        min_limits[i] = volume_min[i] - difference/2  - buffer\n",
    "        max_limits[i] = volume_max[i] + difference/2 + buffer\n",
    "\n",
    "#ipv.xyzlim(-2, 2)\n",
    "ipv.xlim(min_limits[0],max_limits[0])\n",
    "ipv.ylim(min_limits[1],max_limits[1])\n",
    "ipv.zlim(min_limits[2],max_limits[2])\n",
    "\n",
    "ipv.style.set_style_light()\n",
    "#ipv.style.box_off()\n",
    "#ipv.style.axes_off()\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the center of the soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_center = soma_mesh.vertices.mean(axis=0).astype(\"float\")\n",
    "print(\"Poor man's center from just averagin vertices = \" + str(soma_center))\n",
    "print(\"Trimesh center of mass = \" + str(soma_mesh.center_mass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

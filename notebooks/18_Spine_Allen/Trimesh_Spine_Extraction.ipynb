{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import sys\n",
    "#import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import csv\n",
    "from Pathlib import Path\n",
    "\n",
    "import trimesh\n",
    "\n",
    "class SpineClassifier(object):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.mesh = None\n",
    "\n",
    "    #used for when not pulling from datajoint\n",
    "    def get_cgal_data_and_label_local(ob_name,labels_file,sdf_file):\n",
    "\n",
    "\n",
    "        triangles_labels = []\n",
    "        with open(labels_file) as csvfile:\n",
    "\n",
    "            for row in csv.reader(csvfile):\n",
    "                triangles_labels.append(int(row[0])) \n",
    "\n",
    "\n",
    "        ob = bpy.context.object\n",
    "\n",
    "\n",
    "        me = ob.data\n",
    "\n",
    "        #print(\"starting to hide everything\")\n",
    "        #iterate through all of the vertices\n",
    "        verts_raw = ob.data.vertices\n",
    "        #print(len(active_verts_raw))\n",
    "\n",
    "        edges_raw = ob.data.edges\n",
    "\n",
    "        #print(len(active_edges_raw))\n",
    "\n",
    "        faces_raw = ob.data.polygons\n",
    "\n",
    "        #gets a list of the unique labels\n",
    "        unique_segments = list(Counter(triangles_labels).keys())\n",
    "\n",
    "\n",
    "        segmentation_length = len(unique_segments) # equals to list(set(words))\n",
    "        #print(segmentation_length)\n",
    "\n",
    "        #makes a dictionary that maps the unique segments to a number from range(0,len(unique_seg))\n",
    "        unique_index_dict = {unique_segments[x]:x for x in range(0,segmentation_length)}\n",
    "\n",
    "\n",
    "        #print(\"unique_index_dict = \" + str(len(unique_index_dict)))\n",
    "        #print(\"triangle_labels = \" + str(len(triangles_labels)))\n",
    "        #adds all of the labels to the faces\n",
    "        max_length = len(triangles_labels)\n",
    "\n",
    "        #just iterate and add them to the faces\n",
    "        #here is where need to get stats for sdf numbers\n",
    "\n",
    "\n",
    "        labels_list = []\n",
    "        for tri in triangles_labels:\n",
    "\n",
    "            #assembles the label list that represents all of the faces\n",
    "            labels_list.append(str(unique_index_dict[tri])) \n",
    "\n",
    "\n",
    "\n",
    "        #make sure in solid mode\n",
    "        for area in bpy.context.screen.areas: # iterate through areas in current screen\n",
    "            if area.type == 'VIEW_3D':\n",
    "                for space in area.spaces: # iterate through spaces in current VIEW_3D area\n",
    "                    if space.type == 'VIEW_3D': # check if space is a 3D view\n",
    "                        space.viewport_shade = 'SOLID' # set the viewport shading to rendered\n",
    "\n",
    "        bpy.ops.object.mode_set(mode='OBJECT')\n",
    "\n",
    "        #these variables are set in order to keep the functions the same as FINAL_importing_auto_seg.py\n",
    "        newname = ob.name\n",
    "        print(\"done with cgal_segmentation\")\n",
    "\n",
    "        #----------------------now return a dictionary of the sdf values like in the older function get_sdf_dictionary\n",
    "        #get the sdf values and store in sdf_labels\n",
    "        sdf_labels = []\n",
    "        with open(sdf_file) as csvfile:\n",
    "\n",
    "            for row in csv.reader(csvfile):\n",
    "                sdf_labels.append(float(row[0])) \n",
    "\n",
    "\n",
    "        sdf_temp_dict = {}\n",
    "        labels_seen = []\n",
    "        #iterate through the labels_list\n",
    "        for i,label in enumerate(labels_list):\n",
    "            if label not in labels_seen:\n",
    "                labels_seen.append(label)\n",
    "                sdf_temp_dict[label] = []\n",
    "\n",
    "            sdf_temp_dict[label].append(sdf_labels[i])\n",
    "        #print(sdf_temp_dict)\n",
    "\n",
    "        #now calculate the stats on the sdf values for each label\n",
    "        sdf_final_dict = {}\n",
    "        sdf_final_dict_list = {}\n",
    "        for dict_key,value in sdf_temp_dict.items():\n",
    "\n",
    "            #just want to store the median\n",
    "\n",
    "            sdf_final_dict_list[dict_key] = [np.min(value),np.median(value),np.max(value),np.percentile(value,10),np.percentile(value,90),np.std(value),\n",
    "                                        np.percentile(value,80),np.percentile(value,70),len(value)]\n",
    "            sdf_final_dict[dict_key] = np.median(value)\n",
    "\n",
    "        #print these values to files\n",
    "\n",
    "\n",
    "        return sdf_final_dict, labels_list, sdf_final_dict_list\n",
    "\n",
    "    def find_neighbors(labels_list,current_label,verts_to_Face,faces_raw,verts_raw):\n",
    "        \"\"\"will return the number of neighbors that border the segment\"\"\"\n",
    "\n",
    "        #iterate over each face with that label\n",
    "        #   get the vertices of that face\n",
    "        #   get all the faces that have that vertice associated with that\n",
    "        #   get the labels of all of the neighbor faces, for each of these labels, add it to the neighbors \n",
    "        #list if it is not already there and doesn't match the label you are currently checking\n",
    "        #   return the list \n",
    "\n",
    "        #get the indexes of all of the faces with that label that you want to find the neighbors for\n",
    "\n",
    "        #OPTOMIZE\n",
    "        index_list = []\n",
    "        for i,x in enumerate(labels_list):\n",
    "            if x == current_label:\n",
    "                index_list.append(i)\n",
    "\n",
    "        verts_checked = [] #vertices checked\n",
    "        faces_checked = [] #keeps track of the faces checked\n",
    "        neighbors_list = [] #list of all neighboring labels\n",
    "        neighbors_shared_vert = {} #keeps track of total number of faces that are bordering label we care about for a particular neighbor label\n",
    "        for index in index_list:\n",
    "            #gets face\n",
    "            current_face = faces_raw[index]\n",
    "\n",
    "            #get the vertices associates with face\n",
    "            vertices = current_face.vertices\n",
    "\n",
    "            #get the faces associated with the vertices of that specific face\n",
    "\n",
    "            for vert in vertices:\n",
    "                added_this_round = []\n",
    "                #will only check each vertex once\n",
    "                if vert not in verts_checked:\n",
    "                    verts_checked.append(vert)\n",
    "                    faces_associated_vert = verts_to_Face[vert]\n",
    "                    for fac in faces_associated_vert:\n",
    "                        #make sure it is not a fellow face with the label who we are looking for the neighbors of\n",
    "                        if (fac not in index_list):\n",
    "                            #check to see if checked the the face already\n",
    "                            if (fac not in faces_checked):\n",
    "                                if(labels_list[fac] not in neighbors_list):\n",
    "                                    #add the vertex to the count of shared vertices\n",
    "                                    neighbors_shared_vert[labels_list[fac]] = 0 \n",
    "                                    #only store the faces that are different\n",
    "                                    neighbors_list.append(labels_list[fac])\n",
    "                                    #faces_to_check.append(fac)\n",
    "                                    #faces_to_check.insert(0, fac)\n",
    "                                #increment the number of times we have seen that label face\n",
    "                                if labels_list[fac] not in added_this_round:\n",
    "                                    neighbors_shared_vert[labels_list[fac]] = neighbors_shared_vert[labels_list[fac]] + 1 #OPTOMIZE\n",
    "                                    added_this_round.append(labels_list[fac])\n",
    "                                #now add the face to the checked list\n",
    "                                faces_checked.append(fac)\n",
    "\n",
    "\n",
    "        #number of faces of the label we care about\n",
    "        number_of_faces = len(index_list)\n",
    "\n",
    "\n",
    "        #Description of Return List:\n",
    "        #1) neighbors_list = labels of all bordering neighbors\n",
    "        #2) neighbors_shared_vert = number of faces for each bordering neighbor\n",
    "        #3) number_of_faces = total number of faces for current label\n",
    "        return neighbors_list,neighbors_shared_vert,number_of_faces\n",
    "\n",
    "\n",
    "    \n",
    "    #generates the stats: connections on who it is connected to), shared_verts (how many vertices it shares between it's neighbor), mesh_number (number of face for that label)\n",
    "    def export_connection(labels_list,label_name, verts_to_Face,outputFlag=\"False\",file_name=\"None\"):\n",
    "\n",
    "        #print(\"hello from export_connection with label_name = \" + str(label_name) )\n",
    "        #find all the neighbors of the label\n",
    "\n",
    "        currentMode = bpy.context.object.mode\n",
    "\n",
    "        bpy.ops.object.mode_set(mode='OBJECT')\n",
    "        ob = bpy.context.object\n",
    "        ob.update_from_editmode()\n",
    "\n",
    "        #print(\"object_name = \" + bpy.context.object.name)\n",
    "        me = ob.data\n",
    "\n",
    "        faces_raw = me.polygons\n",
    "        verts_raw = me.vertices\n",
    "\n",
    "        #print(\"generating list in export connections\")\n",
    "        #labels_list = generate_labels_list(faces_raw)\n",
    "        #print(\"done generating list in export connections\")\n",
    "\n",
    "\n",
    "        #need to assemble a dictionary that relates vertices to faces\n",
    "        #*****making into a list if the speed is too slow*******#\n",
    "        #print(\"about to making verts_to_Face\")\n",
    "        #verts_to_Face = generate_verts_to_face_dictionary(faces_raw,verts_raw)\n",
    "        #print(\"DONE about to making verts_to_Face\")\n",
    "\n",
    "        total_labels_list = []\n",
    "        faces_checked = []\n",
    "        faces_to_check = [label_name]\n",
    "\n",
    "        still_checking_faces = True\n",
    "\n",
    "        connections = {}\n",
    "        shared_vertices = {}\n",
    "        mesh_number = {}\n",
    "\n",
    "        #print(\"about to start checking faces\")\n",
    "\n",
    "        #will iterate through all of the labels with the label name until find all of the neighbors (until hitting the backbone) of the label\n",
    "        while still_checking_faces:\n",
    "            #will exit if no more faces to check\n",
    "            if not faces_to_check:\n",
    "                still_checking_faces = False\n",
    "                break\n",
    "\n",
    "            for facey in faces_to_check:\n",
    "                if facey != \"backbone\":\n",
    "                    neighbors_list,neighbors_shared_vert,number_of_faces = find_neighbors(labels_list,facey,verts_to_Face,faces_raw,verts_raw)\n",
    "\n",
    "\n",
    "\n",
    "                    #reduce the shared vertices with a face and the backbone to 0 so doesn't mess up the shared vertices percentage\n",
    "                    pairs = list(neighbors_shared_vert.items())\n",
    "                    pre_connections = [k for k,i in pairs]\n",
    "                    pre_shared_vertices = [i for k,i in pairs]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    if (\"backbone\" in pre_connections):\n",
    "                        back_index = pre_connections.index(\"backbone\")\n",
    "                        pre_shared_vertices[back_index] = 0\n",
    "\n",
    "\n",
    "                    connections[facey] = pre_connections\n",
    "                    shared_vertices[facey] = pre_shared_vertices\n",
    "                    mesh_number[facey] = number_of_faces\n",
    "\n",
    "\n",
    "                    for neighbors in neighbors_list:\n",
    "                        if (neighbors != \"backbone\") and (neighbors not in faces_to_check) and (neighbors not in faces_checked):\n",
    "                            faces_to_check.append(neighbors)\n",
    "\n",
    "                    faces_to_check.remove(facey)\n",
    "                    faces_checked.append(facey)\n",
    "\n",
    "            #append the backbone to the graph structure\n",
    "            mesh_number[\"backbone\"] = 0\n",
    "\n",
    "        #print(\"faces_checked = \" + str(faces_checked))\n",
    "        #print(\"DONE about to start checking faces\")\n",
    "\n",
    "        #save off the file to an npz file\n",
    "\n",
    "\n",
    "        if(outputFlag == True):\n",
    "            complete_path = str(\"/Users/brendancelii/Google Drive/Xaq Lab/Datajoint Project/Automatic_Labelers/spine_graphs/\"+file_name)\n",
    "\n",
    "\n",
    "\n",
    "            #package up the data that would go to the database and save it locally name of the file will look something like this \"4_bcelii_2018-10-01_12-12-34\"\n",
    "        #    np.savez(\"/Users/brendancelii/Google Drive/Xaq Lab/Datajoint Project/local_neurons_saved/\"+segment_ID+\"_\"+author+\"_\"+\n",
    "        #        date_time[0:9]+\"_\"+date_time[11:].replace(\":\",\"-\")+\".npz\",segment_ID=segment_ID,author=author,\n",
    "        #\t\t\t\t\tdate_time=date_time,vertices=vertices,triangles=triangles,edges=edges,status=status)\n",
    "            np.savez(complete_path,connections=connections,shared_vertices=shared_vertices,mesh_number=mesh_number ) \n",
    "\n",
    "        return connections,shared_vertices,mesh_number\n",
    "\n",
    "\n",
    "    def relabel_segments(labels_list,current_label,new_label):\n",
    "        for i,x in enumerate(labels_list):\n",
    "            if x == current_label:\n",
    "                labels_list[i] = new_label\n",
    "\n",
    "        return labels_list\n",
    "\n",
    "    def generate_verts_to_face_dictionary(faces_raw,verts_raw):\n",
    "        verts_to_Face = {}\n",
    "\n",
    "        #initialize the lookup dictionary as empty lists\n",
    "        for pre_vertex in verts_raw:\n",
    "            verts_to_Face[pre_vertex.index] = []\n",
    "\n",
    "        #print(len(verts_raw))\n",
    "        #print(len(verts_to_Face))\n",
    "        #print(verts_to_Face[1])\n",
    "\n",
    "        for face in faces_raw:\n",
    "            #get the vertices\n",
    "            verts = face.vertices\n",
    "            #add the index to the list for each of the vertices\n",
    "            for vertex in verts:\n",
    "                verts_to_Face[vertex].append(face.index)\n",
    "\n",
    "        return verts_to_Face\n",
    "\n",
    "\n",
    "    ##Functins from the auto_spine_labeler\n",
    "    def smooth_backbone_vp4(labels_list,sdf_final_dict,backbone_width_threshold = 0.35,max_backbone_threshold = 400,backbone_threshold=300,shared_vert_threshold=25,shared_vert_threshold_new = 5,backbone_neighbor_min=10,number_Flag = False, seg_numbers=1,smooth_Flag=True):\n",
    "        print(\"at beginning of smooth backbone vp4\")\n",
    "        #things that could hint to backbone\n",
    "        #1) larger size\n",
    "        #2) touching 2 or more larger size\n",
    "        #have to go into object mode to do some editing\n",
    "        currentMode = bpy.context.object.mode\n",
    "\n",
    "        bpy.ops.object.mode_set(mode='OBJECT')\n",
    "        ob = bpy.context.object\n",
    "        ob.update_from_editmode()\n",
    "\n",
    "        #print(\"object_name = \" + bpy.context.object.name)\n",
    "        me = ob.data\n",
    "\n",
    "        #print(\"about to get faces_verts raw\")\n",
    "        faces_raw = me.polygons\n",
    "        verts_raw = me.vertices\n",
    "        #print(\"DONE about to get faces_verts raw\")\n",
    "\n",
    "        #print(\"don't need to generate labels_list anymore\")\n",
    "        #print(\"about to generate labels_list\")   ####!!!! This takes a good bit of time#####\n",
    "        #labels_list = generate_labels_list(faces_raw)\n",
    "        #print(\"DONE about to generate labels_list\")\n",
    "\n",
    "        #need to assemble a dictionary that relates vertices to faces\n",
    "        #*****making into a list if the speed is too slow*******#\n",
    "\n",
    "\n",
    "\n",
    "        #print(\"about to generate verts_to_Face\")\n",
    "        verts_to_Face = generate_verts_to_face_dictionary(faces_raw,verts_raw)\n",
    "        #print(\"DONE about to generate verts_to_Face\")\n",
    "        #add new color and reassign all of the labels with those colors as the backbone label\n",
    "\n",
    "        #create a list of all the labels and which ones are the biggest ones\n",
    "        from collections import Counter\n",
    "\n",
    "\n",
    "        myCounter = Counter(labels_list)\n",
    "\n",
    "        spine_labels = []\n",
    "        backbone_labels = []\n",
    "\n",
    "        #put groups that have number of faces > max_backbone_threshold into backbone list\n",
    "        for label,times in myCounter.items():\n",
    "            if(times >= max_backbone_threshold):\n",
    "                #print(str(label) + \":\" + str(times))\n",
    "                backbone_labels.append(label)   \n",
    "\n",
    "        #put groups that have median sdf value > backbone_width_threshold AND greater than backbone_threshold into backbone list\n",
    "        for label in myCounter.keys():\n",
    "            if( sdf_final_dict[label] >= backbone_width_threshold):\n",
    "                #print(str(label) + \":\" + str(times))\n",
    "                if(myCounter[label] > backbone_threshold) and (label not in backbone_labels):\n",
    "                    backbone_labels.append(label)   \n",
    "        #print(\" DONE about to get counter list\")\n",
    "\n",
    "\n",
    "        to_remove = []\n",
    "\n",
    "        backbone_neighbors_dict = {}\n",
    "\n",
    "        #beginning smoothing round that removes ones from backbone list\n",
    "        for i in range(0,5):\n",
    "            print(\"smoothing round \" + str(i+1))\n",
    "            printout_counter = 0\n",
    "            counter = 0\n",
    "            #iterates through all the groups that were designated as backbones\n",
    "            for bkbone in backbone_labels:\n",
    "                if bkbone not in to_remove: #if not already designated to be removed\n",
    "\n",
    "                    if bkbone not in backbone_neighbors_dict.keys(): #if haven't already found the neighbors for that label\n",
    "                        #find_neighbors Description of Return List:\n",
    "                        #1) neighbors_list = labels of all bordering neighbors\n",
    "                        #2) neighbors_shared_vert = number of faces for each bordering neighbor\n",
    "                        #3) number_of_faces = total number of faces for current label\n",
    "                        neighbors_list,neighbors_shared_vert,number_of_faces = find_neighbors(labels_list,bkbone,verts_to_Face,faces_raw,verts_raw)\n",
    "                        #add the neighbor stats and count to the dictionary corresponding to that label\n",
    "                        backbone_neighbors_dict[bkbone] = dict(neighbors_list=neighbors_list,neighbors_shared_vert=neighbors_shared_vert,\n",
    "                            number_of_faces=number_of_faces)\n",
    "                    else:\n",
    "                        #just retrieve the neighbor stats and count of faces that are already stored in dict\n",
    "                        neighbors_list = backbone_neighbors_dict[bkbone][\"neighbors_list\"]\n",
    "                        neighbors_shared_vert = backbone_neighbors_dict[bkbone][\"neighbors_shared_vert\"]\n",
    "                        number_of_faces = backbone_neighbors_dict[bkbone][\"number_of_faces\"]\n",
    "\n",
    "                    #counts up the number of shared vertices with backbone neighbors\n",
    "\n",
    "                    #OPTOMIZE\n",
    "                    backbone_count_flag = False\n",
    "                    neighbor_counter = 0 #TOTAL NUMBER OF BACKBONE NEIGHBORS\n",
    "                    #spine_neighbor_counter = 0\n",
    "                    total_backbone_shared_verts = 0 #TOTAL NUMBER OF FACES SHARED WITH BACKBONE\n",
    "                    for n in neighbors_list:         \n",
    "                        if (n in backbone_labels) and (n not in to_remove):\n",
    "                            neighbor_counter += 1\n",
    "                            total_backbone_shared_verts = total_backbone_shared_verts + neighbors_shared_vert[n] \n",
    "\n",
    "                    interested_id = str(997)\n",
    "                    if str(bkbone) == str(interested_id):\n",
    "                        print(interested_id + \" total_backbone_shared_verts = \" + str(total_backbone_shared_verts))\n",
    "                        print(interested_id + \" neighbor_counter = \" + str(neighbor_counter))\n",
    "\n",
    "\n",
    "\n",
    "                    #OPTOMIZE\n",
    "                    #if meets requirement of shared verts then activates flag     \n",
    "                    if (total_backbone_shared_verts > shared_vert_threshold):\n",
    "                        backbone_count_flag = True\n",
    "\n",
    "                    #if there are no neighbor's that are backbones or does not share enough backbone vertices --> remove from backbone list\n",
    "                    if neighbor_counter <= 0 or backbone_count_flag == False:\n",
    "                            to_remove.append(bkbone)\n",
    "                            counter += 1\n",
    "\n",
    "\n",
    "            #if 1 or less non-backbones were converted to remove list then go ahead to the next step\n",
    "            print(\"counter = \" + str(counter))\n",
    "            if counter <= 1:\n",
    "                print(\"counter caused the break\")\n",
    "                break\n",
    "\n",
    "        print(\"just broke out of the loop\")\n",
    "        \"\"\"\n",
    "        Status: \n",
    "        1) Started with a tentative list of backbones\n",
    "        2) Removed some potential backbone lists\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #now go through and make sure no unconnected backbone segments\n",
    "\n",
    "        \"\"\"Pseudo-code for filtering algorithm\n",
    "        1) iterate through all of the backbone labels\n",
    "        2) Go get the neighbors of the backbone\n",
    "        3) Add all of the neighbors who are too part of the backbone to the backbones to check list\n",
    "        4) While backbone neighbor counter is less than the threshold or until list to check is empty\n",
    "        5) Pop the next neighbor off the list and add it to the neighbors check list\n",
    "        6) Get the neighbors of this guy\n",
    "        7) for each of neighbors that is also on the backbone BUT HASN'T BEEN CHECKED YET append them to the list to be check and update counter\n",
    "        8) continue at beginning of loop\n",
    "        -- once loop breaks\n",
    "        9) if the counter is below the threshold:\n",
    "            Add all of values in the neighbros already checked list to the new_to_remove\n",
    "        10) Use the new_backbone_labels and new_to_remove to rewrite the labels_list\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #gets the new backbones list without the ones removed\n",
    "        new_backbone_labels = [bkbone for bkbone in backbone_labels if bkbone not in to_remove] #OPTOMIZE\n",
    "        new_to_remove = []\n",
    "        skip_labels = []\n",
    "\n",
    "        print(\"new_backbone_labels lenght = \" + str(len(new_backbone_labels)))\n",
    "\n",
    "        for bkbonz in new_backbone_labels:\n",
    "            if bkbonz not in skip_labels:\n",
    "                #print(\"working on backbone = \" + str(bkbonz))\n",
    "                checked_backbone_neighbors = []\n",
    "                backbone_neighbors_to_check = []\n",
    "                new_backbone_neighbor_counter = 0\n",
    "\n",
    "\n",
    "                if bkbonz not in backbone_neighbors_dict.keys(): #should never enter this loop..... #OPTOMIZE\n",
    "                    neighbors_list,neighbors_shared_vert,number_of_faces = find_neighbors(labels_list,bkbonz,verts_to_Face,faces_raw,verts_raw)\n",
    "                    backbone_neighbors_dict[bkbonz] = dict(neighbors_list=neighbors_list,neighbors_shared_vert=neighbors_shared_vert,\n",
    "                        number_of_faces=number_of_faces)\n",
    "                else: #gets the stats of the neighbors and count of current label\n",
    "                    neighbors_list = backbone_neighbors_dict[bkbonz][\"neighbors_list\"]\n",
    "                    neighbors_shared_vert = backbone_neighbors_dict[bkbonz][\"neighbors_shared_vert\"]\n",
    "                    number_of_faces = backbone_neighbors_dict[bkbonz][\"number_of_faces\"]\n",
    "\n",
    "                for bb in neighbors_list:\n",
    "                    #counts as viable backbone neighbor if meets following conditions:\n",
    "                    #1) In the new backbone list\n",
    "                    #2) hasn't been checked yet\n",
    "                    #3) not in the new ones to remove\n",
    "                    #4) The number of neighbors shared by that label is greater than raw threshold shared_vert_threshold_new\n",
    "\n",
    "                    #OPTOMIZE: don't need checked_backbone_neighbors\n",
    "                    if (bb in new_backbone_labels) and (bb not in checked_backbone_neighbors) and (bb not in new_to_remove) and neighbors_shared_vert[bb] > shared_vert_threshold_new:\n",
    "                        backbone_neighbors_to_check.append(bb)\n",
    "                        new_backbone_neighbor_counter += 1\n",
    "\n",
    "                #at this point have :\n",
    "                #1) total number of backbone neighbors: new_backbone_neighbor_counter\n",
    "                #2) backbone neighbors in list: backbone_neighbors_to_check\n",
    "\n",
    "                checked_backbone_neighbors = [nb for nb in backbone_neighbors_to_check]\n",
    "\n",
    "\n",
    "                #4) While backbone neighbor counter is less than the threshold or until list to check is empty\n",
    "\n",
    "                #Iterates through all possible backbone neighbors unitl:\n",
    "                # A) new_backbone_neighbor_counter is greater than set threshold of backbone_neighbor_min OR\n",
    "                # B) no more backbone neighbors to check\n",
    "\n",
    "                #Goal: counts the backbone chain with that label, so in hopes if not high enough then not backbone piece\n",
    "                while new_backbone_neighbor_counter < backbone_neighbor_min and backbone_neighbors_to_check != []:\n",
    "                    #5) Pop the next neighbor off the list and add it to the neighbors check list\n",
    "                    current_backbone = backbone_neighbors_to_check.pop(0)\n",
    "                    if current_backbone not in checked_backbone_neighbors:\n",
    "                        checked_backbone_neighbors.append(current_backbone) #mark it as checked\n",
    "                    #6) Get the neighbors of this guy\n",
    "                    if current_backbone not in backbone_neighbors_dict.keys(): #should already be in there\n",
    "                        neighbors_list,neighbors_shared_vert,number_of_faces = find_neighbors(labels_list,current_backbone,verts_to_Face,faces_raw,verts_raw)\n",
    "                        backbone_neighbors_dict[current_backbone] = dict(neighbors_list=neighbors_list,neighbors_shared_vert=neighbors_shared_vert,\n",
    "                            number_of_faces=number_of_faces)\n",
    "                    else: #gets the current neighbors and counts of one of the possible neighbor backbones\n",
    "                        neighbors_list = backbone_neighbors_dict[current_backbone][\"neighbors_list\"]\n",
    "                        neighbors_shared_vert = backbone_neighbors_dict[current_backbone][\"neighbors_shared_vert\"]\n",
    "                        number_of_faces = backbone_neighbors_dict[current_backbone][\"number_of_faces\"]\n",
    "\n",
    "                    #7) for each of neighbors that is also on the backbone BUT HASN'T BEEN CHECKED YET append them to the list to be check and update counter\n",
    "                    for bb in neighbors_list:\n",
    "                        if (bb in new_backbone_labels) and (bb not in checked_backbone_neighbors) and (bb not in new_to_remove) and neighbors_shared_vert[bb] > shared_vert_threshold_new:\n",
    "                            backbone_neighbors_to_check.append(bb)\n",
    "                            new_backbone_neighbor_counter += 1\n",
    "\n",
    "                #9) if the counter is below the threshold --> Add all of values in the neighbros already checked list to the new_to_remove\n",
    "                if new_backbone_neighbor_counter < backbone_neighbor_min:\n",
    "                    for bz in checked_backbone_neighbors:\n",
    "                        if bz not in new_to_remove:\n",
    "                            new_to_remove.append(bz)\n",
    "                            print(\"removed \" + str(checked_backbone_neighbors))\n",
    "                else:\n",
    "                    skip_labels = skip_labels + checked_backbone_neighbors\n",
    "\n",
    "\n",
    "\n",
    "        print(\"done Analyzing big and small segments\")        \n",
    "        #go through and switch the label of hte \n",
    "        #may not want to relabel until the end in order to preserve the labels in case label a big one wrong\n",
    "        print(\"about to rewrite the labels\")\n",
    "        for i in range(0,len(labels_list)):\n",
    "            if labels_list[i] in new_backbone_labels and labels_list[i] not in new_to_remove:\n",
    "                labels_list[i] = \"backbone\"\n",
    "                faces_raw[i].material_index = 0\n",
    "            else:\n",
    "                faces_raw[i].material_index = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"DONE about to rewrite the labels\")\n",
    "        return labels_list, verts_to_Face\n",
    "\n",
    "    def filter_Stubs(labels_list,verts_to_Face,sdf_final_dict):\n",
    "\n",
    "        myCounter = Counter(labels_list)\n",
    "        complete_labels =  [label for label,times in myCounter.items()]\n",
    "\n",
    "        stub_threshold = 50\n",
    "\n",
    "        processed_labels = []\n",
    "        for i in range(0,len(complete_labels)):\n",
    "            if complete_labels[i] != \"backbone\" and complete_labels[i] not in processed_labels:\n",
    "                #print(\"at beginning of spine labeling loop: about to enter export connection\")\n",
    "                #get the conenections, shared vertices and mesh sizes for the whole spine segment in which label is connected to\n",
    "                connections,shared_vertices,mesh_number = export_connection(labels_list,complete_labels[i], verts_to_Face,outputFlag=\"False\",file_name=\"None\")\n",
    "\n",
    "                total_mesh_faces_outer = sum([k for i,k in mesh_number.items()])\n",
    "\n",
    "                #once done all of the paths go through and label things as stubs\n",
    "                if total_mesh_faces_outer < stub_threshold:\n",
    "                    #print(\"stub threshold triggered\")\n",
    "                    for label_name in mesh_number.keys():\n",
    "                        relabel_segments(labels_list,label_name,\"backbone\")\n",
    "                for key in mesh_number.keys():\n",
    "                    processed_labels.append(key)\n",
    "\n",
    "        return labels_list\n",
    "\n",
    "\n",
    "    #from w1_Visualize_whole_pre_classify import create_local_colors\n",
    "\n",
    "    def get_spine_classification(file_location,file_name,clusters,smoothness):        \n",
    "        original_start_time = time.time()    \n",
    "        start_time = time.time()\n",
    "\n",
    "        #initialize the object\n",
    "        file_location = \"/notebooks/18_Spine_Allen\"\n",
    "        file_name = \"neuron-775959265587_part_2.off\"\n",
    "        full_path = str(Path(file_location) / Path(file_name))\n",
    "        self.mesh = trimesh.load_mesh(full_path)\n",
    "        \n",
    "\n",
    "        faces_raw = obj.data.polygons\n",
    "        #faces_raw = self.mesh.faces\n",
    "\n",
    "        #create_local_colors(obj)\n",
    "\n",
    "        print(\"loading object and box--- %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        labels_file = file_location + \"/cgal/\" + file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \".csv\" \n",
    "        sdf_file = file_location + \"/cgal/\" + file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \"_sdf.csv\"\n",
    "\n",
    "        sdf_final_dict, labels_list, sdf_final_dict_list = get_cgal_data_and_label_local(ob_name,labels_file,sdf_file)\n",
    "\n",
    "        #print out the total statistics: \n",
    "        sdf_analysis_file = file_location + \"/cgal_analysis/\" + file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \".csv\" \n",
    "        with open(sdf_analysis_file[:-4] + \"_STATS_TOTAL.csv\",mode=\"w\") as sdf_stat_file:\n",
    "            sdf_writer = csv.writer(sdf_stat_file,delimiter=\",\")\n",
    "\n",
    "\n",
    "\n",
    "            #[np.min(value),np.median(value),np.max(value),np.percentile(value,10),np.percentile(value,90),np.std(value)]\n",
    "            sdf_writer.writerow([\"label\",\"min\",\"median\",\"max\",\"10th percentile\",\"90th percentile\",\"std dev\",\"80th percentile\",\n",
    "                                    \"70th percentile\",\"NUM OF FACES\"])\n",
    "            for key,value in sdf_final_dict_list.items():\n",
    "                sdf_writer.writerow([str(key),*value])\n",
    "\n",
    "\n",
    "        #write the special files for missed\n",
    "        missed_labels = [385,761,90,593,384,537,811,287,1615,997,693]\n",
    "        with open(sdf_analysis_file[:-4] + \"_STATS_TOTAL_incorrect.csv\",mode=\"w\") as sdf_stat_file:\n",
    "            sdf_writer = csv.writer(sdf_stat_file,delimiter=\",\")\n",
    "\n",
    "            #[np.min(value),np.median(value),np.max(value),np.percentile(value,10),np.percentile(value,90),np.std(value)]\n",
    "            sdf_writer.writerow([\"label\",\"min\",\"median\",\"max\",\"10th percentile\",\"90th percentile\",\"std dev\",\"80th percentile\",\n",
    "                                    \"70th percentile\",\"NUM OF FACES\"])\n",
    "            for key in missed_labels:\n",
    "                sdf_writer.writerow([str(key),*sdf_final_dict_list[str(key)]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Done writing sdf values\")\n",
    "\n",
    "\n",
    "\n",
    "        if(sdf_final_dict == [] and labels_list == []):\n",
    "            print(\"NO CGAL DATA FOR \" + str(neuron_ID))\n",
    "\n",
    "            # deselect all\n",
    "            bpy.ops.object.select_all(action='DESELECT')\n",
    "\n",
    "            # selection\n",
    "            #for ob in bpy.data.objects\n",
    "            #bpy.data.objects[ob_name].select = True\n",
    "\n",
    "            for obj in bpy.data.objects:\n",
    "                if \"neuron\" in obj.name:\n",
    "                    obj.select = True\n",
    "\n",
    "\n",
    "            # remove it\n",
    "            bpy.ops.object.delete() \n",
    "            ##########should this be a return??#########\n",
    "            return\n",
    "\n",
    "        print(\"getting cgal data--- %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "        #complete_path = \"/Users/brendancelii/Google Drive/Xaq Lab/Final_Blender/saved_sdf/sdf_saved_off.npz\"\n",
    "        #np.savez(complete_path,labels_list=labels_list,sdf_final_dict=sdf_final_dict)\n",
    "\n",
    "\n",
    "        max_backbone_threshold = 200 #the absolute size if it is greater than this then labeled as a possible backbone\n",
    "        backbone_threshold=40 #if the label meets the width requirements, these are the size requirements as well in order to be considered possible backbone\n",
    "        shared_vert_threshold=10 #raw number of backbone verts that need to be shared in order for label to possibly be a backbone\n",
    "        shared_vert_threshold_new = 5\n",
    "        backbone_width_threshold = 0.10  #the median sdf/width value the segment has to have in order to be considered a possible backbone \n",
    "        #labels_list,verts_to_Face = smooth_backbone_vp3(labels_list,sdf_final_dict,backbone_width_threshold,max_backbone_threshold = max_backbone_threshold,backbone_threshold=backbone_threshold\n",
    "        #        ,secondary_threshold=secondary_threshold,shared_vert_threshold=shared_vert_threshold,number_Flag = False, seg_numbers=1,smooth_Flag=True)\n",
    "\n",
    "        old_labels_list = labels_list.copy()\n",
    "        backbone_neighbor_min=10 # number of backbones in chain in order for label to keep backbone status\n",
    "        labels_list,verts_to_Face = smooth_backbone_vp4(labels_list,sdf_final_dict,backbone_width_threshold,max_backbone_threshold = max_backbone_threshold,backbone_threshold=backbone_threshold,\n",
    "                shared_vert_threshold=shared_vert_threshold,\n",
    "                shared_vert_threshold_new = shared_vert_threshold_new,\n",
    "                 backbone_neighbor_min=backbone_neighbor_min)\n",
    "\n",
    "        #assemble the number of labels that are labeled backbone\n",
    "\n",
    "        shaft_list = list(Counter(np.array(old_labels_list)[np.where(np.array(labels_list)==\"backbone\")[0].tolist()]).keys())\n",
    "\n",
    "        shaft_list_final = [k for k in shaft_list if k not in missed_labels]\n",
    "\n",
    "        with open(sdf_analysis_file[:-4] + \"_STATS_TOTAL_shaft.csv\",mode=\"w\") as sdf_stat_file:\n",
    "            sdf_writer = csv.writer(sdf_stat_file,delimiter=\",\")\n",
    "\n",
    "            #[np.min(value),np.median(value),np.max(value),np.percentile(value,10),np.percentile(value,90),np.std(value)]\n",
    "            sdf_writer.writerow([\"label\",\"min\",\"median\",\"max\",\"10th percentile\",\"90th percentile\",\"std dev\",\"80th percentile\",\n",
    "                                    \"70th percentile\",\"NUM OF FACES\"])\n",
    "            for key in shaft_list_final:\n",
    "                sdf_writer.writerow([str(key),*sdf_final_dict_list[key]])\n",
    "\n",
    "        print(\"smoothing backbone--- %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "        #smooth away the stubs\n",
    "        labels_list = filter_Stubs(labels_list,verts_to_Face,sdf_final_dict)\n",
    "\n",
    "        print(\"about to rewrite the labels\")\n",
    "        for i in range(0,len(labels_list)):\n",
    "            if labels_list[i] == \"backbone\":\n",
    "                faces_raw[i].material_index = 0\n",
    "            else:\n",
    "                faces_raw[i].material_index = 2\n",
    "\n",
    "\n",
    "\n",
    "        print(\"finished\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - original_start_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import sys\n",
    "#import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import trimesh\n",
    "\n",
    "class SpineClassifier(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mesh = None\n",
    "    \n",
    "    def get_spine_classification(self,file_location,file_name,clusters,smoothness):        \n",
    "        original_start_time = time.time()    \n",
    "        start_time = time.time()\n",
    "\n",
    "        #initialize the object\n",
    "        #file_location = \"/notebooks/18_Spine_Allen\"\n",
    "        #file_name = \"neuron-775959265587_part_2.off\"\n",
    "        full_path = str(Path(file_location) / Path(file_name))\n",
    "        self.mesh = trimesh.load_mesh(full_path)\n",
    "        \n",
    "        \n",
    "        #faces_raw = obj.data.polygons\n",
    "        faces_raw = self.mesh.faces\n",
    "        \n",
    "        print(faces_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98607 43739 83871]\n",
      " [98607 83871 92015]\n",
      " [98607 92015 78666]\n",
      " ...\n",
      " [  937 16959  1501]\n",
      " [  937 20654 16959]\n",
      " [ 2760  4199  4194]]\n"
     ]
    }
   ],
   "source": [
    "file_name = \"neuron-775959265587_part_2.off\"\n",
    "file_location = \"/notebooks/18_Spine_Allen/neurons\"\n",
    "\n",
    "clusters = \"20\"\n",
    "smoothness = \"0.04\"\n",
    " \n",
    "myClassifier = SpineClassifier()\n",
    "myClassifier.get_spine_classification(file_location,file_name,clusters,smoothness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

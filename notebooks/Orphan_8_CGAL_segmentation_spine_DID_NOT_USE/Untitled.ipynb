{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341352006, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 50802\n",
      "648518346341352006 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341352006_Dendrite_0\n",
      "25449 50802\n",
      "finished\n",
      "--- 13.841903686523438 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341352223, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 77823\n",
      "648518346341352223 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341352223_Dendrite_0\n",
      "39026 77823\n",
      "finished\n",
      "--- 20.48962092399597 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341353019, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 17592\n",
      "648518346341353019 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341353019_Dendrite_0\n",
      "8805 17592\n",
      "finished\n",
      "--- 3.943354606628418 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341353186, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 86889\n",
      "648518346341353186 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341353186_Dendrite_0\n",
      "43630 86889\n",
      "finished\n",
      "--- 23.44841456413269 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341353574, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Basal', 'component_index': 0}\n",
      "component_size = 1742\n",
      "648518346341353574 type:Basal index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341353574_Basal_0\n",
      "891 1742\n",
      "finished\n",
      "--- 1.1137659549713135 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341353574, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Basal', 'component_index': 1}\n",
      "component_size = 90304\n",
      "648518346341353574 type:Basal index:1 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341353574_Basal_1\n",
      "45239 90304\n",
      "finished\n",
      "--- 26.373146057128906 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341353607, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 136186\n",
      "648518346341353607 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341353607_Dendrite_0\n",
      "68234 136186\n",
      "finished\n",
      "--- 48.634079456329346 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341353788, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 19506\n",
      "648518346341353788 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341353788_Dendrite_0\n",
      "9789 19506\n",
      "finished\n",
      "--- 5.933373928070068 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341353883, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 77210\n",
      "648518346341353883 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341353883_Dendrite_0\n",
      "38697 77210\n",
      "finished\n",
      "--- 25.303845643997192 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341354048, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 6435\n",
      "648518346341354048 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341354048_Dendrite_0\n",
      "3245 6435\n",
      "finished\n",
      "--- 2.04504132270813 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341354962, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 55519\n",
      "648518346341354962 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341354962_Dendrite_0\n",
      "27844 55519\n",
      "finished\n",
      "--- 20.40753674507141 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341355539, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 143751\n",
      "648518346341355539 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341355539_Dendrite_0\n",
      "71989 143751\n",
      "finished\n",
      "--- 158.63465189933777 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341404619, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 25978\n",
      "648518346341404619 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341404619_Dendrite_0\n",
      "13007 25978\n",
      "finished\n",
      "--- 24.060142278671265 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346341410600, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 198208\n",
      "648518346341410600 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346341410600_Dendrite_0\n",
      "99366 198208\n",
      "finished\n",
      "--- 320.9515483379364 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346344543072, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 15020\n",
      "648518346344543072 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346344543072_Dendrite_0\n",
      "7539 15020\n",
      "finished\n",
      "--- 18.440587282180786 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346346592937, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 65394\n",
      "648518346346592937 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346346592937_Dendrite_0\n",
      "32715 65394\n",
      "finished\n",
      "--- 78.48540544509888 seconds ---\n",
      "key = {'segmentation': 2, 'segment_id': 648518346349478471, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Dendrite', 'component_index': 0}\n",
      "component_size = 49404\n",
      "648518346349478471 type:Dendrite index:0 cluster:12 smoothness:0.04\n",
      "Done making OFF file neuron_648518346349478471_Dendrite_0\n",
      "24735 49404\n",
      "finished\n",
      "--- 52.21714162826538 seconds ---\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import cgal_Segmentation_Module as csm\n",
    "#help(csm)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#####2 goals\n",
    "\"\"\"1)  create the final datajoint tables to store the final spine meshs in:\n",
    "    a. One table to store the segment data (linked to the components table)\n",
    "    b. One table to store the labeled mesh vertices and faces of the neurons (linked to CleanseMesh)\n",
    "2) Create the function that goes through and writes the CGAL library for all of the components\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"Pseduo code for function that goes through and writes the CGAL library for all of the components\n",
    "\n",
    "1) Recieve list of neurons to do and a flag that when set will look to\n",
    "    another table for the clustering parameter for each neuron\n",
    "2) Pull down the neurons mesh data from cleansedMesh\n",
    "3) Pull down all the components with the neuron ID that have size > 100\n",
    "4) For each component:\n",
    "5) Generate the off file:\n",
    "    ---------------Way I do it in the blender file----------------\n",
    "    In load_Neuron_automatic_spine, download whole mesh\n",
    "    a. Before create the mesh object send faces and verts to filter_verts_and_faces\n",
    "    b. filter_verts_and_faces:\n",
    "        downloads the indexes for the compartment\n",
    "        Only saves off the verts that are mentioned in the indexes\n",
    "        Only saves off the faces that are mentioned in the indexes\n",
    "            returns them\n",
    "    c. builds the off file by:\n",
    "        Finding the faces that have all indices included in the verts list\n",
    "        finish with the write_Part_Neuron_Off_file\n",
    "    -------------------\n",
    "    1. create the file name string: \"neuron_\" + str(segment_id) + \"_\" + str(compartment_type_name) + \"_\" + str(found_component_index)\n",
    "    2. get the number of indices and faces\n",
    "    3. Open them and write them to the file\n",
    "    4. For the vertices:\n",
    "        For each index in the vertices blob of the components table, \n",
    "         write the coordinates in the index location of the Cleansed mesh table\n",
    "            while creating a lookup dictionary where it has old_vert_index:new_index  (vert_lookup)\n",
    "    5. For the faces:\n",
    "        For each index in the faces blob of the components table,\n",
    "             Get the new index by (vert_lookup) and save to list\n",
    "        Write the list to the file\n",
    "    \n",
    "    Call the CGAL function to generate the labels\n",
    "    String calculat the CGAL file name and the CGAL SDF value \n",
    "    Write the two lists to the datajoint table (linked to components)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "#from cloudvolume import CloudVolume\n",
    "#from collections import Counter\n",
    "#from funconnect import ta3\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#setting the address and the username\n",
    "dj.config['database.host'] = '10.28.0.34'\n",
    "dj.config['database.user'] = 'celiib'\n",
    "dj.config['database.password'] = 'newceliipass'\n",
    "dj.config['safemode']=True\n",
    "dj.config[\"display.limit\"] = 20\n",
    "\n",
    "\n",
    "# user: celiib\n",
    "# pass: newceliipass\n",
    "# host: at-database.ad.bcm.edu\n",
    "# schemas: microns_% and celiib_%\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "schema = dj.schema('microns_ta3p100')\n",
    "ta3p100 = dj.create_virtual_module('ta3p100', 'microns_ta3p100')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#if temp folder doesn't exist then create it\n",
    "import os\n",
    "if (os.path.isdir(os.getcwd() + \"/temp\")) == False:\n",
    "    os.mkdir(\"temp\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "def generate_component_off_file(neuron_ID, compartment_type, component_id, n_vertex_indices, n_triangle_indices, \n",
    "                                vertex_indices, triangle_indices,vertices, triangles):\n",
    "    \n",
    "    #get the current file location\n",
    "    file_loc = pathlib.Path.cwd() / \"temp\"\n",
    "    filename = \"neuron_\" + str(neuron_ID) + \"_\" + str(compartment_type) + \"_\" + str(component_id)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(n_vertex_indices) + \" \" + str(n_triangle_indices) + \" 0\\n\" )\n",
    "    \n",
    "    #start writing all of the vertices\n",
    "    \"\"\"\n",
    "        4. For the vertices:\n",
    "        For each index in the vertices blob of the components table, \n",
    "         write the coordinates in the index location of the Cleansed mesh table\n",
    "            while creating a lookup dictionary where it has old_vert_index:new_index  (vert_lookup)\n",
    "    \"\"\"       \n",
    "    verts_lookup = {}\n",
    "    for i, vin in enumerate(vertex_indices):\n",
    "        #get the coordinates of the vertex\n",
    "        coordinates = vertices[vin]\n",
    "        #write the coordinates to the off file\n",
    "        f.write(str(coordinates[0]) + \" \" + str(coordinates[1]) + \" \" + str(coordinates[2])+\"\\n\")\n",
    "        #create lookup dictionary for vertices\n",
    "        verts_lookup[vin] = i\n",
    "    \n",
    "    \"\"\"    5. For the faces:\n",
    "        For each index in the faces blob of the components table,\n",
    "             Get the new index by (vert_lookup) and save to list\n",
    "        Write the list to the file\"\"\"\n",
    "    for i,fac in enumerate(triangle_indices):\n",
    "        verts_in_fac = triangles[fac]\n",
    "        #write the verties to the off file\n",
    "        f.write(\"3 \" + str(verts_lookup[verts_in_fac[0]]) + \" \" + str(verts_lookup[verts_in_fac[1]]) + \" \" + str(verts_lookup[verts_in_fac[2]])+\"\\n\")\n",
    "        \n",
    "    \n",
    "    print(\"Done making OFF file \" + str(filename))\n",
    "    #return the name of the off file you created and the location\n",
    "    return str(path_and_filename),str(filename)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#################THE ONE WE ARE USING\n",
    "import cgal_Segmentation_Module as csm\n",
    "import csv\n",
    "import decimal\n",
    "import time\n",
    "import os\n",
    "\n",
    "@schema\n",
    "class ComponentAutoSegmentOrphan(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    # creates the labels for the mesh table\n",
    "    -> ta3p100.CompartmentOrphan.ComponentOrphan\n",
    "    clusters     : tinyint unsigned  #what the clustering parameter was set to\n",
    "    smoothness   : decimal(3,2)             #what the smoothness parameter was set to, number betwee 0 and 1\n",
    "    ---\n",
    "    n_triangles  : int unsigned # number of faces\n",
    "    seg_group    : longblob     # group segmentation ID's for faces from automatic CGAL segmentation\n",
    "    sdf          : longblob     #  width values for faces from from automatic CGAL segmentation\n",
    "    median_sdf   : decimal(6,5) # the median width value for the sdf values\n",
    "    mean_sdf     : decimal(6,5) #the mean width value for the sdf values\n",
    "    third_q      : decimal(6,5) #the upper quartile for the mean width values\n",
    "    ninety_perc  : decimal(6,5) #the 90th percentile for the mean width values\n",
    "    time_updated : timestamp    # the time at which the segmentation was performed\n",
    "   \"\"\"\n",
    "    \n",
    "    key_source = ta3p100.CompartmentOrphan.ComponentOrphan & 'n_triangle_indices>100' & [dict(compartment_type=comp) for comp in ['Basal', 'Apical', 'Oblique', 'Dendrite']]\n",
    "    \n",
    "    whole_neuron_dicts = dict()\n",
    "    \n",
    "    def make(self, key):\n",
    "        print(\"key = \" + str(key))\n",
    "        #key passed to function is just dictionary with the following attributes\n",
    "        \"\"\"segmentation\n",
    "        segment_id\n",
    "        decimation_ratio\n",
    "        compartment_type\n",
    "        component_index\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #clusters_default = 18\n",
    "        smoothness = 0.04\n",
    "\n",
    "        Apical_Basal_Oblique_default = [12]\n",
    "        basal_big = [16]\n",
    "\n",
    "        neuron_ID = key[\"segment_id\"]\n",
    "        component = (ta3p100.CompartmentOrphan.ComponentOrphan & key).fetch1()        \n",
    "\n",
    "        component_id = component[\"component_index\"]\n",
    "        compartment_type = component[\"compartment_type\"]\n",
    "        component_size = int(component[\"n_triangle_indices\"])\n",
    "\n",
    "        print(\"component_size = \" + str(component_size))\n",
    "\n",
    "        if (compartment_type == \"Basal\") & (component_size > 160000):\n",
    "            cluster_list = basal_big\n",
    "        else:\n",
    "            cluster_list = Apical_Basal_Oblique_default\n",
    "\n",
    "\n",
    "        for clusters in cluster_list:\n",
    "            smoothness = 0.04\n",
    "            print(str(component[\"segment_id\"]) + \" type:\" + str(component[\"compartment_type\"]) \n",
    "                      + \" index:\" + str(component[\"component_index\"]) + \" cluster:\" + str(clusters) \n",
    "                  + \" smoothness:\" + str(smoothness))\n",
    "\n",
    "            #generate the off file for each component\n",
    "            #what need to send them:\n",
    "            \"\"\"----From cleansed Mesh---\n",
    "            vertices\n",
    "            triangles\n",
    "            ----From component table--\n",
    "            n_vertex_indices\n",
    "            n_triangle_indices\n",
    "            vertex_indices\n",
    "            triangle_indices\"\"\"\n",
    "            \n",
    "            if key['segment_id'] not in self.whole_neuron_dicts:\n",
    "                self.whole_neuron_dicts[key['segment_id']] = (ta3p100.CleansedMeshOrphan & 'decimation_ratio=0.35' & dict(segment_id=key['segment_id'])).fetch1()\n",
    "            \n",
    "            path_and_filename, off_file_name = generate_component_off_file(neuron_ID, compartment_type, component_id,\n",
    "                                        component[\"n_vertex_indices\"],\n",
    "                                        component[\"n_triangle_indices\"],\n",
    "                                        component[\"vertex_indices\"],\n",
    "                                        component[\"triangle_indices\"],\n",
    "                                        self.whole_neuron_dicts[key['segment_id']][\"vertices\"],\n",
    "                                        self.whole_neuron_dicts[key['segment_id']][\"triangles\"])\n",
    "            \n",
    "            print(len(component['vertex_indices']), len(component['triangle_indices']))\n",
    "            \n",
    "            #will have generated the component file by now so now need to run the segmentation\n",
    "\n",
    "            csm.cgal_segmentation(path_and_filename,clusters,smoothness)\n",
    "\n",
    "            #generate the name of the files\n",
    "            cgal_file_name = path_and_filename + \"-cgal_\" + str(clusters) + \"_\"+str(smoothness)\n",
    "            group_csv_cgal_file = cgal_file_name + \".csv\"\n",
    "            sdf_csv_file_name = cgal_file_name+\"_sdf.csv\"\n",
    "\n",
    "            \n",
    "            try:\n",
    "                with open(group_csv_cgal_file) as f:\n",
    "                  reader = csv.reader(f)\n",
    "                  your_list = list(reader)\n",
    "                group_list = []\n",
    "                for item in your_list:\n",
    "                    group_list.append(int(item[0]))\n",
    "\n",
    "                with open(sdf_csv_file_name) as f:\n",
    "                  reader = csv.reader(f)\n",
    "                  your_list = list(reader)\n",
    "                sdf_list = []\n",
    "                for item in your_list:\n",
    "                    sdf_list.append(float(item[0]))\n",
    "            except:\n",
    "                print(\"no CGAL segmentation for \" + str(off_file_name) )\n",
    "                return\n",
    "\n",
    "            #print(group_list)\n",
    "            #print(sdf_list)\n",
    "\n",
    "            #now write them to the datajoint table  \n",
    "            #table columns for ComponentAutoSegmentation: segmentation, segment_id, decimation_ratio, compartment_type, component_index, seg_group, sdf\n",
    "#             print(dict(key,\n",
    "#                                 clusters=clusters,\n",
    "#                                 smoothness=smoothness,\n",
    "#                                 n_triangles=component[\"n_triangle_indices\"],\n",
    "#                                 seg_group=group_list,\n",
    "#                                 sdf=sdf_list,\n",
    "#                                 median_sdf=np.median(sdf_list),\n",
    "#                                 mean_sdf=np.mean(sdf_list),\n",
    "#                                 third_q=np.percentile(sdf_list, 75),\n",
    "#                                 ninety_perc=np.percentile(sdf_list, 90),\n",
    "#                                 time_updated=str(datetime.datetime.now())[0:19]))\n",
    "            \n",
    "            comp_dict = dict(key,\n",
    "                                clusters=clusters,\n",
    "                                smoothness=smoothness,\n",
    "                                n_triangles=component[\"n_triangle_indices\"],\n",
    "                                seg_group=group_list,\n",
    "                                sdf=sdf_list,\n",
    "                                median_sdf=np.median(sdf_list),\n",
    "                                mean_sdf=np.mean(sdf_list),\n",
    "                                third_q=np.percentile(sdf_list, 75),\n",
    "                                ninety_perc=np.percentile(sdf_list, 90),\n",
    "                                time_updated=str(datetime.datetime.now())[0:19])\n",
    "\n",
    "            self.insert1(comp_dict)\n",
    "\n",
    "            #then go and erase all of the files used: the sdf files, \n",
    "            real_off_file_name = path_and_filename + \".off\"\n",
    "\n",
    "            files_to_delete = [group_csv_cgal_file,sdf_csv_file_name,real_off_file_name]\n",
    "            for fl in files_to_delete:\n",
    "                if os.path.exists(fl):\n",
    "                    os.remove(fl)\n",
    "                else:\n",
    "                    print(fl + \" file does not exist\")\n",
    "\n",
    "        print(\"finished\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ComponentAutoSegmentOrphan.populate(reserve_jobs=True)\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

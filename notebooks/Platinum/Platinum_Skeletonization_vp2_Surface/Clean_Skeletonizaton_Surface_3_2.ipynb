{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykdtree.kdtree import KDTree\n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic utility functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "def run_meshlab_script(mlx_script,input_mesh_file,output_mesh_file):\n",
    "    script_command = (\" -i \" + str(input_mesh_file) + \" -o \" + \n",
    "                                    str(output_mesh_file) + \" -s \" + str(mlx_script))\n",
    "    #return script_command\n",
    "    command_to_run = 'xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@ ' + script_command\n",
    "    #command_to_run = 'meshlabserver ' + script_command\n",
    "    \n",
    "    print(command_to_run)\n",
    "    subprocess_result = subprocess.run(command_to_run,shell=True)\n",
    "    \n",
    "    return subprocess_result\n",
    "\n",
    "def meshlab_fix_manifold_path_specific_mls(input_path_and_filename,\n",
    "                                           output_path_and_filename=\"\",\n",
    "                                           segment_id=-1,meshlab_script=\"\"):\n",
    "    #fix the path if it comes with the extension\n",
    "    if input_path_and_filename[-4:] == \".off\":\n",
    "        path_and_filename = input_path_and_filename[:-4]\n",
    "        input_mesh = input_path_and_filename\n",
    "    else:\n",
    "        raise Exception(\"Not passed off file\")\n",
    "    \n",
    "    \n",
    "    if output_path_and_filename == \"\":\n",
    "        output_mesh = path_and_filename+\"_mls.off\"\n",
    "    else:\n",
    "        output_mesh = output_path_and_filename\n",
    "    \n",
    "    if meshlab_script == \"\":\n",
    "        meshlab_script = str(pathlib.Path.cwd()) + \"/\" + \"remeshing_remove_non_man_edges.mls\"\n",
    "    \n",
    "    #print(\"meshlab_script = \" + str(meshlab_script))\n",
    "    #print(\"starting meshlabserver fixing non-manifolds\")\n",
    "    subprocess_result_1 = run_meshlab_script(meshlab_script,\n",
    "                      input_mesh,\n",
    "                      output_mesh)\n",
    "    #print(\"Poisson subprocess_result= \"+ str(subprocess_result_1))\n",
    "    \n",
    "    if str(subprocess_result_1)[-13:] != \"returncode=0)\":\n",
    "        raise Exception('neuron' + str(segment_id) + \n",
    "                         ' did not fix the manifold edges')\n",
    "    \n",
    "    return output_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the output file\n",
    "##write the OFF file for the neuron\n",
    "import pathlib\n",
    "def write_Whole_Neuron_Off_file(vertices=[], \n",
    "                                triangles=[],\n",
    "                                neuron_ID=\"None\",\n",
    "                                folder=\"None\",\n",
    "                               path_and_filename=\"-1\"):\n",
    "    #primary_key = dict(segmentation=1, segment_id=segment_id, decimation_ratio=0.35)\n",
    "    #vertices, triangles = (mesh_Table_35 & primary_key).fetch1('vertices', 'triangles')\n",
    "    \n",
    "    num_vertices = (len(vertices))\n",
    "    num_faces = len(triangles)\n",
    "    if path_and_filename == \"-1\":\n",
    "        #get the current file location\n",
    "        if folder == \"None\":\n",
    "            file_loc = pathlib.Path.cwd()\n",
    "            \n",
    "        else:\n",
    "            file_loc = pathlib.Path.cwd() / folder\n",
    "            \n",
    "        filename = \"neuron_\" + str(neuron_ID)\n",
    "        path_and_filename = file_loc / filename\n",
    "    \n",
    "    #print(\"path_and_filename = \" + str(path_and_filename))\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(num_vertices) + \" \" + str(num_faces) + \" 0\\n\" )\n",
    "    \n",
    "    \n",
    "    #iterate through and write all of the vertices in the file\n",
    "    for verts in vertices:\n",
    "        f.write(str(verts[0]) + \" \" + str(verts[1]) + \" \" + str(verts[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing verts\")\n",
    "        \n",
    "    for faces in triangles:\n",
    "        f.write(\"3 \" + str(faces[0]) + \" \" + str(faces[1]) + \" \" + str(faces[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing OFF file\")\n",
    "    #f.write(\"end\")\n",
    "    \n",
    "    return str(path_and_filename)#,str(filename),str(file_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton Utility Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_distance_from_skeleton( skeleton_vertices,\n",
    "                            child_mesh_verts,\n",
    "                             child_mesh_faces,\n",
    "                             distance_threshold=500,\n",
    "                             significance_threshold=500,\n",
    "                             n_sample_points=3, #currently this does not do anything\n",
    "                             print_flag=False,\n",
    "                             return_verts_faces=False):\n",
    "    \"\"\"\n",
    "    Will return all of the significant mesh pieces that are far enough away from the \n",
    "    mesh skeleton\n",
    "    \"\"\"\n",
    "    \n",
    "    import time\n",
    "    global_time = time.time()\n",
    "    start_time = time.time()\n",
    "    mesh_tree = KDTree(skeleton_vertices)\n",
    "    distances,closest_node = mesh_tree.query(child_mesh_verts)\n",
    "    if print_flag:\n",
    "        print(f\"Total time for KDTree creation and queries: {time.time() - start_time}\")\n",
    "    \n",
    "    if print_flag:\n",
    "        print(\"Original number vertices in child mesh = \" + str(len(child_mesh_verts)))\n",
    "    vertex_indices = np.where(distances > distance_threshold)[0]\n",
    "    if print_flag:\n",
    "        print(\"Number of vertices after distance threshold applied =  \" + str(len(vertex_indices)))\n",
    "    \n",
    "    #gets the faces after has the vertices\n",
    "    start_time = time.time()\n",
    "    set_vertex_indices = set(list(vertex_indices))\n",
    "    face_indices_lookup = np.linspace(0,len(child_mesh_faces)-1,len(child_mesh_faces)).astype('int')\n",
    "    face_indices_lookup_bool = [len(set_vertex_indices.intersection(set(tri))) > 0 for tri in child_mesh_faces]\n",
    "    face_indices_lookup = face_indices_lookup[face_indices_lookup_bool]\n",
    "\n",
    "    if print_flag:\n",
    "        print(f\"Total time for finding faces after distance threshold applied: {time.time() - start_time}\")\n",
    "    if len(child_mesh_verts)<=0 or len(child_mesh_faces)<=0 or len(face_indices_lookup)<= 0:\n",
    "        print(\"inside boolean function and returning because child faces are 0\")\n",
    "        return []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    trimesh_original = trimesh.Trimesh(child_mesh_verts,child_mesh_faces,process=False) \n",
    "    new_submesh = trimesh_original.submesh([face_indices_lookup],only_watertight=False,append=True)\n",
    "    \n",
    "    \n",
    "    if print_flag:\n",
    "        print(\"------Starting the mesh filter for significant outside pieces-------\")\n",
    "\n",
    "    mesh_pieces = new_submesh.split(only_watertight=False)\n",
    "    \n",
    "    if print_flag:\n",
    "        print(f\"There were {len(mesh_pieces)} pieces after mesh split\")\n",
    "\n",
    "    significant_pieces = [m for m in mesh_pieces if len(m.faces) > significance_threshold]\n",
    "\n",
    "    if print_flag:\n",
    "        print(f\"There were {len(significant_pieces)} pieces found after size threshold\")\n",
    "    if len(significant_pieces) <=0:\n",
    "        print(\"THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\")\n",
    "        return []\n",
    "    \n",
    "    #arrange the significant pieces from largest to smallest\n",
    "    x = [len(k.vertices) for k in significant_pieces]\n",
    "    sorted_indexes = sorted(range(len(x)), key=lambda k: x[k])\n",
    "    sorted_indexes = sorted_indexes[::-1]\n",
    "    sorted_significant_pieces = [significant_pieces[k] for k in sorted_indexes]\n",
    "    \n",
    "    return sorted_significant_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extra skeleton functions:\n",
    "\"\"\"\n",
    "import networkx as nx\n",
    "import time \n",
    "import numpy as np\n",
    "import trimesh\n",
    "import random\n",
    "\n",
    "def generate_surface_skeleton(vertices, faces, surface_samples):\n",
    "    \n",
    "    #return surface_with_poisson_skeleton,path_length\n",
    "    \n",
    "    current_mesh = trimesh.Trimesh(vertices=vertices,\n",
    "                                  faces = faces)\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # test on a sphere mesh\n",
    "    mesh = current_mesh\n",
    "\n",
    "    ga = nx.from_edgelist(mesh.edges)\n",
    "\n",
    "    if surface_samples<len(vertices):\n",
    "        k = surface_samples\n",
    "    else:\n",
    "        k = len(vertices)\n",
    "    sampled_nodes = random.sample(ga.nodes, k)\n",
    "\n",
    "\n",
    "    lp_end_list = []\n",
    "    lp_magnitude_list = []\n",
    "\n",
    "    for s in sampled_nodes: \n",
    "        sp_dict = nx.single_source_shortest_path_length(ga,s)\n",
    "\n",
    "        list_keys = list(sp_dict.keys())\n",
    "        longest_path_node = list_keys[len(list_keys)-1]\n",
    "        longest_path_magnitude = sp_dict[longest_path_node]\n",
    "\n",
    "\n",
    "        lp_end_list.append(longest_path_node)\n",
    "        lp_magnitude_list.append(longest_path_magnitude)\n",
    "\n",
    "    #construct skeleton from shortest path\n",
    "    final_start = sampled_nodes[np.argmax(lp_magnitude_list)]\n",
    "    final_end = sampled_nodes[np.argmax(lp_end_list)]\n",
    "\n",
    "    node_list = nx.shortest_path(ga,final_start,final_end)\n",
    "    if len(node_list) < 2:\n",
    "        print(\"node_list len < 2 so returning empty list\")\n",
    "        return np.array([])\n",
    "    #print(\"node_list = \" + str(node_list))\n",
    "    final_skeleton = mesh.vertices[np.vstack([node_list[:-1],node_list[1:]]).T]\n",
    "    print(f\"   Final Time for surface skeleton with sample size = {k} = {time.time() - start_time}\")\n",
    "\n",
    "    return final_skeleton\n",
    "\n",
    "def save_skeleton_cgal(surface_with_poisson_skeleton,largest_mesh_path):\n",
    "    \"\"\"\n",
    "    surface_with_poisson_skeleton (np.array) : nx2 matrix with the nodes\n",
    "    \"\"\"\n",
    "    first_node = surface_with_poisson_skeleton[0][0]\n",
    "    end_nodes =  surface_with_poisson_skeleton[:,1]\n",
    "    \n",
    "    skeleton_to_write = str(len(end_nodes) + 1) + \" \" + str(first_node[0]) + \" \" +  str(first_node[1]) + \" \" +  str(first_node[2])\n",
    "    \n",
    "    for node in end_nodes:\n",
    "        skeleton_to_write +=  \" \" + str(node[0]) + \" \" +  str(node[1]) + \" \" +  str(node[2])\n",
    "    \n",
    "    output_file = largest_mesh_path\n",
    "    if output_file[-5:] != \".cgal\":\n",
    "        output_file += \".cgal\"\n",
    "        \n",
    "    f = open(output_file,\"w\")\n",
    "    f.write(skeleton_to_write)\n",
    "    f.close()\n",
    "    return \n",
    "\n",
    "#read in the skeleton files into an array\n",
    "def read_skeleton_revised(file_path):\n",
    "    with open(file_path) as f:\n",
    "        bones = np.array([])\n",
    "        for line in f.readlines():\n",
    "            #print(line)\n",
    "            line = (np.array(line.split()[1:], float).reshape(-1, 3))\n",
    "            #print(line[:-1])\n",
    "            #print(line[1:])\n",
    "\n",
    "            #print(bones.size)\n",
    "            if bones.size <= 0:\n",
    "                bones = np.stack((line[:-1],line[1:]),axis=1)\n",
    "            else:\n",
    "                bones = np.vstack((bones,(np.stack((line[:-1],line[1:]),axis=1))))\n",
    "            #print(bones)\n",
    "\n",
    "\n",
    "    return np.array(bones).astype(float)\n",
    "    unique_skeleton_verts = bone_array_total.reshape(-1,3)\n",
    "\n",
    "def calculate_skeleton_distance(my_skeleton):\n",
    "    total_distance = np.sum(np.sqrt(np.sum((my_skeleton[:,0] - my_skeleton[:,1])**2,axis=1)))\n",
    "    return float(total_distance)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Testing if the read and write are correct:\n",
    "\n",
    "import numpy as np\n",
    "file_name = \"107816118160698192_leftover_1-0_skeleton.cgal\"\n",
    "my_skeleton = read_skeleton_revised(file_name)\n",
    "my_skeleton\n",
    "practice_file = \"test_manual_cgal.cgal\"\n",
    "save_skeleton_cgal(my_skeleton,practice_file)\n",
    "my_skeleton_2 = read_skeleton_revised(practice_file)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Pseudocode:\n",
    "1) Get the number of rows to be even (by doing nothing or adding the first one to the final list\n",
    "2) Take every other left neuorn, 1 + Take every other right neuron\n",
    "\n",
    "\n",
    "A B\n",
    "B C\n",
    "C D\n",
    "D E\n",
    "E F\n",
    "F G\n",
    "\n",
    "Turns into\n",
    "\n",
    "A C\n",
    "C E\n",
    "E G\n",
    "\"\"\"\n",
    "\n",
    "def downsample_skeleton(current_skeleton):\n",
    "    #print(\"current_skeleton = \" + str(current_skeleton.shape))\n",
    "    \"\"\"\n",
    "    Downsamples the skeleton by 50% number of edges\n",
    "    \"\"\"\n",
    "    extra_segment = []\n",
    "    if current_skeleton.shape[0] % 2 != 0:\n",
    "        extra_segment = np.array([current_skeleton[0]])\n",
    "        current_skeleton = current_skeleton[1:]\n",
    "        #print(\"extra_segment = \" + str(extra_segment))\n",
    "        #print(\"extra_segment.shape = \" + str(extra_segment.shape))\n",
    "    else:\n",
    "        #print(\"extra_segment = \" + str(extra_segment))\n",
    "        pass\n",
    "\n",
    "    even_indices = [k for k in range(0,current_skeleton.shape[0]) if k%2 == 0]\n",
    "    odd_indices = [k for k in range(0,current_skeleton.shape[0]) if k%2 == 1]\n",
    "    even_verts = current_skeleton[even_indices,0,:]\n",
    "    odd_verts = current_skeleton[odd_indices,1,:]\n",
    "\n",
    "    downsampled_skeleton = np.hstack([even_verts,odd_verts]).reshape(even_verts.shape[0],2,3)\n",
    "    #print(\"dowsampled_skeleton.shape = \" + str(downsampled_skeleton.shape))\n",
    "    if len(extra_segment) > 0:\n",
    "        #print(\"downsampled_skeleton = \" + str(downsampled_skeleton.shape))\n",
    "        final_downsampled_skeleton = np.vstack([extra_segment,downsampled_skeleton])\n",
    "    else:\n",
    "        final_downsampled_skeleton = downsampled_skeleton\n",
    "    return final_downsampled_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "\n",
    "def graph_skeleton_and_mesh(main_mesh_verts=[],main_mesh_faces=[],\n",
    "                            unique_skeleton_verts_final=[],edges_final=[]):\n",
    "    \"\"\"\n",
    "    Graph the final result: \n",
    "    \"\"\"\n",
    "\n",
    "    ipv.figure(figsize=(15,15))\n",
    "    \n",
    "    if len(unique_skeleton_verts_final) > 0 and len(edges_final) > 0:\n",
    "        mesh2 = ipv.plot_trisurf(unique_skeleton_verts_final[:,0], \n",
    "                                unique_skeleton_verts_final[:,1], \n",
    "                                unique_skeleton_verts_final[:,2], \n",
    "                                lines=edges_final, color='blue')\n",
    "\n",
    "        mesh2.color = [0,0.,1,1]\n",
    "        mesh2.material.transparent = True\n",
    "    \n",
    "    if len(main_mesh_verts) > 0 and len(main_mesh_faces) > 0:\n",
    "        main_mesh = trimesh.Trimesh(vertices=main_mesh_verts,faces=main_mesh_faces)\n",
    "\n",
    "        mesh3 = ipv.plot_trisurf(main_mesh.vertices[:,0],\n",
    "                               main_mesh.vertices[:,1],\n",
    "                               main_mesh.vertices[:,2],\n",
    "                               triangles=main_mesh.faces)\n",
    "        mesh3.color = [0.,1.,0.,0.2]\n",
    "        mesh3.material.transparent = True\n",
    "\n",
    "\n",
    "        volume_max = np.max(main_mesh.vertices,axis=0)\n",
    "        volume_min = np.min(main_mesh.vertices,axis=0)\n",
    "\n",
    "        ranges = volume_max - volume_min\n",
    "        index = [0,1,2]\n",
    "        max_index = np.argmax(ranges)\n",
    "        min_limits = [0,0,0]\n",
    "        max_limits = [0,0,0]\n",
    "\n",
    "        buffer = 10000\n",
    "        for i in index:\n",
    "            if i == max_index:\n",
    "                min_limits[i] = volume_min[i] - buffer\n",
    "                max_limits[i] = volume_max[i] + buffer \n",
    "                continue\n",
    "            else:\n",
    "                difference = ranges[max_index] - ranges[i]\n",
    "                min_limits[i] = volume_min[i] - difference/2  - buffer\n",
    "                max_limits[i] = volume_max[i] + difference/2 + buffer\n",
    "\n",
    "        #ipv.xyzlim(-2, 2)\n",
    "        ipv.xlim(min_limits[0],max_limits[0])\n",
    "        ipv.ylim(min_limits[1],max_limits[1])\n",
    "        ipv.zlim(min_limits[2],max_limits[2])\n",
    "\n",
    "    ipv.style.set_style_light()\n",
    "    ipv.style.box_off()\n",
    "    ipv.style.axes_off()\n",
    "\n",
    "    ipv.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main recursive skeleton functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_skeletonizing(verts,faces,current_name,\n",
    "                            directory,current_base_path,mesh_base_path,folder_name,\n",
    "                            current_depth,\n",
    "                            skeletonization_type,\n",
    "                            surface_samples=200,\n",
    "                            max_depth_cgal = 8,\n",
    "                            max_depth_surface = 10,\n",
    "                            min_skeleton_distance = 30,\n",
    "                           boolean_distance_threshold=1000,\n",
    "                            boolean_significance_threshold=1000,\n",
    "                            boolean_n_sample_points=10,\n",
    "                            boolean_print_flag = False,\n",
    "                           largest_mesh_significant=3000,\n",
    "                           list_of_meshes=[]):\n",
    "    \"\"\"\n",
    "    Function that will run on a single mesh or list of mesh and will \n",
    "    run the skeletonization process where will skeletonize as many segments\n",
    "    as possible through a recursive algorithm\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--------Beginning of loop: current_depth = \" + str(current_depth) + \"--------\")\n",
    "    \n",
    "    if len(list_of_meshes):\n",
    "        mesh_pieces=list_of_meshes\n",
    "\n",
    "        print(f\" -->  Total significant pieces left: {[len(k.vertices) for k in mesh_pieces]}\")\n",
    "        # check if the number of \n",
    "        for j,piece in enumerate(mesh_pieces):\n",
    "            new_current_name = current_name + \"-\" + str(j)\n",
    "            print(f\"Starting recursive call for size vertices of {len(piece.vertices)}\")\n",
    "            recursive_skeletonizing(verts = piece.vertices,faces=piece.faces,current_name=new_current_name,\n",
    "                                   directory=directory,current_base_path=current_base_path,\n",
    "                                    mesh_base_path=mesh_base_path,folder_name=folder_name,\n",
    "                                   current_depth=current_depth,\n",
    "                                    skeletonization_type=skeletonization_type,\n",
    "                                    surface_samples=surface_samples,\n",
    "                                    max_depth_cgal = max_depth_cgal,\n",
    "                                    max_depth_surface = max_depth_surface,\n",
    "                                    min_skeleton_distance = min_skeleton_distance,\n",
    "                                   boolean_distance_threshold=boolean_distance_threshold,\n",
    "                                    boolean_significance_threshold=boolean_significance_threshold,\n",
    "                                    boolean_n_sample_points=boolean_n_sample_points,\n",
    "                                    boolean_print_flag = boolean_print_flag,\n",
    "                                   largest_mesh_significant=largest_mesh_significant)\n",
    "        return\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n----------Starting {current_name}----------\")\n",
    "        global_time = time.time()\n",
    "        #print out the off file\n",
    "        if current_depth >= max_depth_surface:\n",
    "            print(f\"\\n\\n*********Reamched maximum depth of {max_depth_surface} so returning********\\n\\n\")\n",
    "            return\n",
    "        if skeletonization_type== \"cgal\" and current_depth >= max_depth_cgal:\n",
    "            print(\"     Reached max depth cgal\")\n",
    "\n",
    "            skeletonization_type = \"surface_with_poisson\"\n",
    "            current_name += \"_sWp\"\n",
    "\n",
    "            recursive_skeletonizing(verts,faces,current_name,\n",
    "                            directory,current_base_path,mesh_base_path,folder_name,\n",
    "                            current_depth=current_depth,\n",
    "                            skeletonization_type=skeletonization_type,\n",
    "                            surface_samples=surface_samples,\n",
    "                            max_depth_cgal = max_depth_cgal,\n",
    "                            max_depth_surface = max_depth_surface,\n",
    "                            min_skeleton_distance = min_skeleton_distance,\n",
    "                           boolean_distance_threshold=boolean_distance_threshold,\n",
    "                            boolean_significance_threshold=boolean_significance_threshold,\n",
    "                            boolean_n_sample_points=boolean_n_sample_points,\n",
    "                            boolean_print_flag = boolean_print_flag,\n",
    "                           largest_mesh_significant=largest_mesh_significant)\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if skeletonization_type == \"surface_without_poisson\":\n",
    "            print(\"Inside WITHOUT poisson generation\")\n",
    "            \n",
    "            \"\"\"\n",
    "            generate all of the mesh pieces if this is the first layer\n",
    "            \"\"\"\n",
    "            if current_depth == 1:\n",
    "                current_depth += 1\n",
    "                #generate the mesh pieces\n",
    "                new_submesh = trimesh.Trimesh(vertices=verts,faces=faces)\n",
    "                mesh_pieces = new_submesh.split(only_watertight=False)\n",
    "    \n",
    "                if boolean_print_flag:\n",
    "                    print(f\"There were {len(mesh_pieces)} pieces after mesh split\")\n",
    "\n",
    "                significant_pieces = [m for m in mesh_pieces if len(m.faces) > boolean_significance_threshold]\n",
    "\n",
    "                if boolean_print_flag:\n",
    "                    print(f\"There were {len(significant_pieces)} pieces found after size threshold\")\n",
    "                if len(significant_pieces) <=0:\n",
    "                    print(\"THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\")\n",
    "                    return []\n",
    "\n",
    "                #arrange the significant pieces from largest to smallest\n",
    "                x = [len(k.vertices) for k in significant_pieces]\n",
    "                sorted_indexes = sorted(range(len(x)), key=lambda k: x[k])\n",
    "                sorted_indexes = sorted_indexes[::-1]\n",
    "                sorted_significant_pieces = [significant_pieces[k] for k in sorted_indexes]\n",
    "                \n",
    "                \n",
    "                mesh_pieces = sorted_significant_pieces\n",
    "\n",
    "                print(f\" -->  Total significant pieces left: {[len(k.vertices) for k in mesh_pieces]}\")\n",
    "                # check if the number of \n",
    "                for j,piece in enumerate(mesh_pieces):\n",
    "                    new_current_name = current_name + \"-\" + str(j)\n",
    "                    print(f\"Starting recursive call for size vertices of {len(piece.vertices)}\")\n",
    "                    recursive_skeletonizing(verts = piece.vertices,faces=piece.faces,current_name=new_current_name,\n",
    "                                           directory=directory,current_base_path=current_base_path,\n",
    "                                            mesh_base_path=mesh_base_path,folder_name=folder_name,\n",
    "                                           current_depth=current_depth,\n",
    "                                            skeletonization_type=skeletonization_type,\n",
    "                                            surface_samples=surface_samples,\n",
    "                                            max_depth_cgal = max_depth_cgal,\n",
    "                                            max_depth_surface = max_depth_surface,\n",
    "                                            min_skeleton_distance = min_skeleton_distance,\n",
    "                                           boolean_distance_threshold=boolean_distance_threshold,\n",
    "                                            boolean_significance_threshold=boolean_significance_threshold,\n",
    "                                            boolean_n_sample_points=boolean_n_sample_points,\n",
    "                                            boolean_print_flag = boolean_print_flag,\n",
    "                                           largest_mesh_significant=largest_mesh_significant)\n",
    "                return\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                surface_without_poisson_skeleton = generate_surface_skeleton(verts,\n",
    "                                                                              faces=faces,\n",
    "                                                                              surface_samples = surface_samples)\n",
    "                if len(surface_without_poisson_skeleton)>2:\n",
    "                    surface_without_poisson_skeleton = downsample_skeleton(surface_without_poisson_skeleton)\n",
    "                if len(surface_without_poisson_skeleton) <= 0 :\n",
    "                    print(\"Returning because the skeleton returned was of size 0\")\n",
    "                    return\n",
    "                #print(f\"Path length for surface skeleton WITHOUT poisson = {path_length}\")\n",
    "\n",
    "                largest_mesh_path = mesh_base_path + current_name\n",
    "\n",
    "                #save off the skeleton\n",
    "                save_skeleton_cgal(surface_without_poisson_skeleton,largest_mesh_path + \"_skeleton\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Inside poisson generation\")\n",
    "\n",
    "            initial_output_path = mesh_base_path + current_name\n",
    "\n",
    "            write_Whole_Neuron_Off_file(vertices=verts, \n",
    "                                        triangles=faces,\n",
    "                                       path_and_filename=initial_output_path)\n",
    "\n",
    "\n",
    "            # do the poisson surface reconstruction\n",
    "\n",
    "            script_name = \"poisson_working_meshlab.mls\"\n",
    "\n",
    "            input_file_base = initial_output_path\n",
    "            output_file = input_file_base + \"_poisson\"\n",
    "            meshlab_script_path_and_name =current_base_path + script_name\n",
    "\n",
    "            start_time = time.time()\n",
    "            print(\"     Starting Screened Poisson\")\n",
    "            meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_file_base + \".off\",\n",
    "                                                               output_path_and_filename=output_file + \".off\",\n",
    "                                                             meshlab_script=meshlab_script_path_and_name)\n",
    "            print(f\"     Total_time for Screended Poisson = {time.time() - start_time}\")\n",
    "\n",
    "            #2) Filter away for largest_poisson_piece:\n",
    "\n",
    "            new_mesh = trimesh.load_mesh(output_file + \".off\")\n",
    "            mesh_splits = new_mesh.split(only_watertight=False)\n",
    "            mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "            largest_index = np.where(mesh_lengths == np.max(mesh_lengths))\n",
    "            largest_mesh = mesh_splits[largest_index][0]\n",
    "\n",
    "            print(\"len(largest_mesh.vertices) = \" + str(len(largest_mesh.vertices)))\n",
    "            #4) If not of a significant size then return (Add the largest_poisson_piece to the Surface_Reconstruction_list and say that it was returned)\n",
    "            if len(largest_mesh.vertices) < largest_mesh_significant:\n",
    "                print(current_name + \" largest mesh not significant AFTER POISSON RECONSTRUCTION\")\n",
    "                print(\"trying surface skeletonization without Poisson\")\n",
    "\n",
    "                \"\"\"3) if no significant poisson piece --> do surface_with_poisson process\n",
    "                    change the name slightly\n",
    "                    change the skeletonization type\"\"\"\n",
    "\n",
    "                skeletonization_type = \"surface_without_poisson\"\n",
    "                current_name += \"_sWOp\"\n",
    "\n",
    "                recursive_skeletonizing(verts,faces,current_name,\n",
    "                                directory,current_base_path,mesh_base_path,folder_name,\n",
    "                                current_depth=current_depth,\n",
    "                                skeletonization_type=skeletonization_type,\n",
    "                                surface_samples=surface_samples,\n",
    "                                max_depth_cgal = max_depth_cgal,\n",
    "                                max_depth_surface = max_depth_surface,\n",
    "                                min_skeleton_distance = min_skeleton_distance,\n",
    "                               boolean_distance_threshold=boolean_distance_threshold,\n",
    "                                boolean_significance_threshold=boolean_significance_threshold,\n",
    "                                boolean_n_sample_points=boolean_n_sample_points,\n",
    "                                boolean_print_flag = boolean_print_flag,\n",
    "                               largest_mesh_significant=largest_mesh_significant)\n",
    "                return \n",
    "\n",
    "            if skeletonization_type == \"surface_with_poisson\":\n",
    "                surface_with_poisson_skeleton = generate_surface_skeleton(largest_mesh.vertices,\n",
    "                                                                          faces=largest_mesh.faces,\n",
    "                                                                          surface_samples = surface_samples)\n",
    "\n",
    "                if len(surface_with_poisson_skeleton) <= 0 :\n",
    "                    print(\"Returning because the skeleton returned was of size 0\")\n",
    "                    return\n",
    "                #print(f\"Path length for surface skeleton with poisson = {path_length}\")\n",
    "\n",
    "                largest_mesh_path = mesh_base_path + current_name\n",
    "\n",
    "                #save off the skeleton\n",
    "                save_skeleton_cgal(surface_with_poisson_skeleton,largest_mesh_path + \"_skeleton\")\n",
    "\n",
    "\n",
    "            elif skeletonization_type == \"cgal\":\n",
    "\n",
    "                #5) If significant size output the mesh\n",
    "                largest_mesh_path = mesh_base_path + current_name\n",
    "                write_Whole_Neuron_Off_file(vertices=largest_mesh.vertices, \n",
    "                                            triangles=largest_mesh.faces,\n",
    "                                           path_and_filename=largest_mesh_path)\n",
    "\n",
    "\n",
    "                #6) Run skeletonization on it:\n",
    "                start_time = time.time()\n",
    "                print(\"     Starting Calcification\")\n",
    "                cm.calcification(largest_mesh_path)\n",
    "                print(f\"     Total_time for Calcification = {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\" ***** GOING TO USE THE SKELETON VERTICES INSTEAD OF THE MESH VERTICES\"\"\"\n",
    "\n",
    "\n",
    "        #7) Subtract skeleton vertices from current mesh\n",
    "        start_time = time.time()\n",
    "        print(\"     Starting Mesh boolean difference\")\n",
    "\n",
    "        cgal_skeleton_file = largest_mesh_path + \"_skeleton.cgal\"\n",
    "\n",
    "        try:\n",
    "            returned_skeleton = read_skeleton_revised(cgal_skeleton_file)\n",
    "        except OSError as e:\n",
    "            print(\"\\n\\n**** No cgal file was found so returning ******\\n\\n\")\n",
    "            return\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"\\n\\n**** unknown error ocured when reading in cgal file so returning ******\\n\\n\")\n",
    "            #return\n",
    "\n",
    "\n",
    "        skeletal_length = calculate_skeleton_distance(returned_skeleton)\n",
    "        print(f\" Skeletal Length = {skeletal_length}  \")\n",
    "        # returning based on skeletal length: \n",
    "        if skeletal_length <= min_skeleton_distance:\n",
    "\n",
    "            if skeletonization_type == \"cgal\":\n",
    "                print(\"Skeleton generated was too small --> cgal skeletonization turning into surface with poisson\")\n",
    "                #try the surface skeletonization with poisson\n",
    "                current_name += \"sWp\"\n",
    "                skeletonization_type = \"surface_with_poisson\"\n",
    "\n",
    "            elif skeletonization_type == \"surface_with_poisson\":\n",
    "                print(\"Skeleton generated was too small --> surface WITH poisson turning into surface WTIHOUT poisson\")\n",
    "                current_name += \"sWOp\"\n",
    "                skeletonization_type = \"surface_without_poisson\"\n",
    "            else:\n",
    "                return\n",
    "\n",
    "            recursive_skeletonizing(verts,faces,current_name,\n",
    "                        directory,current_base_path,mesh_base_path,folder_name,\n",
    "                        current_depth=current_depth,\n",
    "                        skeletonization_type=skeletonization_type,\n",
    "                        surface_samples=surface_samples,\n",
    "                        max_depth_cgal = max_depth_cgal,\n",
    "                        max_depth_surface = max_depth_surface,\n",
    "                        min_skeleton_distance = min_skeleton_distance,\n",
    "                       boolean_distance_threshold=boolean_distance_threshold,\n",
    "                        boolean_significance_threshold=boolean_significance_threshold,\n",
    "                        boolean_n_sample_points=boolean_n_sample_points,\n",
    "                        boolean_print_flag = boolean_print_flag,\n",
    "                       largest_mesh_significant=largest_mesh_significant)\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        skeleton_vertices = np.vstack([returned_skeleton[:,0],returned_skeleton[:,1]])\n",
    "        #getting the boolean difference\n",
    "\n",
    "        if skeletonization_type == \"cgal\":\n",
    "            current_boolean_distance_threshold = boolean_distance_threshold\n",
    "        else:\n",
    "            current_boolean_distance_threshold = 500\n",
    "\n",
    "        mesh_pieces = filter_distance_from_skeleton( skeleton_vertices,\n",
    "                                verts,\n",
    "                                 faces,\n",
    "                                 distance_threshold=current_boolean_distance_threshold,\n",
    "                                 significance_threshold=boolean_significance_threshold,\n",
    "                                 n_sample_points=boolean_n_sample_points,\n",
    "                                 print_flag=boolean_print_flag,\n",
    "                                 return_verts_faces=False)\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"     Total_time for Mesh boolean difference = {time.time() - start_time}\")\n",
    "\n",
    "        print(f\"Total time for one mesh piece skeleton = {time.time() - global_time}\")\n",
    "\n",
    "        print(f\"{current_name} there were {len(mesh_pieces)} significant pieces leftover after largest mesh\")\n",
    "\n",
    "        if len(mesh_pieces) <= 0:\n",
    "            print(f\"{current_name} returning because 0 significant pieces\")\n",
    "            return \n",
    "\n",
    "\n",
    "\n",
    "        #increment the current depth \n",
    "        current_depth = current_depth + 1\n",
    "\n",
    "        print(f\" -->  Total significant pieces left: {[len(k.vertices) for k in mesh_pieces]}\")\n",
    "        # check if the number of \n",
    "        for j,piece in enumerate(mesh_pieces):\n",
    "            new_current_name = current_name + \"-\" + str(j)\n",
    "            print(f\"Starting recursive call for size vertices of {len(piece.vertices)}\")\n",
    "            recursive_skeletonizing(verts = piece.vertices,faces=piece.faces,current_name=new_current_name,\n",
    "                                   directory=directory,current_base_path=current_base_path,\n",
    "                                    mesh_base_path=mesh_base_path,folder_name=folder_name,\n",
    "                                   current_depth=current_depth,\n",
    "                                    skeletonization_type=skeletonization_type,\n",
    "                                    surface_samples=surface_samples,\n",
    "                                    max_depth_cgal = max_depth_cgal,\n",
    "                                    max_depth_surface = max_depth_surface,\n",
    "                                    min_skeleton_distance = min_skeleton_distance,\n",
    "                                   boolean_distance_threshold=boolean_distance_threshold,\n",
    "                                    boolean_significance_threshold=boolean_significance_threshold,\n",
    "                                    boolean_n_sample_points=boolean_n_sample_points,\n",
    "                                    boolean_print_flag = boolean_print_flag,\n",
    "                                   largest_mesh_significant=largest_mesh_significant)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pseudocode for skeleton stitcher: \n",
    "-------Old method-------\n",
    "\n",
    "1) creates a network with unique nodes and their coordinates and the edges of the graph\n",
    "2) extract subgraphs: Appends the list of subgraphs\n",
    "3) Iterates through all of the subgraphs\n",
    "a. gets first subgraph\n",
    "b. gets current coordinates of subgraph\n",
    "c. strip these coordinates from all the list of entire coordinates\n",
    "d. Find closest node\n",
    "e. Add edge (make sure not greater than max length)\n",
    "f. recompute the subgraphs\n",
    "g. keep looping until only one component\n",
    "4) Output the edges as a skeleton of coordinates\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pykdtree.kdtree import KDTree\n",
    "\n",
    "import scipy\n",
    "def stitch_skeleton_with_degree_check_vp4(staring_edges,max_stitch_distance=18000,end_node=False):\n",
    "    \n",
    "\n",
    "    #unpacks so just list of vertices\n",
    "    vertices_unpacked  = staring_edges.reshape(-1,3)\n",
    "\n",
    "    #reduce the number of repeat vertices and convert to list\n",
    "    unique_rows = np.unique(vertices_unpacked, axis=0)\n",
    "    unique_rows_list = unique_rows.tolist()\n",
    "\n",
    "    #assigns the number to the vertex (in the original vertex list) that corresponds to the index in the unique list\n",
    "    vertices_unpacked_coefficients = np.array([unique_rows_list.index(a) for a in vertices_unpacked.tolist()])\n",
    "\n",
    "    #reshapes the vertex list to become an edge list\n",
    "    edges_with_coefficients =  np.array(vertices_unpacked_coefficients).reshape(-1,2)\n",
    "    \n",
    "    #create the graph from the edges\n",
    "    B = nx.Graph()\n",
    "    B.add_nodes_from([(x,{\"coordinates\":y}) for x,y in enumerate(unique_rows_list)])\n",
    "    B.add_edges_from(edges_with_coefficients)\n",
    "\n",
    "    # find the shortest distance between the two different subgraphs:\n",
    "    from scipy.spatial import distance_matrix\n",
    "\n",
    "    UG = B.to_undirected()\n",
    "\n",
    "#     # extract subgraphs\n",
    "#     sub_graphs = connected_component_subgraphs(UG)\n",
    "\n",
    "#     subgraphs_list = []\n",
    "#     for i, sg in enumerate(sub_graphs):\n",
    "#         #print(\"subgraph {} has {} nodes\".format(i, sg.number_of_nodes()))\n",
    "#         #print(\"\\tNodes:\", sg.nodes(data=True))\n",
    "#         #print(\"\\tEdges:\", sg.edges())\n",
    "#         subgraphs_list.append(sg)\n",
    "\n",
    "    #get all of the coordinates\n",
    "\n",
    "    print(\"len_subgraphs AT BEGINNING of the loop\")\n",
    "    counter = 0\n",
    "    print_flag = True\n",
    "    while True:\n",
    "        counter+= 1\n",
    "        if print_flag:\n",
    "            print(f\"Starting Loop {counter}\")\n",
    "        start_time = time.time()\n",
    "        \"\"\"\n",
    "        1) Get the indexes of the subgraph\n",
    "        2) Build a KDTree from those not in the subgraph (save the vertices of these)\n",
    "        3) Query against the nodes in the subgraph  and get the smallest distance\n",
    "        4) Create this new edge\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #1) Get the indexes of the subgraph\n",
    "        n_components, labels = scipy.sparse.csgraph.connected_components(csgraph=nx.adjacency_matrix(UG), directed=False, return_labels=True)\n",
    "        subgraph_components = np.where(labels==0)[0]\n",
    "        #print(\"subgraph_components = \" + str(subgraph_components))\n",
    "        if len(subgraph_components) == len(UG.nodes):\n",
    "            print(\"all graph is one component!\")\n",
    "            break\n",
    "\n",
    "        outside_components = np.where(labels !=0)[0]\n",
    "        #print(\"outside_components = \" + str(outside_components))\n",
    "        \n",
    "        #2) Build a KDTree from those not in the subgraph (save the vertices of these)\n",
    "        mesh_tree = KDTree(unique_rows[outside_components])\n",
    "\n",
    "        \n",
    "        #3) Query against the nodes in the subgraph  and get the smallest distance\n",
    "        \"\"\"\n",
    "        Conclusion:\n",
    "        Distance is of the size of the parts that are in the KDTree\n",
    "        The closest nodes represent those that were queryed\n",
    "\n",
    "        \"\"\"\n",
    "        distances,closest_node = mesh_tree.query(unique_rows[subgraph_components])\n",
    "        min_index = np.argmin(distances)\n",
    "        #closest_subgraph_node = subgraph_components[closest_node[min_index]]\n",
    "        #closest_outside_node = outside_components[min_index]\n",
    "        \n",
    "        \n",
    "#         print(\"distances = \" + str(distances))\n",
    "#         print(\"closest_node = \" + str(closest_node))\n",
    "#         print(\"Closest distance = \" + str(distances[min_index]))\n",
    "        closest_outside_node = outside_components[closest_node[min_index]]\n",
    "        closest_subgraph_node = subgraph_components[min_index]\n",
    "        \n",
    "        #get the edge distance of edge about to create:\n",
    "        \n",
    "#         graph_coordinates=nx.get_node_attributes(UG,'coordinates')\n",
    "#         prospective_edge_length = np.linalg.norm(np.array(graph_coordinates[closest_outside_node])-np.array(graph_coordinates[closest_subgraph_node]))\n",
    "#         print(f\"Edge distance going to create = {prospective_edge_length}\")\n",
    "        \n",
    "\n",
    "        #4) Create this new edge\n",
    "        UG.add_edge(closest_subgraph_node,closest_outside_node)\n",
    "        \n",
    "        \n",
    "\n",
    "        if print_flag:\n",
    "            print(f\"Total Time for loop = {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    # get the largest subgraph!!! in case have missing pieces\n",
    "\n",
    "    #add all the new edges to the \n",
    "\n",
    "    total_coord = nx.get_node_attributes(UG,'coordinates')\n",
    "\n",
    "    current_coordinates = np.array(list(total_coord.values()))\n",
    "\n",
    "    return current_coordinates[UG.edges()]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For finding the distances of the skeleton\n",
    "\"\"\"\n",
    "def find_skeleton_distance(example_edges):\n",
    "    total_distance = np.sum([np.linalg.norm(a-b) for a,b in example_edges])\n",
    "    return total_distance\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def find_skeleton_distance_scipy(example_edges):\n",
    "    total_distance = np.sum([distance.euclidean(a, b) for a,b in example_edges])\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calcification_Module as cm #module that allows for calcification\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "def calculate_skeleton_edges(input_verts,input_faces,segment_id,\n",
    "                            boolean_distance_threshold=1000,\n",
    "                                   boolean_significance_threshold=300,\n",
    "                                   surface_samples=200,\n",
    "                                   boolean_print_flag=False,\n",
    "                                   largest_mesh_significant=300,\n",
    "                            skeletonization_type=\"surface_without_poisson\",\n",
    "                            list_of_meshes=[],\n",
    "                            delete_after_finishing = False,\n",
    "                            stitch_edges_flag=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that will oversee the whole skeletonization process:\n",
    "    1) first generate all of the lone skeletal pieces\n",
    "    2) Then call the stitching function that will piece all of the lone skeletal pieces together\n",
    "    \"\"\"\n",
    "    \n",
    "    original_mesh = trimesh.Trimesh(vertices=input_verts,faces=input_faces)\n",
    "    name2 = segment_id\n",
    "    \n",
    "    \n",
    "    directory = \"./\" + name2\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "    current_base_path = str(pathlib.Path.cwd()) + \"/\"\n",
    "    mesh_base_path = current_base_path + str(name2) + \"/\"\n",
    "    folder_name = str(name2)\n",
    "\n",
    "    current_layer = 1\n",
    "    \n",
    "    \n",
    "    recursive_skeletonizing(original_mesh.vertices,original_mesh.faces,name2,\n",
    "                            directory,current_base_path,mesh_base_path,folder_name,\n",
    "                            current_depth=1,\n",
    "                            skeletonization_type=skeletonization_type,\n",
    "                            surface_samples=surface_samples,\n",
    "                            max_depth_cgal = 5,\n",
    "                            max_depth_surface = 7,\n",
    "                            min_skeleton_distance = 10,\n",
    "                           boolean_distance_threshold=boolean_distance_threshold,\n",
    "                            boolean_significance_threshold=boolean_significance_threshold,\n",
    "                            boolean_n_sample_points=surface_samples,\n",
    "                            boolean_print_flag = boolean_print_flag,\n",
    "                            list_of_meshes=list_of_meshes\n",
    "                           )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cgal_skeleton_file_list = []\n",
    "    \n",
    "    for file in os.listdir(name2):\n",
    "        if file.endswith(\".cgal\"):\n",
    "            cgal_skeleton_file_list.append(str(os.path.join(name2, file)))\n",
    "    if len(cgal_skeleton_file_list) <= 0:\n",
    "        #check to see if cgal list is empty\n",
    "        raise Exception(\"There were no cgal files generated in process\")\n",
    "    \n",
    "    \n",
    "    if stitch_edges_flag:\n",
    "        bone_array_total = np.vstack([read_skeleton_revised(k) \n",
    "                                      for k in cgal_skeleton_file_list])\n",
    "\n",
    "        unique_skeleton_verts = bone_array_total.reshape(-1,3)\n",
    "        edges = np.arange(0,len(unique_skeleton_verts)).astype(\"int\").reshape(-1,2)\n",
    "\n",
    "\n",
    "        total_edges = np.array([])\n",
    "\n",
    "        for k in cgal_skeleton_file_list:\n",
    "            bone_array = read_skeleton_revised(k) \n",
    "\n",
    "            #add the skeleton edges to the total edges\n",
    "            if not total_edges.any():\n",
    "                total_edges = bone_array\n",
    "            else:\n",
    "                total_edges = np.vstack([total_edges,bone_array])\n",
    "\n",
    "        total_edges_stitched = stitch_skeleton_with_degree_check_vp4(total_edges)\n",
    "\n",
    "        unique_skeleton_verts = total_edges_stitched.reshape(-1,3)\n",
    "        edges = np.arange(0,len(unique_skeleton_verts)).astype(\"int\").reshape(-1,2)\n",
    "\n",
    "        if delete_after_finishing:\n",
    "            #erase the folder with all of the data in it\n",
    "            import shutil\n",
    "            shutil.rmtree(directory)\n",
    "\n",
    "        #return the edges\n",
    "        return edges,unique_skeleton_verts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_run_stitching(folder_list=[],delete_after_finishing=False):\n",
    "    if len(folder_list) <= 0:\n",
    "        raise Exception(\"There were no folders listed in the parameters\")\n",
    "        \n",
    "    cgal_skeleton_file_list = []\n",
    "    for directory in folder_list:\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith(\".cgal\"):\n",
    "                cgal_skeleton_file_list.append(str(os.path.join(directory, file)))\n",
    "        if len(cgal_skeleton_file_list) <= 0:\n",
    "            #check to see if cgal list is empty\n",
    "            raise Exception(\"There were no cgal files generated in process\")\n",
    "\n",
    "\n",
    "    bone_array_total = np.vstack([read_skeleton_revised(k) \n",
    "                                  for k in cgal_skeleton_file_list])\n",
    "\n",
    "    unique_skeleton_verts = bone_array_total.reshape(-1,3)\n",
    "    edges = np.arange(0,len(unique_skeleton_verts)).astype(\"int\").reshape(-1,2)\n",
    "\n",
    "\n",
    "    total_edges = np.array([])\n",
    "\n",
    "    for k in cgal_skeleton_file_list:\n",
    "        bone_array = read_skeleton_revised(k) \n",
    "\n",
    "        #add the skeleton edges to the total edges\n",
    "        if not total_edges.any():\n",
    "            total_edges = bone_array\n",
    "        else:\n",
    "            total_edges = np.vstack([total_edges,bone_array])\n",
    "\n",
    "    total_edges_stitched = stitch_skeleton_with_degree_check_vp4(total_edges)\n",
    "\n",
    "    unique_skeleton_verts = total_edges_stitched.reshape(-1,3)\n",
    "    edges = np.arange(0,len(unique_skeleton_verts)).astype(\"int\").reshape(-1,2)\n",
    "\n",
    "    if delete_after_finishing:\n",
    "        #erase the folder with all of the data in it\n",
    "        import shutil\n",
    "        shutil.rmtree(directory)\n",
    "\n",
    "    #return the edges\n",
    "    return edges,unique_skeleton_verts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------  Running the Skeletonization ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the neuron mesh and assigning the id\n",
    "import trimesh\n",
    "main_mesh = trimesh.load_mesh(\"L5_neuron.off\")\n",
    "segment_id = \"L_5\"\n",
    "\n",
    "import time\n",
    "total_start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the first pass that does the cgal skeletonization\n",
    "calculate_skeleton_edges(main_mesh.vertices,main_mesh.faces,segment_id,\n",
    "                                   boolean_distance_threshold=2000,\n",
    "                                   boolean_significance_threshold=1500,\n",
    "                                   surface_samples=200,\n",
    "                                   boolean_print_flag=True,\n",
    "                                   largest_mesh_significant=500,\n",
    "                                    skeletonization_type=\"cgal\",\n",
    "                                    stitch_edges_flag=False)\n",
    "print(f\"Total time for calculating skeleton = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the total skeleton vertices\n",
    "\n",
    "import os\n",
    "name2 = [segment_id]\n",
    "cgal_skeleton_file_list = []\n",
    "\n",
    "for n in name2:\n",
    "    for file in os.listdir(n):\n",
    "        if file.endswith(\".cgal\"):\n",
    "            cgal_skeleton_file_list.append(str(os.path.join(n, file)))\n",
    "    if len(cgal_skeleton_file_list) <= 0:\n",
    "        #check to see if cgal list is empty\n",
    "        raise Exception(\"There were no cgal files generated in process\")\n",
    "\n",
    "bone_array_total = np.vstack([read_skeleton_revised(k) \n",
    "                              for k in cgal_skeleton_file_list])\n",
    "\n",
    "unique_skeleton_verts = bone_array_total.reshape(-1,3)\n",
    "edges = np.arange(0,len(unique_skeleton_verts)).astype(\"int\").reshape(-1,2)\n",
    "\n",
    "\n",
    "# Find the difference in mesh in order to do for the second pass\n",
    "returned_mesh = filter_distance_from_skeleton( unique_skeleton_verts,\n",
    "                            main_mesh.vertices,\n",
    "                             main_mesh.faces,\n",
    "                             distance_threshold=1900,\n",
    "                             significance_threshold=200,\n",
    "                             n_sample_points=3, #currently this does not do anything\n",
    "                             print_flag=False,\n",
    "                             return_verts_faces=False)\n",
    "total_leftover_mesh = trimesh.Trimesh(vertices=[],faces=[])\n",
    "\n",
    "for rm in returned_mesh:\n",
    "    total_leftover_mesh += rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first pass skeletonization\n",
    "graph_skeleton_and_mesh(total_leftover_mesh.vertices,total_leftover_mesh.faces,\n",
    "                       unique_skeleton_verts,edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "total_start_time = time.time()\n",
    "\n",
    "returned_mesh\n",
    "\n",
    "segment_id_run_2 = \"L_5_Run_2\"\n",
    "\n",
    "calculate_skeleton_edges(main_mesh.vertices,main_mesh.faces,segment_id_run_2,\n",
    "                                   boolean_distance_threshold=500,\n",
    "                                   boolean_significance_threshold=250,\n",
    "                                   surface_samples=500,\n",
    "                                   boolean_print_flag=False,\n",
    "                                   largest_mesh_significant=250,\n",
    "                                                      skeletonization_type=\"surface_without_poisson\",\n",
    "                                                      list_of_meshes=returned_mesh,\n",
    "                                    stitch_edges_flag=False)\n",
    "print(f\"Total time for calculating skeleton = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the total skeleton vertices\n",
    "\n",
    "import os\n",
    "name2 = [segment_id_run_2]\n",
    "cgal_skeleton_file_list = []\n",
    "\n",
    "for n in name2:\n",
    "    for file in os.listdir(n):\n",
    "        if file.endswith(\".cgal\"):\n",
    "            cgal_skeleton_file_list.append(str(os.path.join(n, file)))\n",
    "    if len(cgal_skeleton_file_list) <= 0:\n",
    "        #check to see if cgal list is empty\n",
    "        raise Exception(\"There were no cgal files generated in process\")\n",
    "\n",
    "bone_array_total = np.vstack([read_skeleton_revised(k) \n",
    "                              for k in cgal_skeleton_file_list])\n",
    "\n",
    "unique_skeleton_verts_Run_2 = bone_array_total.reshape(-1,3)\n",
    "edges_Run_2 = np.arange(0,len(unique_skeleton_verts)).astype(\"int\").reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_skeleton_and_mesh(total_leftover_mesh.vertices,total_leftover_mesh.faces,\n",
    "                       unique_skeleton_verts_Run_2,edges_Run_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_skeleton_and_mesh(main_mesh.vertices,main_mesh.faces,\n",
    "                       unique_skeleton_verts_Run_2,edges_Run_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitching Up both Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges,unique_skeleton_verts = combined_run_stitching([\"L_5\",\"L_5_Run_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving off final Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"Final_L5_skeleton_and_mesh_consolidated.npz\",unique_skeleton_verts_final=unique_skeleton_verts,edges_final=edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

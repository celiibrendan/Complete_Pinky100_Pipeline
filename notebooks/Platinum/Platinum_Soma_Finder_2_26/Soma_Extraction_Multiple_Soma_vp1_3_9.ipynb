{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing out the full soma extraction\n",
    "\n",
    "Pseudocode for Algorithm: \n",
    "Load in mesh\n",
    "Split mesh into largest pieces: \n",
    "    Iterate through all mesh pieces of a certain threshold\n",
    "    Do the Poisson surface reconstruction:\n",
    "    Find all the mesh pieces of a certain threshold:\n",
    "        (Optional step) Run the screened poisson surface reconstruction\n",
    "        Run the segmentation algorithm\n",
    "        Identify all somas\n",
    "        Save of the soma meshes\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "from whole_neuron_classifier_datajoint_adapted import extract_branches_whole_neuron\n",
    "import whole_neuron_classifier_datajoint_adapted as wcda \n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_meshlab_script(mlx_script,input_mesh_file,output_mesh_file):\n",
    "    script_command = (\" -i \" + str(input_mesh_file) + \" -o \" + \n",
    "                                    str(output_mesh_file) + \" -s \" + str(mlx_script))\n",
    "    #return script_command\n",
    "    command_to_run = 'xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@ ' + script_command\n",
    "    #command_to_run = 'meshlabserver ' + script_command\n",
    "    \n",
    "    print(command_to_run)\n",
    "    subprocess_result = subprocess.run(command_to_run,shell=True)\n",
    "    \n",
    "    return subprocess_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess\n",
    "def meshlab_fix_manifold_path_specific_mls(input_path_and_filename,\n",
    "                                           output_path_and_filename=\"\",\n",
    "                                           segment_id=-1,meshlab_script=\"\"):\n",
    "    #fix the path if it comes with the extension\n",
    "    if input_path_and_filename[-4:] == \".off\":\n",
    "        path_and_filename = input_path_and_filename[:-4]\n",
    "        input_mesh = input_path_and_filename\n",
    "    else:\n",
    "        raise Exception(\"Not passed off file\")\n",
    "    \n",
    "    \n",
    "    if output_path_and_filename == \"\":\n",
    "        output_mesh = path_and_filename+\"_mls.off\"\n",
    "    else:\n",
    "        output_mesh = output_path_and_filename\n",
    "    \n",
    "    if meshlab_script == \"\":\n",
    "        meshlab_script = str(pathlib.Path.cwd()) + \"/\" + \"remeshing_remove_non_man_edges.mls\"\n",
    "    \n",
    "    #print(\"meshlab_script = \" + str(meshlab_script))\n",
    "    #print(\"starting meshlabserver fixing non-manifolds\")\n",
    "    subprocess_result_1 = run_meshlab_script(meshlab_script,\n",
    "                      input_mesh,\n",
    "                      output_mesh)\n",
    "    #print(\"Poisson subprocess_result= \"+ str(subprocess_result_1))\n",
    "    \n",
    "    if str(subprocess_result_1)[-13:] != \"returncode=0)\":\n",
    "        raise Exception('neuron' + str(segment_id) + \n",
    "                         ' did not fix the manifold edges')\n",
    "    \n",
    "    return output_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1) Import mesh and find all the significant pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 81498689075439039_multiple_somas.off\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting up the mesh file and the output files\n",
    "\"\"\"\n",
    "\n",
    "total_test_meshes = [\n",
    "'81498689075439039_multiple_somas.off']\n",
    "\n",
    "output_file = total_test_meshes[0]\n",
    "folder_name = \"soma_extraction_tests_vp1/\" \n",
    "\n",
    "output_mesh_name = folder_name + output_file\n",
    "print(f\"Working on {output_file}\")\n",
    "\n",
    "indices = [i for i, a in enumerate(output_file) if a == \"_\"]\n",
    "indices\n",
    "seg_id_stripped = output_file[:indices[0]]\n",
    "n = dict(segment_id=int(seg_id_stripped))\n",
    "segment_id = int(seg_id_stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def run_poisson_surface_reconstruction(pre_largest_mesh_path,\n",
    "                                       segment_id = \"None\",\n",
    "                                      script_name = \"poisson_working_meshlab.mls\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Will run the poisson surface reconstruction\n",
    "    \n",
    "    \"\"\"\n",
    "    # run the meshlab server script\n",
    "\n",
    "    meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "    input_path =str(pathlib.Path.cwd()) + \"/\" +  pre_largest_mesh_path\n",
    "\n",
    "    indices = [i for i, a in enumerate(input_path) if a == \"_\"]\n",
    "    stripped_ending = input_path[:-4]\n",
    "\n",
    "    output_path = stripped_ending + \"_mls.off\"\n",
    "    # print(meshlab_script_path_and_name)\n",
    "    # print(input_path)\n",
    "    # print(output_path)\n",
    "    print(\"Running the mls function\")\n",
    "    meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_path,\n",
    "                                               output_path_and_filename=output_path,\n",
    "                                               segment_id=segment_id,\n",
    "                                               meshlab_script=meshlab_script_path_and_name)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fe4477d592c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnew_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_mesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_mesh_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmesh_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_watertight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trimesh/constants.py\u001b[0m in \u001b[0;36mtimed\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtimed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         log.debug('%s executed in %.4f seconds.',\n\u001b[1;32m    138\u001b[0m                   \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trimesh/exchange/load.py\u001b[0m in \u001b[0;36mload_mesh\u001b[0;34m(file_obj, file_type, resolver, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m                                           \u001b[0mfile_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                                           \u001b[0mresolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                           **kwargs)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/trimesh/exchange/off.py\u001b[0m in \u001b[0;36mload_off\u001b[0;34m(file_obj, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# convert faces to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# will fail on mixed garbage as FSM intended -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# save data as kwargs for a trimesh.Trimesh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loop that will compute the soma meshes and locations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ------------parameters------------------\n",
    "large_mesh_threshold = 600000\n",
    "large_mesh_threshold_inner = 40000\n",
    "soma_width_threshold = 0.35\n",
    "soma_size_threshold = 10000\n",
    "\n",
    "# ------------------------------\n",
    "\n",
    "new_mesh = trimesh.load_mesh(output_mesh_name)\n",
    "mesh_splits = new_mesh.split(only_watertight=False)\n",
    "\n",
    "#len(\"Total mesh splits = \" + str(mesh_splits))\n",
    "#get the largest mesh\n",
    "mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# sns.distplot(mesh_lengths)\n",
    "\n",
    "largest_index = np.where(mesh_lengths == np.max(mesh_lengths))\n",
    "largest_mesh = mesh_splits[largest_index][0]\n",
    "\n",
    "\"\"\" -- temporarily changing to the second largest mesh\"\"\"\n",
    "total_mesh_split_lengths = [len(k.faces) for k in mesh_splits]\n",
    "ordered_mesh_splits = mesh_splits[np.flip(np.argsort(total_mesh_split_lengths))]\n",
    "list_of_largest_mesh = [k for k in ordered_mesh_splits if len(k.faces) > large_mesh_threshold]\n",
    "\n",
    "print(f\"Total found significant pieces before Poisson = {list_of_largest_mesh}\")\n",
    "\n",
    "# total_soma_mesh = trimesh.Trimesh(vertices=np.array([]),\n",
    "#                                  triangles = np.array([]))\n",
    "\n",
    "total_soma_list = []\n",
    "total_classifier_list = []\n",
    "total_poisson_list = []\n",
    "\n",
    "#start iterating through \n",
    "for i,largest_mesh in enumerate(list_of_largest_mesh):\n",
    "    print(f\"----- working on large mesh #{i}: {largest_mesh}\")\n",
    "    \n",
    "    somas_found_in_big_loop = False\n",
    "\n",
    "    stripped_ending = output_mesh_name[:-4]\n",
    "    pre_largest_mesh_path = stripped_ending + \"_\" + str(i) + \"_largest_piece.off\"\n",
    "\n",
    "    largest_mesh.export(pre_largest_mesh_path)\n",
    "    print(\"done exporting\")\n",
    "    \n",
    "    output_path = run_poisson_surface_reconstruction(pre_largest_mesh_path)\n",
    "    \n",
    "    #---------------- Will carry out the cgal segmentation -------- #\n",
    "    #import the mesh\n",
    "    new_mesh_inner = trimesh.load_mesh(output_path)\n",
    "    \n",
    "    mesh_splits_inner = new_mesh_inner.split(only_watertight=False)\n",
    "    total_mesh_split_lengths_inner = [len(k.faces) for k in mesh_splits_inner]\n",
    "    ordered_mesh_splits_inner = mesh_splits_inner[np.flip(np.argsort(total_mesh_split_lengths_inner))]\n",
    "    list_of_largest_mesh_inner = [k for k in ordered_mesh_splits_inner if len(k.faces) > large_mesh_threshold_inner]\n",
    "    print(f\"Total found significant pieces AFTER Poisson = {list_of_largest_mesh}\")\n",
    "    \n",
    "    stripped_ending = output_path[:-4]\n",
    "\n",
    "    n_failed_inner_soma_loops = 0\n",
    "    for j, largest_mesh_inner in enumerate(list_of_largest_mesh_inner):\n",
    "\n",
    "        print(f\"----- working on mesh after poisson #{j}: {largest_mesh_inner}\")\n",
    "        \n",
    "        largest_mesh_path_inner = stripped_ending +\"_\" + str(j) + \"_largest_inner.off\"\n",
    "\n",
    "        #DON'T NEED THIS WRITE NOW BECAUSE IT ALREADY OUTPUTS THE MESH\n",
    "#         largest_mesh.export(largest_mesh_path_inner)\n",
    "#         print(\"done exporting\")\n",
    "        \n",
    "        # Starts the actual cgal segmentation:\n",
    "        \n",
    "        faces = np.array(largest_mesh_inner.faces)\n",
    "        verts = np.array(largest_mesh_inner.vertices)\n",
    "        #run the whole algorithm on the neuron to test\n",
    "        verts_labels, faces_labels, soma_value,classifier = wcda.extract_branches_whole_neuron(\n",
    "                            import_Off_Flag=False,\n",
    "                            segment_id=segment_id,\n",
    "                            vertices=verts,\n",
    "                             triangles=faces,\n",
    "                            pymeshfix_Flag=False,\n",
    "                             import_CGAL_Flag=False,\n",
    "                             return_Only_Labels=True,\n",
    "                             clusters=3,\n",
    "                             smoothness=0.2,\n",
    "                            soma_only=True,\n",
    "                            return_classifier = True\n",
    "                            )\n",
    "        \n",
    "        total_classifier_list.append(classifier)\n",
    "        total_poisson_list.append(largest_mesh_inner)\n",
    "\n",
    "        # Save all of the portions that resemble a soma\n",
    "        median_values = np.array([v[\"median\"] for k,v in classifier.sdf_final_dict.items()])\n",
    "        segmentation = np.array([k for k,v in classifier.sdf_final_dict.items()])\n",
    "\n",
    "        #order the compartments by greatest to smallest\n",
    "        sorted_medians = np.flip(np.argsort(median_values))\n",
    "        print(f\"segmentation[sorted_medians],median_values[sorted_medians] = {(segmentation[sorted_medians],median_values[sorted_medians])}\")\n",
    "        valid_soma_segments_width = [g for g,h in zip(segmentation[sorted_medians],median_values[sorted_medians]) if ((h > soma_width_threshold)\n",
    "                                                            and (classifier.sdf_final_dict[g][\"n_faces\"] > soma_size_threshold))]\n",
    "        \n",
    "        valid_soma_segments_width\n",
    "        if len(valid_soma_segments_width) > 0:\n",
    "            print(f\"      ------ Found {len(valid_soma_segments_width)} viable somas: {valid_soma_segments_width}\")\n",
    "            somas_found_in_big_loop = True\n",
    "            #get the meshes only if signfiicant length\n",
    "            labels_list = classifier.labels_list\n",
    "            for v in valid_soma_segments_width:\n",
    "                interest_labels = [k for k in labels_list if k == v]\n",
    "                soma_mesh = largest_mesh.submesh([interest_labels],append=True)\n",
    "                total_soma_list.append(v)\n",
    "\n",
    "            n_failed_inner_soma_loops = 0\n",
    "            \n",
    "        else:\n",
    "            n_failed_inner_soma_loops += 1\n",
    "            \n",
    "        \n",
    "        # --------------- KEEP TRACK IF FAILED TO FIND SOMA (IF TOO MANY FAILS THEN BREAK)\n",
    "        if n_failed_inner_soma_loops >= 2:\n",
    "            print(\"breaking inner loop because 2 soma fails in a row\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # --------------- KEEP TRACK IF FAILED TO FIND SOMA (IF TOO MANY FAILS THEN BREAK)\n",
    "    if somas_found_in_big_loop == False:\n",
    "        no_somas_found_in_big_loop += 1\n",
    "        if no_somas_found_in_big_loop >= 2:\n",
    "            print(\"breaking because 2 fails in a row in big loop\")\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        no_somas_found_in_big_loop = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_soma_segments_width_new = [g for g,h in zip(segmentation[sorted_medians],median_values[sorted_medians]) if ((h > soma_width_threshold)\n",
    "                                                            )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 39]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_soma_segments_width = [g for g,h in zip(segmentation[sorted_medians],median_values[sorted_medians]) if ((h > soma_width_threshold)\n",
    "                                                            and (classifier.sdf_final_dict[g][\"n_faces\"] > large_mesh_threshold_inner))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20078"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.sdf_final_dict[22][\"n_faces\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "labels_counter = Counter(faces_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_faces = np.where(faces_labels == 5.0)[0]\n",
    "soma_mesh = largest_mesh.submesh([soma_faces],append=True)\n",
    "soma_mesh.export(folder_name + str(n[\"segment_id\"]) + \"_soma.off\")\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_soma_faces = np.where(faces_labels != 5.0)[0]\n",
    "non_soma_mesh = largest_mesh.submesh([non_soma_faces],append=True)\n",
    "non_soma_mesh.export(folder_name + str(n[\"segment_id\"]) + \"_NON_soma.off\")\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the CGAL classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_values = np.array([v[\"median\"] for k,v in classifier.sdf_final_dict.items()])\n",
    "segmentation = np.array([k for k,v in classifier.sdf_final_dict.items()])\n",
    "\n",
    "#order the compartments by greatest to smallest\n",
    "sorted_medians = np.flip(np.argsort(median_values))\n",
    "median_values[sorted_medians],segmentation[sorted_medians]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the soma in ipyvolume\n",
    "import ipyvolume as ipv\n",
    "import matplotlib.colors as cl\n",
    "fig_2 = ipv.figure(figsize=(15,15))\n",
    "\n",
    "main_mesh = largest_mesh\n",
    "\n",
    "default_color = \"green\"\n",
    "\n",
    "interest_label = [7]\n",
    "color = [\"red\",\"red\",\"red\"]\n",
    "\n",
    "#interest_label = []\n",
    "#color = []\n",
    "\n",
    "labels_list = classifier.labels_list\n",
    "\n",
    "\n",
    "mesh = ipv.plot_trisurf(main_mesh.vertices[:,0],\n",
    "                   main_mesh.vertices[:,1],\n",
    "                   main_mesh.vertices[:,2],\n",
    "                    triangles=main_mesh.faces\n",
    "                   )\n",
    "mesh.color = cl.to_rgb(default_color)\n",
    "\n",
    "\n",
    "volume_max = np.max(main_mesh.vertices,axis=0)\n",
    "volume_min = np.min(main_mesh.vertices,axis=0)\n",
    "\n",
    "ranges = volume_max - volume_min\n",
    "index = [0,1,2]\n",
    "max_index = np.argmax(ranges)\n",
    "min_limits = [0,0,0]\n",
    "max_limits = [0,0,0]\n",
    "\n",
    "buffer = 10000\n",
    "for i in index:\n",
    "    if i == max_index:\n",
    "        min_limits[i] = volume_min[i] - buffer\n",
    "        max_limits[i] = volume_max[i] + buffer \n",
    "        continue\n",
    "    else:\n",
    "        difference = ranges[max_index] - ranges[i]\n",
    "        min_limits[i] = volume_min[i] - difference/2  - buffer\n",
    "        max_limits[i] = volume_max[i] + difference/2 + buffer\n",
    "\n",
    "#ipv.xyzlim(-2, 2)\n",
    "ipv.xlim(min_limits[0],max_limits[0])\n",
    "ipv.ylim(min_limits[1],max_limits[1])\n",
    "ipv.zlim(min_limits[2],max_limits[2])\n",
    "\n",
    "ipv.style.set_style_light()\n",
    "#ipv.style.box_off()\n",
    "#ipv.style.axes_off()\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the soma in ipyvolume\n",
    "import ipyvolume as ipv\n",
    "import matplotlib.colors as cl\n",
    "\n",
    "fig1 = ipv.figure(figsize=(15,15))\n",
    "\n",
    "main_mesh = largest_mesh\n",
    "\n",
    "default_color = \"green\"\n",
    "\n",
    "\n",
    "interest_label = [7]\n",
    "color = [\"red\"]*len(interest_label)\n",
    "\n",
    "#interest_label = []\n",
    "#color = []\n",
    "\n",
    "labels_list = classifier.labels_list\n",
    "\n",
    "\"\"\"\n",
    "#This method did not work for coloring the faces\n",
    "#compute the colors_list\n",
    "current_color_list = np.tile(np.array(cl.to_rgb(default_color)),(len(main_mesh.faces),1))\n",
    "\n",
    "for lab,c in zip(interest_label,color):\n",
    "    faces_of_interest = np.where(labels_list == lab)[0]\n",
    "    print(\"faces_of_interest = \" + str(faces_of_interest))\n",
    "    current_color_list[faces_of_interest,:] = np.tile(np.array(cl.to_rgb(c)),(len(faces_of_interest),1))\n",
    "\n",
    "#set the figure size\n",
    "ipv.figure(figsize=(15,15))\n",
    "mesh = ipv.plot_trisurf(main_mesh.vertices[:,0],\n",
    "                       main_mesh.vertices[:,1],\n",
    "                       main_mesh.vertices[:,2],\n",
    "                        triangles=main_mesh.faces,\n",
    "                       color=current_color_list\n",
    "                       )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for lab,c in zip(interest_label,color):\n",
    "    print(\"inside\")\n",
    "    current_mesh = largest_mesh.submesh(np.where(labels_list == lab),append=True)\n",
    "    print(\"current_mesh size = \" + str(len(current_mesh.vertices)))\n",
    "    mesh_interested = ipv.plot_trisurf(current_mesh.vertices[:,0],\n",
    "                       current_mesh.vertices[:,1],\n",
    "                       current_mesh.vertices[:,2],\n",
    "                        triangles=current_mesh.faces\n",
    "                       )\n",
    "    mesh_interested.color = cl.to_rgb(c)\n",
    "\n",
    "#assemble mesh that was not in the labels of interest\n",
    "not_interest_labels = [k for k in labels_list if k not in interest_label]\n",
    "current_mesh = largest_mesh.submesh([not_interest_labels],append=True)\n",
    "mesh_not_interested = ipv.plot_trisurf(current_mesh.vertices[:,0],\n",
    "                   current_mesh.vertices[:,1],\n",
    "                   current_mesh.vertices[:,2],\n",
    "                    triangles=current_mesh.faces\n",
    "                   )\n",
    "mesh_not_interested.color = cl.to_rgb(default_color)\n",
    "\n",
    "\n",
    "volume_max = np.max(main_mesh.vertices,axis=0)\n",
    "volume_min = np.min(main_mesh.vertices,axis=0)\n",
    "\n",
    "ranges = volume_max - volume_min\n",
    "index = [0,1,2]\n",
    "max_index = np.argmax(ranges)\n",
    "min_limits = [0,0,0]\n",
    "max_limits = [0,0,0]\n",
    "\n",
    "buffer = 10000\n",
    "for i in index:\n",
    "    if i == max_index:\n",
    "        min_limits[i] = volume_min[i] - buffer\n",
    "        max_limits[i] = volume_max[i] + buffer \n",
    "        continue\n",
    "    else:\n",
    "        difference = ranges[max_index] - ranges[i]\n",
    "        min_limits[i] = volume_min[i] - difference/2  - buffer\n",
    "        max_limits[i] = volume_max[i] + difference/2 + buffer\n",
    "\n",
    "#ipv.xyzlim(-2, 2)\n",
    "ipv.xlim(min_limits[0],max_limits[0])\n",
    "ipv.ylim(min_limits[1],max_limits[1])\n",
    "ipv.zlim(min_limits[2],max_limits[2])\n",
    "\n",
    "ipv.style.set_style_light()\n",
    "#ipv.style.box_off()\n",
    "#ipv.style.axes_off()\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the center of the soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_center = soma_mesh.vertices.mean(axis=0).astype(\"float\")\n",
    "print(\"Poor man's center from just averagin vertices = \" + str(soma_center))\n",
    "print(\"Trimesh center of mass = \" + str(soma_mesh.center_mass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

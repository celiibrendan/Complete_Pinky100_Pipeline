{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import time\n",
    "import pymeshfix\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#for supressing the output\n",
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "#for error counting\n",
    "from collections import Counter\n",
    "\n",
    "#for reading in the new raw_skeleton files\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "#setting the address and the username\n",
    "dj.config['database.host'] = '10.28.0.34'\n",
    "dj.config['database.user'] = 'celiib'\n",
    "dj.config['database.password'] = 'newceliipass'\n",
    "dj.config['safemode']=True\n",
    "dj.config[\"display.limit\"] = 20\n",
    "\n",
    "schema = dj.schema('microns_pinky_fc')\n",
    "pinky = dj.create_virtual_module('pinky', 'microns_pinky')\n",
    "pinky_fc = dj.create_virtual_module('pinky_fc','microns_pinky_fc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the pearson correlation and cosine similarity while accounting for the corner cases\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "\n",
    "def check_nan(v1,v2):\n",
    "    if np.isscalar(v1):\n",
    "        if np.isnan(v1) or v1 == None:\n",
    "            return True\n",
    "    else:\n",
    "        if True in np.isnan(v1) or len(v1) <= 0:\n",
    "            return True\n",
    "        \n",
    "    if np.isscalar(v2):\n",
    "        if np.isnan(v2) or v2 == None:\n",
    "            return True\n",
    "    else:\n",
    "        if True in np.isnan(v2) or len(v2) <= 0:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def find_pearson(v1,v2):\n",
    "    v1 = v1.astype(\"float\")\n",
    "    v2 = v2.astype(\"float\")\n",
    "#     print(\"v1 = \" + str(v1))\n",
    "#     print(\"v2 = \" + str(v2))\n",
    "    if check_nan(v1,v2):\n",
    "        return np.NaN\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        #perform the pearson correlation\n",
    "        if v1.size <= 1 or v2.size <= 1:\n",
    "            return np.NaN\n",
    "        if np.array_equal(v1,v2):\n",
    "            return 1\n",
    "        elif abs(sum(v1 - v2)) >= v1.size:\n",
    "            return -1\n",
    "        else:\n",
    "            corr_conversion, p_value_conversion = pearsonr(v1, v2)\n",
    "            return corr_conversion\n",
    "\n",
    "def find_cosine(v1,v2):\n",
    "    v1 = v1.astype(\"float\")\n",
    "    v2 = v2.astype(\"float\")\n",
    "    if check_nan(v1,v2):\n",
    "        return np.NaN\n",
    "    if v1.size <= 1 or v2.size <= 1:\n",
    "            return np.NaN\n",
    "    if np.array_equal(v1,v2):\n",
    "        return 1\n",
    "    elif abs(sum(v1 - v2)) == v1.size:\n",
    "        return 0\n",
    "    else:\n",
    "        v1 = v1.reshape(1,len(v1))\n",
    "        v2 = v2.reshape(1,len(v2))\n",
    "        return cosine_similarity(v1, v2)[0][0]\n",
    "\n",
    "def find_binary_sim(v1,v2):\n",
    "    v1 = v1.astype(\"float\")\n",
    "    v2 = v2.astype(\"float\")\n",
    "    if check_nan(v1,v2):\n",
    "            return np.NaN\n",
    "    a = np.dot(v1,v2)\n",
    "    b = np.dot(1-v1,v2)\n",
    "    c = np.dot(v1,1-v2)\n",
    "    d = np.dot(1-v1,1-v2)\n",
    "    \n",
    "    return (a)/(a + b + c + d),(a + d)/(a + b + c + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "        <style type=\"text/css\">\n",
       "            .Relation{\n",
       "                border-collapse:collapse;\n",
       "            }\n",
       "            .Relation th{\n",
       "                background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "                font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "            }\n",
       "            .Relation td{\n",
       "                padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "            }\n",
       "            .Relation tr:nth-child(odd){\n",
       "                background: #ffffff;\n",
       "            }\n",
       "            .Relation tr:nth-child(even){\n",
       "                background: #f3f1ff;\n",
       "            }\n",
       "            /* Tooltip container */\n",
       "            .djtooltip {\n",
       "            }\n",
       "            /* Tooltip text */\n",
       "            .djtooltip .djtooltiptext {\n",
       "                visibility: hidden;\n",
       "                width: 120px;\n",
       "                background-color: black;\n",
       "                color: #fff;\n",
       "                text-align: center;\n",
       "                padding: 5px 0;\n",
       "                border-radius: 6px;\n",
       "                /* Position the tooltip text - see examples below! */\n",
       "                position: absolute;\n",
       "                z-index: 1;\n",
       "            }\n",
       "            #primary {\n",
       "                font-weight: bold;\n",
       "                color: black;\n",
       "            }\n",
       "\n",
       "            #nonprimary {\n",
       "                font-weight: normal;\n",
       "                color: white;\n",
       "            }\n",
       "\n",
       "            /* Show the tooltip text when you mouse over the tooltip container */\n",
       "            .djtooltip:hover .djtooltiptext {\n",
       "                visibility: visible;\n",
       "            }\n",
       "        </style>\n",
       "        \n",
       "        <b></b>\n",
       "            <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "            <table border=\"1\" class=\"Relation\">\n",
       "                <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">segmentation</p>\n",
       "                                <span class=\"djtooltiptext\">segmentation id</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">segment_id</p>\n",
       "                                <span class=\"djtooltiptext\">segment id unique within each Segmentation</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">cluster_id</p>\n",
       "                                <span class=\"djtooltiptext\">numeric label of cluster</span>\n",
       "                            </div> </th> </tr> </thead>\n",
       "                <tbody> <tr> <td>3</td>\n",
       "<td>648518346341352081</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341352417</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341352790</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341353409</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341353780</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341353907</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341354590</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341354681</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341355016</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341355554</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341355936</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341356158</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341356736</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341356752</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341356798</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341356908</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341357287</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341357400</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341357454</td>\n",
       "<td>3</td></tr><tr><td>3</td>\n",
       "<td>648518346341357529</td>\n",
       "<td>3</td> </tr> </tbody>\n",
       "            </table>\n",
       "            <p>...</p>\n",
       "            <p>Total: 1210</p></div>\n",
       "            "
      ],
      "text/plain": [
       "*segmentation  *segment_id    cluster_id    \n",
       "+------------+ +------------+ +------------+\n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "3              64851834634135 3             \n",
       "   ...\n",
       " (Total: 1210)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pinky.SpineClusters & \"cluster_id=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class ContactCorrelationShaft(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> pinky.Segment\n",
    "    segment_b :bigint unsigned #id of the postsynaptic neuron\n",
    "    ---\n",
    "    n_seg_a              :bigint unsigned #n_presyns contacting onto segment_id\n",
    "    n_seg_b              :bigint unsigned #n_presyns contacting onto segment_b\n",
    "    n_seg_shared           :bigint unsigned #n_presyns contacting onto both segment_id and segment_b\n",
    "    n_seg_union            :bigint unsigned #n_presyns contacting either segment_id or segment_b\n",
    "    n_seg_shared_converted :bigint unsigned #n_presyns contacting onto both and converting on at least 1 postsyn\n",
    "    n_seg_a_converted      :bigint unsigned #n_presyns contacting onto both and converting on postsyna a\n",
    "    n_seg_a_converted_prop=null :float           #proportion of n_presyns contacting onto both which convert at least onto postsyna a\n",
    "    n_seg_b_converted      :bigint unsigned #n_presyns contacting onto both and converting on postsyna b\n",
    "    n_seg_b_converted_prop=null :float           #proportion of n_presyns contacting onto both which convert at least onto postsyna b\n",
    "    binary_conversion_pearson=null :float   #pearson correlation for binary n_synapse/n_contact rate\n",
    "    binary_conversion_cosine=null :float    #cosine similarity correlation for binary n_synapse/n_contact rate\n",
    "    binary_conv_jaccard_ones_ratio=null :float   #a / (a + b + c  + d) for jaccard similarity of binary conversion rate\n",
    "    binary_conv_jaccard_matching_ratio=null :float  # ( a + d )/ (a + b + c  + d) for jaccard similarity of binary conversion rate\n",
    "    conversion_pearson=null :float          #Pearson correlation for n_synapse/n_contact rate\n",
    "    conversion_cosine=null :float           #cosine similarity for n_synapse/n_contact rate\n",
    "    density_pearson=null :float             #Pearson correlation for n_synapse/postsyn_length rate\n",
    "    density_cosine=null :float              #cosine similarity for n_synapse/postsyn_length rate\n",
    "    synapse_volume_mean_pearson=null :float     #Pearson correlation for mean of synaptic volume\n",
    "    synapse_volume_mean_cosine=null :float      #cosine similarity for mean of synaptic volume\n",
    "    synapse_vol_density_pearson=null :float         #Pearson correlation for n_synapses*synapse_sizes_mean/postsyn_length rate\n",
    "    synapse_vol_density_cosine=null :float          #cosine similarity for n_synapses*synapse_sizes_mean/postsyn_length rate\n",
    "    binary_conversion_pearson_converted=null :float   #pearson correlation for binary n_synapse/n_contact rate for axon group with at least 1 conversion\n",
    "    binary_conversion_cosine_converted=null :float    #cosine similarity correlation for binary n_synapse/n_contact rate for axon group with at least 1 conversion\n",
    "    binary_conv_jaccard_ones_ratio_converted=null :float   #a / (a + b + c  + d) for jaccard similarity of binary conversion rate with at least 1 conversion\n",
    "    binary_conv_jaccard_matching_ratio_converted=null :float  # ( a + d )/ (a + b + c  + d) for jaccard similarity of binary conversion rate with at least 1 conversion\n",
    "    conversion_pearson_converted=null :float          #Pearson correlation for n_synapse/n_contact rate for axon group with at least 1 conversion\n",
    "    conversion_cosine_converted=null :float           #cosine similarity for n_synapse/n_contact rate for axon group with at least 1 conversion\n",
    "    density_pearson_converted=null :float             #Pearson correlation for n_synapse/postsyn_length rate for axon group with at least 1 conversion\n",
    "    density_cosine_converted=null :float              #cosine similarity for n_synapse/postsyn_length rate for axon group with at least 1 conversion\n",
    "    synapse_volume_mean_pearson_converted=null :float     #Pearson correlation for mean of synaptic volume for axon group with at least 1 conversion\n",
    "    synapse_volume_mean_cosine_converted=null :float      #cosine similarity for mean of synaptic volume for axon group with at least 1 conversion\n",
    "    synapse_vol_density_pearson_converted=null :float         #Pearson correlation for n_synapses*synapse_sizes_mean/postsyn_length rate for axon group with at least 1 conversion\n",
    "    synapse_vol_density_cosine_converted=null :float          #cosine similarity for n_synapses*synapse_sizes_mean/postsyn_length rate for axon group with at least 1 conversion\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    key_source = pinky.Segmentation & pinky.CurrentSegmentation\n",
    "    \n",
    "    def make(self,key):\n",
    "        #Retrieves the PrePost table that will be using in an all in one insertion (MAY HAVE TO ADJUST FOR BIGGER DATA SETS IN FUTURE)\n",
    "        prepost_data = (pinky_fc.ContactPrePost & (pinky.SpineClusters & \"cluster_id=3\")).proj(\"postsyn\",\"total_contact_conversion\",\n",
    "                \"total_contact_density\",\"total_synapse_sizes_mean\",\n",
    "                syn_density=\"(total_n_synapses*total_synapse_sizes_mean)/total_postsyn_length\",\n",
    "                presyn=\"segment_id\").fetch()\n",
    "        df = pd.DataFrame(prepost_data)\n",
    "\n",
    "        #gets all the combinations of postsyn-postsyn without any repeats\n",
    "        targets = (dj.U(\"postsyn\") & pinky.SkeletonContact).proj(segment_id=\"postsyn\") - pinky.SegmentExclude\n",
    "        info = targets * targets.proj(segment_b='segment_id') & 'segment_id < segment_b'\n",
    "        segment_pairs = info.fetch()\n",
    "        \n",
    "        total_correlations = []\n",
    "\n",
    "        for i,posts in tqdm(enumerate(segment_pairs)):\n",
    "            index = 0\n",
    "            multiple = 96924\n",
    "            if i < index*multiple:\n",
    "                continue\n",
    "            if i > (index+1)*multiple:\n",
    "                break\n",
    "            \n",
    "            postsyn1,postsyn2 = posts\n",
    "            start_time = time.time()\n",
    "\n",
    "            #print(\"postsyn1 = \" + str(postsyn1))\n",
    "            #print(\"postsyn2 = \" + str(postsyn2))\n",
    "\n",
    "            #get all of the rows with postsyn 1 and 2 AKA find the number of presyns for each\n",
    "            df_1 = df[df[\"postsyn\"].to_numpy()==postsyn1]\n",
    "            df_2 = df[df[\"postsyn\"].to_numpy()==postsyn2]\n",
    "\n",
    "            #reduce both tables down to common presyns\n",
    "            df_1_common = df_1[df_1[\"presyn\"].isin(df_2[\"presyn\"].to_numpy())].sort_values(by=['presyn'])\n",
    "            df_2_common = df_2[df_2[\"presyn\"].isin(df_1[\"presyn\"].to_numpy())].sort_values(by=['presyn'])\n",
    "\n",
    "\n",
    "            ###########------------------------------------------------------###########\n",
    "            #need to get the common axons that have at least one converted contact on one of the postsyns\n",
    "            \"\"\"pseudocode\n",
    "            #get the conversion rates for both tables\n",
    "            #add them up\n",
    "            #get the indices that are greater than 0\n",
    "            #get the presyn ids that match those rows\n",
    "            #further restrict both groups by those ids\n",
    "            \"\"\"\n",
    "\n",
    "            #get both of their conversion rates\n",
    "            test_1_conv = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "            test_2_conv = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "\n",
    "            test_1_presyn = df_1_common[\"presyn\"].to_numpy()\n",
    "            new_presyns = test_1_presyn[(test_1_conv + test_2_conv) > 0]\n",
    "\n",
    "            df_1_common_converted = df_1_common[df_1_common[\"presyn\"].isin(new_presyns)]\n",
    "            df_2_common_converted = df_2_common[df_2_common[\"presyn\"].isin(new_presyns)]\n",
    "            ###########------------------------------------------------------###########\n",
    "\n",
    "\n",
    "            #finds the number of segments, shared_segments and union segments\n",
    "            n_seg_a = df_1.shape[0]\n",
    "            n_seg_b = df_2.shape[0]\n",
    "            n_seg_shared = df_1_common.shape[0]\n",
    "            n_seg_shared_converted = df_1_common_converted.shape[0]\n",
    "            n_seg_union = n_seg_a + n_seg_b - n_seg_shared\n",
    "            \n",
    "            \n",
    "            #get the number and proportion on presyns that convert onto each segment inside the converted axon group\n",
    "            if n_seg_shared_converted > 0:\n",
    "                test_1_conv[test_1_conv>1] = 1\n",
    "                n_seg_a_converted = sum(np.ceil(test_1_conv))\n",
    "                test_2_conv[test_2_conv>1] = 1\n",
    "                n_seg_b_converted = sum(np.ceil(test_2_conv))\n",
    "\n",
    "                n_seg_a_converted_prop = n_seg_a_converted/n_seg_shared_converted\n",
    "                n_seg_b_converted_prop = n_seg_b_converted/n_seg_shared_converted\n",
    "            else:\n",
    "                n_seg_a_converted = 0\n",
    "                n_seg_b_converted = 0\n",
    "                n_seg_a_converted_prop = np.NaN\n",
    "                n_seg_b_converted_prop = np.NaN\n",
    "     \n",
    "            dict_segmenation=3\n",
    "            dict_segment_id=postsyn1\n",
    "            dict_segment_b=postsyn2\n",
    "            #initialize the dictionary that will be saved:\n",
    "            corr_dict = dict(segmentation=3,segment_id=postsyn1,\n",
    "                                          segment_b=postsyn2,\n",
    "                                          n_seg_a=n_seg_a,\n",
    "                                            n_seg_b=n_seg_b,\n",
    "                                            n_seg_shared=n_seg_shared,\n",
    "                                            n_seg_shared_converted=n_seg_shared_converted,\n",
    "                                            n_seg_union=n_seg_union,\n",
    "                                            n_seg_a_converted=n_seg_a_converted,\n",
    "                                            n_seg_b_converted=n_seg_b_converted,\n",
    "                                            n_seg_a_converted_prop=n_seg_a_converted_prop,\n",
    "                                            n_seg_b_converted_prop=n_seg_b_converted_prop)\n",
    "\n",
    "\n",
    "            #initialize the variables that need to be set in the dictionary\n",
    "\n",
    "\n",
    "            #ones that are set by 1st group\n",
    "            binary_conversion_pearson = np.NaN\n",
    "            binary_conversion_cosine = np.NaN\n",
    "            binary_conv_jaccard_ones_ratio = np.NaN\n",
    "            binary_conv_jaccard_matching_ratio = np.NaN\n",
    "            conversion_pearson = np.NaN\n",
    "            conversion_cosine = np.NaN\n",
    "            density_pearson = np.NaN\n",
    "            density_cosine = np.NaN\n",
    "            synapse_volume_mean_pearson = np.NaN\n",
    "            synapse_volume_mean_cosine = np.NaN\n",
    "            synapse_vol_density_pearson = np.NaN\n",
    "            synapse_vol_density_cosine = np.NaN\n",
    "\n",
    "            #ones that are set by 2nd group\n",
    "            binary_conversion_pearson_converted = np.NaN\n",
    "            binary_conversion_cosine_converted = np.NaN\n",
    "            binary_conv_jaccard_ones_ratio_converted = np.NaN\n",
    "            binary_conv_jaccard_matching_ratio_converted = np.NaN\n",
    "            conversion_pearson_converted = np.NaN\n",
    "            conversion_cosine_converted = np.NaN\n",
    "            density_pearson_converted = np.NaN\n",
    "            density_cosine_converted = np.NaN\n",
    "            synapse_volume_mean_pearson_converted = np.NaN\n",
    "            synapse_volume_mean_cosine_converted = np.NaN\n",
    "            synapse_vol_density_pearson_converted = np.NaN\n",
    "            synapse_vol_density_cosine_converted = np.NaN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (not df_1_common.to_numpy().any()) or (not df_2_common.to_numpy().any()):\n",
    "                #total_correlations.append(corr_dict)\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                #retrieve the conversion rates\n",
    "                df_1_common_conversion = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "                df_2_common_conversion = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "                \n",
    "                #calculate the binary conversion rates\n",
    "                df_1_common_binary_conversion = np.copy(df_1_common_conversion)\n",
    "                df_2_common_binary_conversion = np.copy(df_2_common_conversion)\n",
    "\n",
    "\n",
    "                df_1_common_binary_conversion[df_1_common_binary_conversion>0] = 1.0\n",
    "                df_2_common_binary_conversion[df_2_common_binary_conversion>0] = 1.0\n",
    "                \n",
    "                #retrieve the synapse/postsyn_len\n",
    "                df_1_common_density = df_1_common[\"total_contact_density\"].to_numpy()\n",
    "                df_2_common_density = df_2_common[\"total_contact_density\"].to_numpy()\n",
    "\n",
    "                #retrieve mean of synapse_size\n",
    "                df_1_common_synaptic_size = df_1_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "                df_2_common_synaptic_size = df_2_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "\n",
    "                #retrieve (total_n_synapses*total_synapse_sizes_mean)/total_postsyn_length\n",
    "                df_1_common_syn_density = df_1_common[\"syn_density\"].to_numpy()\n",
    "                df_2_common_syn_density = df_2_common[\"syn_density\"].to_numpy()\n",
    "\n",
    "\n",
    "                binary_conversion_pearson = find_pearson(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                binary_conversion_cosine = find_cosine(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                \n",
    "                #new added metric for the binary calculations based on jacard_similarity\n",
    "                binary_conv_jaccard_ones_ratio,binary_conv_jaccard_matching_ratio = find_binary_sim(df_1_common_binary_conversion,df_2_common_binary_conversion)\n",
    "                \n",
    "                conversion_pearson = find_pearson(df_1_common_conversion, df_2_common_conversion)\n",
    "                conversion_cosine = find_cosine(df_1_common_conversion, df_2_common_conversion)\n",
    "                density_pearson = find_pearson(df_1_common_density, df_2_common_density)\n",
    "                density_cosine = find_cosine(df_1_common_density, df_2_common_density)\n",
    "                synapse_volume_mean_pearson = find_pearson(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                synapse_volume_mean_cosine = find_cosine(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                synapse_vol_density_pearson = find_pearson(df_1_common_syn_density, df_2_common_syn_density)\n",
    "                synapse_vol_density_cosine = find_cosine(df_1_common_syn_density, df_2_common_syn_density)\n",
    "\n",
    "                \n",
    "                ####reset the df_1_common and df_1_common to reuse code\n",
    "                df_1_common = df_1_common_converted\n",
    "                df_2_common = df_2_common_converted\n",
    "\n",
    "                if (not df_1_common.to_numpy().any()) or (not df_2_common.to_numpy().any()):\n",
    "                    #print(\"none_in_converted\")\n",
    "                    pass\n",
    "                else:\n",
    "                    df_1_common_conversion = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "                    df_2_common_conversion = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "\n",
    "                    df_1_common_binary_conversion = np.copy(df_1_common_conversion)\n",
    "                    df_2_common_binary_conversion = np.copy(df_2_common_conversion)\n",
    "\n",
    "\n",
    "                    df_1_common_binary_conversion[df_1_common_binary_conversion>0] = 1.0\n",
    "                    df_2_common_binary_conversion[df_2_common_binary_conversion>0] = 1.0\n",
    "\n",
    "                    df_1_common_density = df_1_common[\"total_contact_density\"].to_numpy()\n",
    "                    df_2_common_density = df_2_common[\"total_contact_density\"].to_numpy()\n",
    "\n",
    "\n",
    "                    df_1_common_synaptic_size = df_1_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "                    df_2_common_synaptic_size = df_2_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "\n",
    "                    df_1_common_syn_density = df_1_common[\"syn_density\"].to_numpy()\n",
    "                    df_2_common_syn_density = df_2_common[\"syn_density\"].to_numpy()\n",
    "\n",
    "                    \n",
    "\n",
    "                    binary_conversion_pearson_converted = find_pearson(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                    binary_conversion_cosine_converted = find_cosine(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                    \n",
    "                    #new added metric for the binary calculations based on jacard_similarity\n",
    "                    binary_conv_jaccard_ones_ratio_converted,binary_conv_jaccard_matching_ratio_converted = find_binary_sim(df_1_common_binary_conversion,df_2_common_binary_conversion)\n",
    "                \n",
    "                    conversion_pearson_converted = find_pearson(df_1_common_conversion, df_2_common_conversion)\n",
    "                    conversion_cosine_converted = find_cosine(df_1_common_conversion, df_2_common_conversion)\n",
    "                    density_pearson_converted = find_pearson(df_1_common_density, df_2_common_density)\n",
    "                    density_cosine_converted = find_cosine(df_1_common_density, df_2_common_density)\n",
    "                    synapse_volume_mean_pearson_converted = find_pearson(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                    synapse_volume_mean_cosine_converted = find_cosine(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                    synapse_vol_density_pearson_converted = find_pearson(df_1_common_syn_density, df_2_common_syn_density)\n",
    "                    synapse_vol_density_cosine_converted = find_cosine(df_1_common_syn_density, df_2_common_syn_density)\n",
    "\n",
    "\n",
    "\n",
    "            corr_dict[\"binary_conversion_pearson\"] = binary_conversion_pearson\n",
    "            corr_dict[\"binary_conversion_cosine\"] = binary_conversion_cosine\n",
    "            corr_dict[\"binary_conv_jaccard_ones_ratio\"] = binary_conv_jaccard_ones_ratio\n",
    "            corr_dict[\"binary_conv_jaccard_matching_ratio\"] = binary_conv_jaccard_matching_ratio\n",
    "            corr_dict[\"conversion_pearson\"] = conversion_pearson\n",
    "            corr_dict[\"conversion_cosine\"] = conversion_cosine\n",
    "            corr_dict[\"density_pearson\"] = density_pearson\n",
    "            corr_dict[\"density_cosine\"] = density_cosine\n",
    "            corr_dict[\"synapse_volume_mean_pearson\"] = synapse_volume_mean_pearson\n",
    "            corr_dict[\"synapse_volume_mean_cosine\"] = synapse_volume_mean_cosine\n",
    "            corr_dict[\"synapse_vol_density_pearson\"] = synapse_vol_density_pearson\n",
    "            corr_dict[\"synapse_vol_density_cosine\"] = synapse_vol_density_cosine\n",
    "\n",
    "            corr_dict[\"binary_conversion_pearson_converted\"] = binary_conversion_pearson_converted\n",
    "            corr_dict[\"binary_conversion_cosine_converted\"] = binary_conversion_cosine_converted\n",
    "            corr_dict[\"binary_conv_jaccard_ones_ratio_converted\"] = binary_conv_jaccard_ones_ratio_converted\n",
    "            corr_dict[\"binary_conv_jaccard_matching_ratio_converted\"] = binary_conv_jaccard_matching_ratio_converted\n",
    "            corr_dict[\"conversion_pearson_converted\"] = conversion_pearson_converted\n",
    "            corr_dict[\"conversion_cosine_converted\"] = conversion_cosine_converted\n",
    "            corr_dict[\"density_pearson_converted\"] = density_pearson_converted\n",
    "            corr_dict[\"density_cosine_converted\"] = density_cosine_converted\n",
    "            corr_dict[\"synapse_volume_mean_pearson_converted\"] = synapse_volume_mean_pearson_converted\n",
    "            corr_dict[\"synapse_volume_mean_cosine_converted\"] = synapse_volume_mean_cosine_converted\n",
    "            corr_dict[\"synapse_vol_density_pearson_converted\"] = synapse_vol_density_pearson_converted\n",
    "            corr_dict[\"synapse_vol_density_cosine_converted\"] = synapse_vol_density_cosine_converted\n",
    "\n",
    "\n",
    "\n",
    "            total_correlations.append(corr_dict)\n",
    "\n",
    "\n",
    "        #write all of the dictionaries to the database\n",
    "        self.insert(total_correlations,skip_duplicates=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5108/484620 [00:48<9:11:23, 14.49it/s] "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ContactCorrelationShaft.populate()\n",
    "print(f\"Total time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

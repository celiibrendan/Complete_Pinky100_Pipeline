{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Changes adding: \\n1) Want to filter spines to only return those that are for certain spines\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook takes Final_spine_extraction and adds the classification functionality of finding the spine heads/necks and generic spines\n",
    "\n",
    "Other things to add:\n",
    "1) Size multiplier\n",
    "2) Add in an error classifier for merge errors with spines\n",
    "\n",
    "Change the way that things are labels: \n",
    "1) Backbones currently labeled as -1,\n",
    "Head -2\n",
    "Neck -3\n",
    "Spine -4\n",
    "Error -5\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Changes adding: \n",
    "1) Want to filter spines to only return those that are for certain spines\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import sys\n",
    "#import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import trimesh\n",
    "\n",
    "#for cgal segmentation\n",
    "import cgal_Segmentation_Module as csm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGING BACK TO UNOPTIMIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyMesh(object):\n",
    "    \n",
    "    #generates the mapping of vertices to the faces that are touching it\n",
    "    def generate_verts_to_face_dictionary(self):\n",
    "        verts_to_Face = {}\n",
    "\n",
    "        #initialize the lookup dictionary as empty lists\n",
    "        faces_raw = self.mesh.faces\n",
    "        verts_raw = self.mesh.vertices\n",
    "        \n",
    "        for i,pre_vertex in enumerate(verts_raw):\n",
    "            verts_to_Face[i] = []\n",
    "        \n",
    "\n",
    "        for i,verts in enumerate(faces_raw):\n",
    "            #add the index to the list for each of the vertices\n",
    "            for vertex in verts:\n",
    "                verts_to_Face[vertex].append(i)\n",
    "\n",
    "        return verts_to_Face\n",
    "    \n",
    "    def __init__(self,mesh_file_location,file_name,error_threshold=700):\n",
    "    #import the mesh\n",
    "\n",
    "        full_path = str(Path(mesh_file_location) / Path(file_name))\n",
    "        self.mesh = trimesh.load_mesh(full_path)\n",
    "        self.verts_to_Face = self.generate_verts_to_face_dictionary()\n",
    "        self.error_threshold = error_threshold\n",
    "        #get the vertices to faces lookup table\n",
    "\n",
    "    def find_neighbors(self,current_label):\n",
    "        \"\"\"will return the number of neighbors that border the segment\"\"\"\n",
    "\n",
    "        #iterate over each face with that label\n",
    "        #   get the vertices of that face\n",
    "        #   get all the faces that have that vertice associated with that\n",
    "        #   get the labels of all of the neighbor faces, for each of these labels, add it to the neighbors \n",
    "        #list if it is not already there and doesn't match the label you are currently checking\n",
    "        #   return the list \n",
    "\n",
    "        labels_list = self.labels_list\n",
    "        verts_to_Face = self.verts_to_Face\n",
    "        faces_raw = self.mesh.faces\n",
    "        \n",
    "        \n",
    "        #get the indexes of all of the faces with that label that you want to find the neighbors for\n",
    "        index_list = []\n",
    "        for i,x in enumerate(labels_list):\n",
    "            if x == current_label:\n",
    "                index_list.append(i)\n",
    "\n",
    "        verts_checked = []\n",
    "        faces_checked = []\n",
    "        neighbors_list = []\n",
    "        neighbors_shared_vert = {}\n",
    "        for index in index_list:\n",
    "            \n",
    "            #get the vertices associates with face\n",
    "            vertices = faces_raw[index]\n",
    "\n",
    "            #get the faces associated with the vertices of that specific face\n",
    "            for vert in vertices:\n",
    "                #will only check each vertex once\n",
    "                if vert not in verts_checked:\n",
    "                    verts_checked.append(vert)\n",
    "                    faces_associated_vert = verts_to_Face[vert]\n",
    "                    for fac in faces_associated_vert:\n",
    "                        #make sure it is not a fellow face with the label who we are looking for the neighbors of\n",
    "                        if (fac not in index_list):\n",
    "                            #check to see if checked the the face already\n",
    "                            if (fac not in faces_checked):\n",
    "                                if(labels_list[fac] not in neighbors_list):\n",
    "                                    #add the vertex to the count of shared vertices\n",
    "                                    neighbors_shared_vert[labels_list[fac]] = 0 \n",
    "                                    #only store the faces that are different\n",
    "                                    neighbors_list.append(labels_list[fac])\n",
    "                                    #faces_to_check.append(fac)\n",
    "                                    #faces_to_check.insert(0, fac)\n",
    "                                #increment the number of times we have seen that label face\n",
    "                                neighbors_shared_vert[labels_list[fac]] = neighbors_shared_vert[labels_list[fac]] + 1\n",
    "                                #now add the face to the checked list\n",
    "                                faces_checked.append(fac)\n",
    "\n",
    "        #have all of the faces to check\n",
    "\n",
    "\n",
    "        number_of_faces = len(index_list)\n",
    "\n",
    "       \n",
    "\n",
    "        return neighbors_list,neighbors_shared_vert,number_of_faces\n",
    "\n",
    "\n",
    "    \n",
    "    def find_neighbors_optomized(self,current_label):\n",
    "        \n",
    "        \n",
    "        col1_member = self.adjacency_labels_col1  == current_label\n",
    "        col2_member = self.adjacency_labels_col2  == current_label\n",
    "        \n",
    "        logical_xor = np.logical_xor(col1_member,col2_member)\n",
    "\n",
    "        total_array = np.concatenate([self.adjacency_labels_col1[logical_xor],\n",
    "              self.adjacency_labels_col2[logical_xor]])\n",
    "        \n",
    "        neighbors_shared_vert = dict(Counter(total_array))\n",
    "        del neighbors_shared_vert[current_label]\n",
    "        \n",
    "        neighbors_list = list(neighbors_shared_vert.keys())\n",
    "        number_of_faces = self.labels_list_counter[current_label]\n",
    "\n",
    "        \n",
    "        return neighbors_list,neighbors_shared_vert,number_of_faces\n",
    "    \n",
    "    def smooth_backbone_vp4_optomized(self,backbone_width_threshold = 0.10,\n",
    "                                      max_backbone_threshold = 400,\n",
    "                                      backbone_threshold=40,\n",
    "                                      shared_vert_threshold=20,\n",
    "                                      shared_vert_threshold_new = 5,\n",
    "                                      backbone_neighbor_min=20):\n",
    "        #print(\"at beginning of smooth backbone vp4\")\n",
    "        \n",
    "        faces_raw = self.mesh.faces\n",
    "        verts_raw = self.mesh.vertices\n",
    "\n",
    "        #generate the easy lookup table\n",
    "        verts_to_Face = self.verts_to_Face\n",
    "        \n",
    "        #new optomized way of getting initial backbone list\n",
    "        total_items = np.array(sorted(self.labels_list_counter.items()))\n",
    "        keys = total_items[:,0]\n",
    "        values = total_items[:,1]\n",
    "        big_threshold = values >= max_backbone_threshold\n",
    "\n",
    "        small_threshold = values > backbone_threshold \n",
    "        sdf_threshold = np.array(list(self.sdf_final_dict.values())) >= backbone_width_threshold\n",
    "        total_list = np.logical_or(big_threshold,np.logical_and(small_threshold,sdf_threshold))\n",
    "        backbone_labels = keys[total_list]\n",
    " \n",
    "        list_flag = False\n",
    "    \n",
    "        if list_flag == True:\n",
    "            to_remove = []\n",
    "        else:\n",
    "            to_remove = set()\n",
    "\n",
    "        backbone_neighbors_dict = {}\n",
    "\n",
    "        \n",
    "        \n",
    "        #finds all of the neighbors and how many shared vertices they have\n",
    "        for bkbone in backbone_labels:\n",
    "            #find_neighbors Description of Return List:\n",
    "            #1) neighbors_list = labels of all bordering neighbors\n",
    "            #2) neighbors_shared_vert = number of faces for each bordering neighbor\n",
    "            #3) number_of_faces = total number of faces for current label\n",
    "            \n",
    "            #neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors_optomized(bkbone)\n",
    "            neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors(bkbone)\n",
    "            #neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors(self.labels_list,bkbone)\n",
    "            #add the neighbor stats and count to the dictionary corresponding to that label\n",
    "            backbone_neighbors_dict[bkbone] = dict(neighbors_list=neighbors_list,neighbors_shared_vert=neighbors_shared_vert,\n",
    "                number_of_faces=number_of_faces)\n",
    "            \n",
    "        \n",
    "         #beginning smoothing round that removes ones from backbone list\n",
    "        for i in range(0,5):\n",
    "            print(\"smoothing round \" + str(i+1))\n",
    "            counter = 0\n",
    "            #iterates through all the groups that were designated as backbones\n",
    "            for bkbone in backbone_labels:\n",
    "                if bkbone not in to_remove: #if not already designated to be removed\n",
    "\n",
    "                    #just retrieve the neighbor stats and count of faces that are already stored in dict\n",
    "                    neighbors_list = backbone_neighbors_dict[bkbone][\"neighbors_list\"]\n",
    "                    neighbors_shared_vert = backbone_neighbors_dict[bkbone][\"neighbors_shared_vert\"]\n",
    "                    number_of_faces = backbone_neighbors_dict[bkbone][\"number_of_faces\"]\n",
    "\n",
    "                    #counts up the number of shared vertices with backbone neighbors\n",
    "\n",
    "                    #FUTURE OPTOMIZATION\n",
    "                    backbone_count_flag = False\n",
    "                    neighbor_counter = 0 #TOTAL NUMBER OF BACKBONE NEIGHBORS\n",
    "                    #spine_neighbor_counter = 0\n",
    "                    total_backbone_shared_verts = 0 #TOTAL NUMBER OF FACES SHARED WITH BACKBONE\n",
    "                    for n in neighbors_list:         \n",
    "                        if (n in backbone_labels) and (n not in to_remove):\n",
    "                            neighbor_counter += 1\n",
    "                            total_backbone_shared_verts = total_backbone_shared_verts + neighbors_shared_vert[n] \n",
    "                    \n",
    "\n",
    "                    #FUTURE OPTOMIZATION\n",
    "                    #if meets requirement of shared verts then activates flag     \n",
    "                    if (total_backbone_shared_verts > shared_vert_threshold):\n",
    "                        backbone_count_flag = True\n",
    "\n",
    "                    #if there are no neighbor's that are backbones or does not share enough backbone vertices --> remove from backbone list\n",
    "                    if neighbor_counter <= 0 or backbone_count_flag == False:\n",
    "                        if list_flag == True:\n",
    "                            to_remove.append(bkbone)\n",
    "                        else:\n",
    "                            to_remove.add(bkbone)\n",
    "                        counter += 1\n",
    "\n",
    "\n",
    "            #if 1 or less non-backbones were converted to remove list then go ahead to the next step\n",
    "            if counter <= 1:\n",
    "                #print(\"counter caused the break\")\n",
    "                break\n",
    "\n",
    "        #print(\"just broke out of the loop\")\n",
    "        \"\"\"\n",
    "        Status: \n",
    "        1) Started with a tentative list of backbones\n",
    "        2) Removed some potential backbone lists\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #now go through and make sure no unconnected backbone segments\n",
    "\n",
    "        \"\"\"Pseudo-code for filtering algorithm\n",
    "        1) iterate through all of the backbone labels\n",
    "        2) Go get the neighbors of the backbone\n",
    "        3) Add all of the neighbors who are too part of the backbone to the backbones to check list\n",
    "        4) While backbone neighbor counter is less than the threshold or until list to check is empty\n",
    "        5) Pop the next neighbor off the list and add it to the neighbors check list\n",
    "        6) Get the neighbors of this guy\n",
    "        7) for each of neighbors that is also on the backbone BUT HASN'T BEEN CHECKED YET append them to the list to be check and update counter\n",
    "        8) continue at beginning of loop\n",
    "        -- once loop breaks\n",
    "        9) if the counter is below the threshold:\n",
    "            Add all of values in the neighbros already checked list to the new_to_remove\n",
    "        10) Use the new_backbone_labels and new_to_remove to rewrite the labels_list\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #gets the new backbones list without the ones removed\n",
    "        #new_backbone_labels = [bkbone for bkbone in backbone_labels if bkbone not in to_remove] #OPTOMIZE\n",
    "        new_backbone_labels = list(set(backbone_labels).difference(to_remove))\n",
    "        \n",
    "        list_flag = True\n",
    "        if list_flag == True:\n",
    "            new_to_remove = []\n",
    "            skip_labels = []\n",
    "        else:\n",
    "            new_to_remove = set({})\n",
    "            skip_labels = set({})\n",
    "        \n",
    "\n",
    "        for bkbonz in new_backbone_labels:\n",
    "            if bkbonz not in skip_labels:\n",
    "                #print(\"working on backbone = \" + str(bkbonz))\n",
    "                if list_flag == True:\n",
    "                    checked_backbone_neighbors = []\n",
    "                    backbone_neighbors_to_check = []\n",
    "                else:\n",
    "                    checked_backbone_neighbors = set()\n",
    "                    backbone_neighbors_to_check = set()\n",
    "                new_backbone_neighbor_counter = 0\n",
    "\n",
    "\n",
    "#                 if bkbonz not in backbone_neighbors_dict.keys(): #should never enter this loop..... #OPTOMIZE\n",
    "#                     neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors(labels_list,bkbonz)\n",
    "#                     backbone_neighbors_dict[bkbonz] = dict(neighbors_list=neighbors_list,neighbors_shared_vert=neighbors_shared_vert,\n",
    "#                         number_of_faces=number_of_faces)\n",
    "                #gets the stats of the neighbors and count of current label\n",
    "                neighbors_list = backbone_neighbors_dict[bkbonz][\"neighbors_list\"]\n",
    "                neighbors_shared_vert = backbone_neighbors_dict[bkbonz][\"neighbors_shared_vert\"]\n",
    "                number_of_faces = backbone_neighbors_dict[bkbonz][\"number_of_faces\"]\n",
    "\n",
    "                for bb in neighbors_list:\n",
    "                    #counts as viable backbone neighbor if meets following conditions:\n",
    "                    #1) In the new backbone list\n",
    "                    #2) hasn't been checked yet\n",
    "                    #3) not in the new ones to remove\n",
    "                    #4) The number of neighbors shared by that label is greater than raw threshold shared_vert_threshold_new\n",
    "\n",
    "                    #OPTOMIZE: don't need checked_backbone_neighbors\n",
    "                    if (bb in new_backbone_labels) and (bb not in checked_backbone_neighbors) and (bb not in new_to_remove) and neighbors_shared_vert[bb] > shared_vert_threshold_new:\n",
    "                        if list_flag == True:\n",
    "                            backbone_neighbors_to_check.append(bb)\n",
    "                        else:\n",
    "                            backbone_neighbors_to_check.add(bb)\n",
    "                        new_backbone_neighbor_counter += 1\n",
    "\n",
    "                #at this point have :\n",
    "                #1) total number of backbone neighbors: new_backbone_neighbor_counter\n",
    "                #2) backbone neighbors in list: backbone_neighbors_to_check\n",
    "\n",
    "                if list_flag == True:\n",
    "                    checked_backbone_neighbors = [nb for nb in backbone_neighbors_to_check]\n",
    "                else:\n",
    "                    checked_backbone_neighbors = set([nb for nb in backbone_neighbors_to_check])\n",
    "\n",
    "\n",
    "                #4) While backbone neighbor counter is less than the threshold or until list to check is empty\n",
    "\n",
    "                #Iterates through all possible backbone neighbors unitl:\n",
    "                # A) new_backbone_neighbor_counter is greater than set threshold of backbone_neighbor_min OR\n",
    "                # B) no more backbone neighbors to check\n",
    "\n",
    "                #Goal: counts the backbone chain with that label, so in hopes if not high enough then not backbone piece\n",
    "                while new_backbone_neighbor_counter < backbone_neighbor_min and len(backbone_neighbors_to_check)>0:\n",
    "                    #5) Pop the next neighbor off the list and add it to the neighbors check list\n",
    "                    if list_flag == True:\n",
    "                        current_backbone = backbone_neighbors_to_check.pop(0)\n",
    "                    else:\n",
    "                        current_backbone = backbone_neighbors_to_check.pop()\n",
    "                        \n",
    "                    if current_backbone not in checked_backbone_neighbors:\n",
    "                        if list_flag == True:\n",
    "                            checked_backbone_neighbors.append(current_backbone) #mark it as checked\n",
    "                        else:\n",
    "                            checked_backbone_neighbors.add(current_backbone)\n",
    "                    \n",
    "                    #gets the current neighbors and counts of one of the possible neighbor backbones\n",
    "                    neighbors_list = backbone_neighbors_dict[current_backbone][\"neighbors_list\"]\n",
    "                    neighbors_shared_vert = backbone_neighbors_dict[current_backbone][\"neighbors_shared_vert\"]\n",
    "                    number_of_faces = backbone_neighbors_dict[current_backbone][\"number_of_faces\"]\n",
    "\n",
    "                    #7) for each of neighbors that is also on the backbone BUT HASN'T BEEN CHECKED YET append them to the list to be check and update counter\n",
    "                    for bb in neighbors_list:\n",
    "                        if (bb in new_backbone_labels) and (bb not in checked_backbone_neighbors) and (bb not in new_to_remove) and neighbors_shared_vert[bb] > shared_vert_threshold_new:\n",
    "                            if list_flag == True:\n",
    "                                backbone_neighbors_to_check.append(bb)\n",
    "                            else:\n",
    "                                backbone_neighbors_to_check.add(bb)\n",
    "                            new_backbone_neighbor_counter += 1\n",
    "\n",
    "                #9) if the counter is below the threshold --> Add all of values in the neighbros already checked list to the new_to_remove\n",
    "                if new_backbone_neighbor_counter < backbone_neighbor_min:\n",
    "                    for bz in checked_backbone_neighbors:\n",
    "                        if bz not in new_to_remove:\n",
    "                            if list_flag == True:\n",
    "                                new_to_remove.append(bz)\n",
    "                            else:\n",
    "                                new_to_remove.add(bz)\n",
    "                            #print(\"removed \" + str(checked_backbone_neighbors))\n",
    "                else:\n",
    "                    \n",
    "                    if list_flag == True:\n",
    "                        skip_labels = skip_labels + checked_backbone_neighbors\n",
    "                    else:\n",
    "                        skip_labels.update(checked_backbone_neighbors)\n",
    "                    \n",
    "     \n",
    "        #go through and switch the label of hte \n",
    "        #may not want to relabel until the end in order to preserve the labels in case label a big one wrong\n",
    "\n",
    "        for i in range(0,len(self.labels_list)):\n",
    "            if self.labels_list[i] in new_backbone_labels and self.labels_list[i] not in new_to_remove:\n",
    "                self.labels_list[i] = -1\n",
    "\n",
    "\n",
    "        #print(\"Done backbone extraction\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #used for when not pulling from datajoint\n",
    "    def get_cgal_data_and_label_local_optomized(self,ob_name,labels_file,sdf_file):\n",
    "        \n",
    "        #reads int the cgal labels for all of the faces\n",
    "        triangles_labels = np.zeros(len(self.mesh.faces)).astype(\"int64\")\n",
    "        with open(labels_file) as csvfile:\n",
    "            #print(\"inside labels file\")\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                triangles_labels[i] = int(row[0])\n",
    "\n",
    "        \"\"\" OLD WAY OF GETTING BLENDER MESH OBJECT\n",
    "        ob = bpy.context.object\n",
    "        me = ob.data\n",
    "        verts_raw = ob.data.vertices\n",
    "        faces_raw = ob.data.polygons\n",
    "        \"\"\"\n",
    "        \n",
    "        #converts the cgal labels into a list that\n",
    "        # starts at 0\n",
    "        # progresses in order for all unique labels (so no numbers are skipped and don't have corresponding face)\n",
    "        verts_raw = self.mesh.vertices\n",
    "        faces_raw = self.mesh.faces\n",
    "        #gets a list of the unique labels\n",
    "        unique_segments = list(Counter(triangles_labels).keys())\n",
    "        segmentation_length = len(unique_segments) \n",
    "        unique_index_dict = {unique_segments[x]:x for x in range(0,segmentation_length )}\n",
    "        \n",
    "        labels_list = np.zeros(len(triangles_labels)).astype(\"int64\")\n",
    "        for i,tri in enumerate(triangles_labels):\n",
    "\n",
    "            #assembles the label list that represents all of the faces\n",
    "            labels_list[i] = int(unique_index_dict[tri])\n",
    "        \n",
    "        #print(\"triangles_labels = \" + str(Counter(triangles_labels)))\n",
    "        #print(\"labels_list = \" + str(Counter(labels_list)))\n",
    "        \n",
    "\n",
    "        #print(\"done with cgal_segmentation\")\n",
    "\n",
    "        #----------------------now return a dictionary of the sdf values like in the older function get_sdf_dictionary\n",
    "        #get the sdf values and store in sdf_labels\n",
    "        sdf_labels = np.zeros(len(labels_list)).astype(\"float\")\n",
    "        with open(sdf_file) as csvfile:\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                sdf_labels[i] = float(row[0])\n",
    "\n",
    "        \n",
    "        sdf_temp_dict = {}\n",
    "        for i in range(0,segmentation_length):\n",
    "            sdf_temp_dict[i] = []\n",
    "        \n",
    "        #print(\"sdf_temp_dict = \" + str(sdf_temp_dict))\n",
    "        #print(\"sdf_labels = \" + str(sdf_labels))\n",
    "        #iterate through the labels_list\n",
    "        for i,label in enumerate(labels_list):\n",
    "            sdf_temp_dict[label].append(sdf_labels[i])\n",
    "        #print(sdf_temp_dict)\n",
    "\n",
    "        #now calculate the stats on the sdf values for each label\n",
    "        sdf_final_dict = {}\n",
    "        \n",
    "        for dict_key,value in sdf_temp_dict.items():\n",
    "\n",
    "            #just want to store the median\n",
    "            sdf_final_dict[dict_key] = np.median(value)\n",
    "\n",
    "        self.sdf_final_dict = sdf_final_dict\n",
    "        self.labels_list = labels_list\n",
    "        self.labels_list_counter = Counter(labels_list)\n",
    "    \n",
    "        adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "        \n",
    "        self.adjacency_labels_col1, self.adjacency_labels_col2 = adjacency_labels.T\n",
    "        \n",
    "        return \n",
    "\n",
    "    def filter_Stubs_optomized(self,stub_threshold):\n",
    "        \n",
    "        #update the adjacency labels graph and counter\n",
    "        adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "        self.labels_list_counter = Counter(self.labels_list)\n",
    "        \n",
    "        #feed into the networkx graph generator\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(adjacency_labels)\n",
    "        \n",
    "\n",
    "        #removes the backbone node\n",
    "        G.remove_node(-1)\n",
    "        \n",
    "        #get all of the sub graphs once backbone node is deleted\n",
    "        sub_graphs = nx.connected_component_subgraphs(G)\n",
    "\n",
    "        \n",
    "        labels_to_remove = []\n",
    "        for i, sg in enumerate(sub_graphs):\n",
    "            node_sum = sum([self.labels_list_counter[n] for n in sg.nodes() if n != -1])\n",
    "            if node_sum < stub_threshold:\n",
    "                labels_to_remove = labels_to_remove + list(sg.nodes())\n",
    "\n",
    "        print(f\"removing {len(labels_to_remove)} labels with stub threshold {stub_threshold}\")\n",
    "\n",
    "        self.labels_list[np.isin(self.labels_list,labels_to_remove)] = -1\n",
    "\n",
    "    def get_spine_classification(self,labels_file_location,file_name,clusters,smoothness,\n",
    "                                    smooth_backbone_parameters,stub_threshold=50,size_multiplier=1): \n",
    "        \n",
    "        max_backbone_threshold = smooth_backbone_parameters.pop(\"max_backbone_threshold\",200) #the absolute size if it is greater than this then labeled as a possible backbone\n",
    "        backbone_threshold=smooth_backbone_parameters.pop(\"backbone_threshold\",40) #if the label meets the width requirements, these are the size requirements as well in order to be considered possible backbone\n",
    "        shared_vert_threshold=smooth_backbone_parameters.pop(\"shared_vert_threshold\",20) #raw number of backbone verts that need to be shared in order for label to possibly be a backbone\n",
    "        shared_vert_threshold_new = smooth_backbone_parameters.pop(\"shared_vert_threshold_new\",5)\n",
    "        backbone_width_threshold = smooth_backbone_parameters.pop(\"backbone_width_threshold\",0.10)  #the median sdf/width value the segment has to have in order to be considered a possible backbone \n",
    "        backbone_neighbor_min=smooth_backbone_parameters.pop(\"backbone_neighbor_min\",20) # number of backbones in chain in order for label to keep backbone status\n",
    "       \n",
    "        #multiply all of the size thresholds by the multiplier to help with decimations\n",
    "        max_backbone_threshold = max_backbone_threshold* size_multiplier\n",
    "        backbone_threshold = backbone_threshold* size_multiplier\n",
    "        shared_vert_threshold = shared_vert_threshold* size_multiplier\n",
    "        shared_vert_threshold_new = shared_vert_threshold_new* size_multiplier\n",
    "        \n",
    "        \n",
    "        print(\"\\nbackbone Parameters\")\n",
    "        print(f\"max_backbone_threshold = {max_backbone_threshold}, \\\n",
    "                            backbone_threshold = {backbone_threshold}, \\\n",
    "                            shared_vert_threshold = {shared_vert_threshold}, \\\n",
    "                            shared_vert_threshold_new = {shared_vert_threshold_new} \\\n",
    "                             backbone_width_threshold = {backbone_width_threshold}, \\\n",
    "                             backbone_neighbor_min = {backbone_neighbor_min}, \\\n",
    "                            size_multiplier = {size_multiplier}\")\n",
    "        \n",
    "        print(\"\\nstub_threshold = \" + str(stub_threshold))\n",
    "        \n",
    "        original_start_time = time.time()    \n",
    "        start_time = time.time()\n",
    "\n",
    "        faces_raw = self.mesh.faces        \n",
    "        file_name = file_name[:-4]\n",
    "\n",
    "        labels_file = str(Path(labels_file_location) / Path(file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \".csv\" ))  \n",
    "        sdf_file = str(Path(labels_file_location) / Path(file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \"_sdf.csv\" ))  \n",
    "        \n",
    "        #check to make sure thatcgal files were generated:\n",
    "        #clean up the cgal files \n",
    "        #clean up the cgal files \n",
    "        for f in [labels_file,sdf_file]:\n",
    "            if not os.path.isfile(f):\n",
    "                print(\"CGAL segmentation files weren't generated\")\n",
    "                raise ValueError(\"CGAL segmentation files weren't generated\")\n",
    "                return \"Failure\"\n",
    "        \n",
    "\n",
    "        self.get_cgal_data_and_label_local_optomized(file_name,labels_file,sdf_file)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(self.sdf_final_dict == [] and labels_list == []):\n",
    "            print(\"NO CGAL DATA FOR \" + str(neuron_ID))\n",
    "\n",
    "            return\n",
    "\n",
    "        print(\"getting cgal data--- %s seconds ---\" % (np.round(time.time() - start_time,5)))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.smooth_backbone_vp4_optomized(backbone_width_threshold,max_backbone_threshold = max_backbone_threshold,backbone_threshold=backbone_threshold,\n",
    "                shared_vert_threshold=shared_vert_threshold,\n",
    "                shared_vert_threshold_new = shared_vert_threshold_new,\n",
    "                 backbone_neighbor_min=backbone_neighbor_min)\n",
    "\n",
    "        #check and make sure that there exists a backbone, and if not then return that whole thing is error:\n",
    "        if -1 not in self.labels_list:\n",
    "            self.labels_list = np.ones(len(self.labels_list))*10\n",
    "            return \"No Backbone\"\n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"smoothing backbone--- %s seconds ---\" % (np.round(time.time() - start_time,5)))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.filter_Stubs_optomized(stub_threshold)\n",
    "        print(\"---removing stubs: %s seconds ---\" % (np.round(time.time() - start_time,5)))\n",
    "        \n",
    "        #clean up the cgal files \n",
    "        for f in [labels_file,sdf_file]:\n",
    "            os.remove(f)\n",
    "            \n",
    "        \n",
    "        #print(\"finished\")\n",
    "        print(\"Total spine extraction --- %s seconds ---\" % (np.round(time.time() - original_start_time,5)))\n",
    "        \n",
    "        status = \"Success\"\n",
    "        \n",
    "        return status\n",
    "    \n",
    "    \n",
    "    def extract_spines(self,labels_file_location,file_name,clusters,smoothness,\n",
    "                                       split_up_spines=True,shaft_mesh=False,**kwargs):\n",
    "        \n",
    "        \n",
    "        smooth_backbone_parameters = kwargs.pop('smooth_backbone_parameters', dict())\n",
    "        stub_threshold = kwargs.pop('stub_threshold', 40)\n",
    "\n",
    "        \n",
    "        \n",
    "        status = self.get_spine_classification(labels_file_location,file_name,clusters,\n",
    "                                      smoothness,smooth_backbone_parameters,stub_threshold)\n",
    "        \n",
    "        if status != \"Success\":\n",
    "            print(f\"spine classification did not execute properly with status {status}\")\n",
    "            return None\n",
    "        \n",
    "        spine_indexes = np.where(np.array(self.labels_list) != -1)\n",
    "        spine_meshes_whole = self.mesh.submesh(spine_indexes,append=True)\n",
    "        \n",
    "        individual_spines_seperated = []\n",
    "        individual_spines = []\n",
    "        temp_spines = spine_meshes_whole.split(only_watertight=False)\n",
    "        for spine in temp_spines:\n",
    "                if len(spine.faces) >= stub_threshold and len(spine.faces) < self.error_threshold:\n",
    "                    individual_spines_seperated.append(spine)\n",
    "        \n",
    "        \n",
    "        #decides if passing back spines as one whole mesh or seperate meshes\n",
    "        if split_up_spines==True:\n",
    "            individual_spines = individual_spines_seperated\n",
    "        else:\n",
    "            #recombine the individual spines back into one\n",
    "            if len(individual_spines_seperated) > 0:\n",
    "                individual_spines_addup = individual_spines_seperated[0]\n",
    "                for i in range(1,len(individual_spines_seperated)):\n",
    "                    individual_spines_addup = individual_spines_addup + individual_spines_seperated[i]\n",
    "\n",
    "                individual_spines = individual_spines_addup\n",
    "\n",
    "        #will also pass back the shaft of the mesh with the extracted spines\n",
    "        if shaft_mesh==False:\n",
    "            return individual_spines\n",
    "        else:\n",
    "            shaft_indexes = np.where(np.array(self.labels_list) == -1) \n",
    "            shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "            return individual_spines,shaft_mesh_whole\n",
    "\n",
    "    \"\"\"\n",
    "    ###------------------- Part that deal with classifying the parts of the spine --------------------- ###\n",
    "    ###------------------- Part that deal with classifying the parts of the spine --------------------- ###\n",
    "    ###------------------- Part that deal with classifying the parts of the spine --------------------- ###\n",
    "    \"\"\"\n",
    "\n",
    "    def update_label_list_dependencies(self):\n",
    "        \"\"\"\n",
    "        1) Things that need to be made sure that are updated\n",
    "        self.adjacency_labels_col1 \n",
    "        self.adjacency_labels_col2  \n",
    "\n",
    "        self.labels_list\n",
    "        self.labels_list_counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.labels_list_counter = Counter(self.labels_list)\n",
    "    \n",
    "        adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "        \n",
    "        self.adjacency_labels_col1, self.adjacency_labels_col2 = adjacency_labels.T\n",
    "        \n",
    "    \n",
    "    def get_split_heads_vp2(self,current_label,current_index, path,connections,shared_vertices,\n",
    "                            mesh_number,\n",
    "                            sdf_final_dict,\n",
    "                            absolute_head_threshold,\n",
    "                           split_head_threshold = 0.35):\n",
    "        \"\"\"\n",
    "        parameters: percentage of shared verts with other head/total mesh segment to see if a shared head \n",
    "        (if above this number then shared head)\n",
    "        \"\"\"\n",
    "        final_split_heads = [current_label]\n",
    "\n",
    "        split_head_threshold = split_head_threshold\n",
    "        #underneath_threshold = 0.20\n",
    "\n",
    "        #the only solid number threshold\n",
    "        split_head_absolute_threshold = 8\n",
    "\n",
    "        heads_to_check = True\n",
    "        while heads_to_check:\n",
    "            #1) go to the next label below it\n",
    "            if(current_index < (len(path)-1)):\n",
    "                next_index = current_index + 1\n",
    "                next_label = path[next_index]\n",
    "\n",
    "            if(next_label == -1):\n",
    "                #no_more_split_head_Flag = True\n",
    "                break\n",
    "\n",
    "            #ask if this next satisfies  1) enough shared verts?  2) SDF head possible?\n",
    "            verts_sharing_index = connections[current_label].index(next_label)\n",
    "            verts_sharing = shared_vertices[current_label][verts_sharing_index]\n",
    "\n",
    "            #print(\"split share for faces \" + str(current_label) + \" \" +str(next_label) + \"=\"+str(verts_sharing/mesh_number[current_label]))\n",
    "            sdf_guess = self.sdf_likely_category(next_label,next_index,path,True,self.sdf_final_dict,connections,mesh_number,absolute_head_threshold)\n",
    "            if verts_sharing/mesh_number[current_label] > split_head_threshold and  sdf_guess == \"head\" and mesh_number[next_label] > split_head_absolute_threshold:\n",
    "                #add next label to the list\n",
    "                final_split_heads.append(next_label)\n",
    "                current_index = next_index\n",
    "                current_label = next_label\n",
    "\n",
    "            else:\n",
    "                heads_to_check = False\n",
    "\n",
    "        return final_split_heads      \n",
    "\n",
    "\n",
    "\n",
    "    def sdf_likely_category(self,current_label,current_index,path,head_flag,sdf_final_dict,connections,mesh_number,absolute_head_threshold\n",
    "                           ):\n",
    "        \"\"\"\n",
    "        Returns the most likely category of a certain label: as neck or head label\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #width thresholding constants\n",
    "        width_thresholds = {\"base\":0.04, \"item_top_threshold\":1.5} \n",
    "        #if size is smaller than the max threshold for a head then return neck\n",
    "        if mesh_number[current_label] < absolute_head_threshold:\n",
    "            return \"neck\"\n",
    "\n",
    "        #get the mean, max, and median\n",
    "        median_width = sdf_final_dict[current_label]\n",
    "\n",
    "\n",
    "\n",
    "        #common characteristics of neck:\n",
    "        #1) median width Less than neck_cuttoff_threshold\n",
    "        #2) if larger item on top and that item is not a head\n",
    "        #3) if larger item on top with more then 50% heads but less width\n",
    "        #4) connected to backbone\n",
    "\n",
    "\n",
    "\n",
    "        #1) median width Less than neck_cuttoff_threshold, return as neck\n",
    "        if median_width < width_thresholds[\"base\"]:\n",
    "            return \"neck\"\n",
    "\n",
    "        #2) if larger item on top and that item is not a head or #3) if larger item on top with more then 50% heads but less width\n",
    "        #width_on_top = []\n",
    "        #face_number_on_top = []\n",
    "\n",
    "        for i in range(0,current_index):\n",
    "            face_number_on_top = mesh_number[path[i]]\n",
    "            width_on_top = sdf_final_dict[path[i]]\n",
    "\n",
    "            if face_number_on_top > mesh_number[current_label]:\n",
    "                if head_flag == False:\n",
    "                    return \"neck\"\n",
    "\n",
    "                if median_width > width_thresholds[\"item_top_threshold\"]*width_on_top:\n",
    "                    return \"neck\"\n",
    "\n",
    "        #4) connected to backbone\n",
    "        if -1 in connections[current_label]:\n",
    "            return \"neck\"\n",
    "\n",
    "\n",
    "        ######check for head based on if there is significantly smaller neck underneath it (because can be very close to 0.04 cuttoff sometimes\n",
    "\n",
    "        #get the mean, median and max\n",
    "\n",
    "        #will return head or neck\n",
    "        return \"head\"      \n",
    "\n",
    "    \n",
    "    def find_endpoints(self,G,mesh_number):\n",
    "        #will first calculate all the shortest paths for each of the nodes\n",
    "        \n",
    "        \n",
    "        #removes the backbone from the node list but not remove it from the nodes\n",
    "        node_list = list(G.nodes)\n",
    "        if(-1 in node_list):\n",
    "            node_list.remove(-1)\n",
    "        else:\n",
    "            return [],[] \n",
    "\n",
    "        #gets the shortest path from every node to the backbone\n",
    "        shortest_paths = {}\n",
    "        for node in node_list:\n",
    "            shortest_paths[node] = [k for k in nx.all_shortest_paths(G,node,-1)]\n",
    "\n",
    "        #identify the nodes that are not a subset of other nodes --> these called endpoints\n",
    "        endpoints = []\n",
    "        \n",
    "        for node in node_list:\n",
    "            other_nodes = [k for k in node_list if k != node ]\n",
    "            not_unique = 0\n",
    "            for path in shortest_paths[node]:\n",
    "                not_unique_Flag = False\n",
    "                for o_node in other_nodes:\n",
    "                    for o_shortest_path in shortest_paths[o_node]:\n",
    "                        if set(path) <= set(o_shortest_path): #only if shortest path is subset of other path\n",
    "                            not_unique_Flag = True\n",
    "\n",
    "                if not_unique_Flag == True: #counts the number of the unique paths that are not unique\n",
    "                    not_unique = not_unique + 1\n",
    "\n",
    "            \"\"\"#decide if unique endpoint, because not_unique measures the number of paths out of all \n",
    "            of the shortest paths that are not unique, so if have one that is still unique then\n",
    "            not_unique will be less than number of total shortest paths, which means it is an endpoint\n",
    "            \"\"\"\n",
    "            if not_unique < len(shortest_paths[node]):   # this means there is a unique path\n",
    "\n",
    "                #if not_unique != 0:\n",
    "                    #print(node + \"-some unique and some non-unique paths for endpoint\")\n",
    "                endpoints.append(node)\n",
    "\n",
    "        #gets the number of most possible faces between the endpoint and the bakcbone\n",
    "        #out of all of the shortest paths for each endpoint\n",
    "        ##Result: have a list that for each endpoint has the shortest path faces length\n",
    "        longest_paths_list = []\n",
    "        for end_node in endpoints:\n",
    "            longest_path = 0\n",
    "            for path in shortest_paths[end_node]:\n",
    "                path_length = 0\n",
    "                for point in path:\n",
    "                    path_length = path_length + mesh_number[point]\n",
    "                if path_length > longest_path:\n",
    "                    longest_path = path_length\n",
    "\n",
    "            longest_paths_list.append((end_node,longest_path))\n",
    "\n",
    "        \n",
    "        \n",
    "        #sorts the list so that the node with the greatest path length is first\n",
    "        longest_paths_list.sort(key=lambda pair: pair[1], reverse=True)\n",
    "        \n",
    "        ranked_endpoints = [x for x,i in longest_paths_list]\n",
    "        endpoint_paths_lengths = [i for x,i in longest_paths_list]\n",
    "\n",
    "        #creates dictionary that maps the endpoints to the shortest path face number\n",
    "        enpoint_path_list = {}\n",
    "        for endpt in ranked_endpoints:\n",
    "            enpoint_path_list[endpt] = shortest_paths[endpt]\n",
    "\n",
    "\n",
    "        #ranked_endpoints, longest_paths_list = (list(t) for t in zip(*sorted(zip(endpoints, longest_paths_list))))\n",
    "\n",
    "        #returns ranked list of endpoints by greatest number of faces along shortest path\n",
    "        #and the dictionary that mapes endpoints to all of the shortest paths\n",
    "        return ranked_endpoints, enpoint_path_list \n",
    "    \n",
    "    def classify_spine_vp2(self,connections,shared_vertices,mesh_number,\n",
    "                           absolute_head_threshold,\n",
    "                            stub_threshold,\n",
    "                            path_threshold,\n",
    "                          split_head_threshold):\n",
    "\n",
    "        \n",
    "\n",
    "        #set this as variable so don't get errors when porting over from blender to trimesh\n",
    "        sdf_final_dict = self.sdf_final_dict\n",
    "        \n",
    "        #make a new dictionary to hold the final labels of the spine for that group\n",
    "        end_labels = {k:\"none\" for k in mesh_number.keys()}\n",
    "\n",
    "\n",
    "        #only one segment so label it as a spine\n",
    "        if len(connections.keys()) <= 1:\n",
    "            end_labels[list(connections.keys())[0]] = \"spine_one_seg\"\n",
    "\n",
    "        #make a new dictionary to hold the final labels  of the spine segmentations\n",
    "        end_labels = {k:\"none\" for k in mesh_number.keys()}\n",
    "        end_labels[-1] = \"backbone\"\n",
    "\n",
    "        total_mesh_faces_outer = sum([k for i,k in mesh_number.items()])\n",
    "        #print(\"total_mesh_faces = \" + str( total_mesh_faces_outer))\n",
    "        \n",
    "        if total_mesh_faces_outer > self.error_threshold:\n",
    "            for k in end_labels.keys():\n",
    "                end_labels[k] = \"error\"\n",
    "                \n",
    "            end_labels[-1] = \"backbone\"\n",
    "            return end_labels\n",
    "\n",
    "        #create the graph from the connections\n",
    "        G=nx.Graph(connections)\n",
    "\n",
    "        #find the endpoints of the graph and all of the corresponding shortest paths\n",
    "        endpoint_labels,shortest_paths = self.find_endpoints(G,mesh_number)\n",
    "\n",
    "        if endpoint_labels == []:\n",
    "            for jk in end_labels.keys():\n",
    "                end_labels[jk] = \"backbone\"\n",
    "                return end_labels\n",
    "\n",
    "\n",
    "        #iterates through all of the endpoints\n",
    "        for endpoint in endpoint_labels:\n",
    "            \n",
    "            #get the shortest path lists\n",
    "            endpoint_short_paths = shortest_paths[endpoint]\n",
    "            #iterates through all of the shortest paths\n",
    "            for path in endpoint_short_paths:\n",
    "                path.remove(-1)\n",
    "                #gets total number of faces along the path\n",
    "                path_total_mesh_faces = sum([k for i,k in mesh_number.items() if i in path])\n",
    "                \n",
    "                travel_index = 0\n",
    "                head_found = False\n",
    "                label_everything_above_as_head = False\n",
    "                #travels up the path until find a head or reached the end of the path\n",
    "                while (head_found == False ) and travel_index < len(path):\n",
    "                    current_face = path[travel_index]\n",
    "                    sdf_guess = self.sdf_likely_category(current_face,travel_index,path,False,self.sdf_final_dict,connections,mesh_number,absolute_head_threshold)\n",
    "                    if  sdf_guess != \"head\" or mesh_number[current_face] < absolute_head_threshold:\n",
    "                        #then not of any significance BUT ONLY REASSIGN IF NOT HAVE ASSIGNMENT***\n",
    "                        if end_labels[current_face] == \"none\":\n",
    "                            end_labels[current_face] = \"no_significance\"\n",
    "                        travel_index = travel_index + 1\n",
    "                    else:\n",
    "                        #end_labels[current_face] = \"head_reg\" WAIT TO ASSIGN TILL LATER\n",
    "                        if \"neck\" != end_labels[current_face][0:4] and \"spine\" !=  end_labels[current_face][0:5] :   #if not already labeled as neck or spine\n",
    "                            head_found = True\n",
    "                            label_everything_above_as_head = True\n",
    "                        else:\n",
    "                            travel_index = travel_index + 1\n",
    "\n",
    "\n",
    "                #print(\"end of first while loop, travel_index = \"+ str(travel_index) + \" head_found = \"+ str(head_found))\n",
    "                ############Added new threshold that makes it so path length can't be really small\n",
    "                if travel_index < len(path):\n",
    "                    travel_face = path[travel_index]\n",
    "                else:\n",
    "                    travel_face = path[travel_index-1]\n",
    "                    travel_index = travel_index-1\n",
    "                \n",
    "                \n",
    "                if (path[travel_index] == -1) or (-1 in connections[path[travel_index]]):\n",
    "                    head_found = False\n",
    "                    label_everything_above_as_head = True\n",
    "\n",
    "                if path_total_mesh_faces<path_threshold:\n",
    "                    head_found = False\n",
    "                    label_everything_above_as_head = True\n",
    "\n",
    "\n",
    "                ####do the head splitting####\n",
    "                #see if there are any labels that border it that also share a high percentage of faces\n",
    "                if head_found == True:\n",
    "                    ##will return the names of the faces that have unusually high verts sharing\n",
    "                    split_head_labels = self.get_split_heads_vp2(path[travel_index],travel_index,path,connections,shared_vertices,mesh_number,sdf_final_dict\n",
    "                                                                 ,absolute_head_threshold,split_head_threshold)\n",
    "                    #print(\"split_head_labels = \" + str(split_head_labels))\n",
    "\n",
    "                    #if two or more split heads\n",
    "                    if len(split_head_labels) >= 2:\n",
    "                        #print(\"adding the split head labels\")\n",
    "                        for split_label in split_head_labels:\n",
    "                            #######may need to add in CHECK FOR ALREADY LABELED\n",
    "                            if (\"head\" == end_labels[split_label][0:4] or end_labels[split_label] == \"none\"):\n",
    "                                end_labels[split_label] = \"head_split\"\n",
    "\n",
    "                        label_everything_above_as_head = True\n",
    "\n",
    "\n",
    "                ###if no head was found\n",
    "                if head_found == False:\n",
    "                    #print(\"no head found so labeling as neck\")\n",
    "                    #######WILL NOT OVERWRITE UNLESS LABELED AS NO SIGNIFICANCE\n",
    "                    for i in path: \n",
    "\n",
    "                        if end_labels[i] == \"no_significance\" or end_labels[i] == \"none\" or end_labels[i][0:4] == \"head\":\n",
    "                            end_labels[i] = \"neck_no_head_on_path_head_false\"\n",
    "\n",
    "                    label_everything_above_as_head = False\n",
    "\n",
    "\n",
    "\n",
    "                #print(\"label_everything_above_as_head = \" + str(label_everything_above_as_head))\n",
    "                #need to label any of those above it in the chain labeled as insignificant to heads\n",
    "                if label_everything_above_as_head == True and head_found == True:\n",
    "                    if end_labels[travel_face] == \"none\":\n",
    "                        #print(\"labeled as head reg\")\n",
    "                        end_labels[travel_face] = \"head_reg\"\n",
    "                    #else:               ########don't need this because don't want to overwrite already written spine neck\n",
    "                        #if \"head\" not in end_labels[travel_index]:\n",
    "                            #end_labels[travel_index] = \"spine_head_disagree\"\n",
    "\n",
    "\n",
    "                    #will label everything above it as a head and then everything below it as neck\n",
    "                    #####need to account for special case where not overwrite the head_split####\n",
    "                    if \"head\" == end_labels[travel_face][0:4]:\n",
    "                        #print('labeling all no_significance above as head hats')\n",
    "                        for i in range(0,travel_index):\n",
    "                            current_label = path[i]\n",
    "                            if end_labels[current_label] == \"no_significance\":\n",
    "                                end_labels[current_label] = \"head_hat\"\n",
    "                            else:\n",
    "                                if \"head\" != end_labels[current_label][0:4]:\n",
    "                                    end_labels[current_label] = \"spine_head_disagree_above_head\"\n",
    "                        #print('labeling all below head as necks')\n",
    "                        for i in range(travel_index+1,len(path)):\n",
    "                            current_label = path[i]\n",
    "                            if current_label not in split_head_labels and end_labels[current_label] != \"head_split\":\n",
    "                                end_labels[current_label] = \"neck_under_head\"\n",
    "                    else: ###not sure when this will be activated but maybe?\n",
    "                        #print(\"head not present so labeling everything above as neck_hat\")\n",
    "                        for i in range(0,travel_index):\n",
    "                            current_label = path[i]\n",
    "                            #####need to account for special case where not overwrite the head_split####\n",
    "                            if end_labels[current_label] == \"no_significance\":\n",
    "                                end_labels[current_label] == \"neck_hats_no_head\"\n",
    "\n",
    "                #print(\"at end of one cycle of big loop\")\n",
    "                #print(\"end_labels = \" + str(end_labels))\n",
    "\n",
    "                #what about a head being accidentally written under another head? \n",
    "                #####you should not write a head to a spine that has already been labeled as under a head\n",
    "                #####you should overwrite all labels under a head as spine_under_head\n",
    "\n",
    "        #print(\"outside of big loop\")\n",
    "        #print(\"end_labels = \" + str(end_labels))\n",
    "\n",
    "        #if no heads present at all label as spines\n",
    "        spine_flag_no_head = False\n",
    "\n",
    "        for face,label in end_labels.items():\n",
    "            if \"head\" == label[0:4]:\n",
    "                spine_flag_no_head = True\n",
    "\n",
    "        if spine_flag_no_head == False:\n",
    "            #print(\"no face detected in all of spine\")\n",
    "            for label_name in end_labels.keys():\n",
    "                end_labels[label_name] = \"spine_no_head_at_all\"\n",
    "\n",
    "\n",
    "        ###### TO DO: can put in a piece of logic that seekss and labels the ones we know are necks for sure based on width\n",
    "\n",
    "\n",
    "        #once done all of the paths go through and label things as stubs\n",
    "        if total_mesh_faces_outer < stub_threshold:\n",
    "            #print(\"stub threshold triggered\")\n",
    "            for label_name in end_labels.keys():\n",
    "                if \"head\" == end_labels[label_name][0:4]:\n",
    "                    end_labels[label_name] = \"stub_head\"\n",
    "\n",
    "                elif \"neck\" == end_labels[label_name][0:4]:\n",
    "                    end_labels[label_name] = \"stub_neck\"\n",
    "                else:\n",
    "                    end_labels[label_name] = \"stub_spine\"\n",
    "\n",
    "\n",
    "\n",
    "        end_labels[-1] = \"backbone\"\n",
    "\n",
    "        ###To Do: replace where look only in 1st four indexes\n",
    "        return end_labels\n",
    "\n",
    "\n",
    "\n",
    "    def export_connection(self,label_name):\n",
    "\n",
    "        #print(\"hello from export_connection with label_name = \" + str(label_name) )\n",
    "        #find all the neighbors of the label\n",
    "\n",
    "        \n",
    "        total_labels_list = []\n",
    "        faces_checked = []\n",
    "        faces_to_check = [label_name]\n",
    "\n",
    "        still_checking_faces = True\n",
    "\n",
    "        connections = {}\n",
    "        shared_vertices = {}\n",
    "        mesh_number = {}\n",
    "\n",
    "        #print(\"about to start checking faces\")\n",
    "\n",
    "        #will iterate through all of the labels with the label name until find all of the neighbors (until hitting the backbone) of the label\n",
    "        while still_checking_faces:\n",
    "            #will exit if no more faces to check\n",
    "            if not faces_to_check:\n",
    "                still_checking_faces = False\n",
    "                break\n",
    "\n",
    "            for facey in faces_to_check:\n",
    "                if facey != -1:\n",
    "                    neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors(facey)\n",
    "\n",
    "\n",
    "\n",
    "                    #reduce the shared vertices with a face and the backbone to 0 so doesn't mess up the shared vertices percentage\n",
    "                    pairs = list(neighbors_shared_vert.items())\n",
    "                    pre_connections = [k for k,i in pairs]\n",
    "                    pre_shared_vertices = [i for k,i in pairs]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    if (-1 in pre_connections):\n",
    "                        back_index = pre_connections.index(-1)\n",
    "                        pre_shared_vertices[back_index] = 0\n",
    "\n",
    "\n",
    "                    connections[facey] = pre_connections\n",
    "                    shared_vertices[facey] = pre_shared_vertices\n",
    "                    mesh_number[facey] = number_of_faces\n",
    "\n",
    "\n",
    "                    for neighbors in neighbors_list:\n",
    "                        if (neighbors != -1) and (neighbors not in faces_to_check) and (neighbors not in faces_checked):\n",
    "                            faces_to_check.append(neighbors)\n",
    "\n",
    "                    faces_to_check.remove(facey)\n",
    "                    faces_checked.append(facey)\n",
    "\n",
    "            #append the backbone to the graph structure\n",
    "            mesh_number[-1] = 0\n",
    "\n",
    "        return connections,shared_vertices,mesh_number\n",
    "\n",
    "    def relabel_segments(self,labels_list,current_label,new_label):\n",
    "\n",
    "        for i,x in enumerate(labels_list):\n",
    "            if x == current_label:\n",
    "                labels_list[i] = new_label\n",
    "\n",
    "        return labels_list\n",
    "\n",
    "    def automatic_spine_classification_vp3(self,\n",
    "                                          absolute_head_threshold = 30,\n",
    "                                           stub_threshold = 40,\n",
    "                                           path_threshold = 40,\n",
    "                                           split_head_threshold = 0.35\n",
    "                                          \n",
    "                                          ):\n",
    "\n",
    "    \n",
    "\n",
    "        #process of labeling\n",
    "        \"\"\"1) Get a list of all of the labels\n",
    "        2) Iterate through the labels and for each:\n",
    "            a. Get the connections, verts_shared and mesh_sizes for all labels connected to said label \n",
    "            b. Run the automatic spine classification to get the categories for each label\n",
    "            c. Create a new list that stores the categories for each label processed\n",
    "            d. repeat until all labels have been processed\n",
    "        3) Delete all the old colors and then setup the global colors with the regular labels\n",
    "        4) Change the material index for all labels based on the categorical classification\"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Pseudo code: \n",
    "        1) Takes in a list of the faces values containing either the CGAL segmentation group or\n",
    "            the word \"backbone\" if it was determined to be that by the smoothing_backbone algorithm\n",
    "        2) Iterates through each of the CGAL segmentation groups left:\n",
    "            a. gets the connections and shared vertices data for that spine cluster\n",
    "            b. Sends the above to a spine classfier to get the spine_head,spine_neck or spine classification\n",
    "            c. rewrites the copy of the labels list for all of those CGAL segmentation groups with head/neck/spine\n",
    "            d. Marks all those CGAL segmentation labels as already being processed\n",
    "            e. increments the spine head/neck counter\n",
    "        3) Creates an entire new copy of the label list and relabels the head/neck/spine or backbone index\n",
    "        4) Translates the face labels to the vertices labels\n",
    "        5) Returns both lists\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"inside auto_spine_classification\")\n",
    "        \n",
    "        #update the list data based on the new backbone labels being placed\n",
    "        self.update_label_list_dependencies()\n",
    "\n",
    "        #but now they have the backbone labels as the label if it was changed in the smoothing backbone\n",
    "        final_spine_labels = self.labels_list.copy()\n",
    "\n",
    "        processed_labels = []\n",
    "\n",
    "        myCounter = Counter(self.labels_list)\n",
    "        complete_labels =  [label for label,times in myCounter.items()] #OPTOMIZE BY USING KEYS\n",
    "\n",
    "        head_counter = 0\n",
    "        spine_counter = 0\n",
    "        neck_counter = 0\n",
    "        stub_counter = 0\n",
    "        error_counter = 0\n",
    "        print(\"About to iterate through labels\")\n",
    "        start_time = time.time()\n",
    "        for i in range(0,len(complete_labels)):\n",
    "            if complete_labels[i] != -1 and complete_labels[i] not in processed_labels:\n",
    "                #print(f\"working on label {complete_labels[i]}\")\n",
    "                #get the conenections, shared vertices and mesh sizes for the whole spine segment in which label is connected to\n",
    "                connections,shared_vertices,mesh_number = self.export_connection(complete_labels[i])\n",
    "                \n",
    "        \n",
    "                #send that graph data to the spine classifier to get labels for that group\n",
    "                #final_labels is dictionary matching the segmentation number to the english label\n",
    "                final_labels = self.classify_spine_vp2(connections,shared_vertices,mesh_number,\n",
    "                           absolute_head_threshold,\n",
    "                            stub_threshold,\n",
    "                            path_threshold,\n",
    "                            split_head_threshold)\n",
    "\n",
    "                #print(f\"final_labels for {complete_labels[i]} =  {final_labels}\")\n",
    "                head_Flag = False\n",
    "                spine_Flag = False\n",
    "                stub_Flag = False\n",
    "                neck_Flag = False\n",
    "                error_Flag = False\n",
    "                #relabel the list accordingly\n",
    "                ############could speed this up where they return the number of types of labels instead of having to search for them############\n",
    "                #print(\"about to find number of heads/spines/stubs/necks PLUS RELABEL AND append them to list\")\n",
    "                for key,value in final_labels.items():\n",
    "                    \n",
    "                    if value[0:4] == \"head\":\n",
    "                        new_label = -2\n",
    "                        head_Flag = True\n",
    "                    elif value[0:4] == \"spin\":\n",
    "                        new_label = -4\n",
    "                        spine_Flag = True\n",
    "                    elif value[0:4] == \"stub\":\n",
    "                        new_label = -5\n",
    "                        stub_Flag = True\n",
    "                    elif value[0:4] == \"neck\":\n",
    "                        new_label = -3\n",
    "                        neck_Flag = True\n",
    "                    elif value[0:4] == \"erro\":\n",
    "                        new_label = -6\n",
    "                        error_Flag = True\n",
    "                    else:\n",
    "                        new_label = -1\n",
    "#                         if value == \"backbone\":\n",
    "#                             new_label = -1\n",
    "\n",
    "                    self.relabel_segments(final_spine_labels,key,new_label)\n",
    "                    #add them to the list of processed labels\n",
    "                    processed_labels.append(key)\n",
    "                    #print(\"str(-1 in final_spine_labels) = \" + str(-1 in final_spine_labels))\n",
    "                #print(\"about to find number of heads/spines/stubs/necks PLUS RELABEL AND append them to list\")\n",
    "\n",
    "                if head_Flag == True:\n",
    "                    head_counter += 1\n",
    "                if spine_Flag == True:\n",
    "                    spine_counter += 1\n",
    "                if stub_Flag == True:\n",
    "                    stub_counter += 1\n",
    "                if neck_Flag == True:\n",
    "                    neck_counter += 1\n",
    "                if error_Flag == True:\n",
    "                    error_counter += 1\n",
    "\n",
    "        print(\"str(-1 in final_spine_labels) = \" + str(-1 in final_spine_labels))\n",
    "        print(f\"done classifying labels: {time.time() - start_time}\")\n",
    "\n",
    "        #current mapping of labels:\n",
    "        \"\"\"\n",
    "        -1 --> backbone\n",
    "        -2 --> head\n",
    "        -3 --> neck\n",
    "        -4 --> spine\n",
    "        -5 --> stub\n",
    "        -6 --> error\n",
    "        \n",
    "        \"\"\"\n",
    "        datajoint_Flag = False\n",
    "        \n",
    "        if datajoint_Flag == True:\n",
    "            #get the indexes for the labeling from the datajoint table\n",
    "            label_data = ta3p100.LabelKey().fetch(\"numeric\",\"description\")\n",
    "            #print(label_data)\n",
    "\n",
    "            label_names = label_data[1].tolist()\n",
    "            label_indexes = label_data[0].tolist()\n",
    "            #print(label_names)\n",
    "\n",
    "            spine_head_index = label_indexes[label_names.index(\"Spine Head\")]\n",
    "            spine_neck_index = label_indexes[label_names.index(\"Spine Neck\")]\n",
    "            spine_reg_index = label_indexes[label_names.index(\"Spine\")]\n",
    "        else:\n",
    "            spine_head_index = 13\n",
    "            spine_neck_index = 15\n",
    "            spine_reg_index = 14\n",
    "            error_index = 10\n",
    "\n",
    "\n",
    "        final_faces_labels_list = np.zeros(len(self.mesh.faces))\n",
    "        final_verts_labels_list = np.zeros(len(self.mesh.vertices))\n",
    "        print(\"Starting Relabeling final faces and vertices\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(\"Counter(final_spine_labels) = \" + str(Counter(final_spine_labels)))\n",
    "        #assign the labels to the correct faces\n",
    "        for i,fi in enumerate(final_spine_labels):\n",
    "            if fi == -2:\n",
    "                #fac.material_index = 2\n",
    "                final_faces_labels_list[i] = spine_head_index\n",
    "            elif fi == -3:\n",
    "                #fac.material_index = 3\n",
    "                final_faces_labels_list[i] = spine_neck_index\n",
    "            elif fi == -4:\n",
    "                #fac.material_index = 4\n",
    "                final_faces_labels_list[i] = spine_reg_index\n",
    "            elif fi == -6:\n",
    "                final_faces_labels_list[i] = error_index\n",
    "            else:\n",
    "                #fac.material_index = 0\n",
    "                final_faces_labels_list[i] = 0\n",
    "\n",
    "            #assign the vertices an index\n",
    "            for vert in self.mesh.faces[i]:\n",
    "                if final_verts_labels_list[vert] == 0:\n",
    "                    final_verts_labels_list[vert] = final_faces_labels_list[i]\n",
    "\n",
    "\n",
    "        print(f\"Done relabeling final faces: {time.time() - start_time}\")\n",
    "        return head_counter,neck_counter, spine_counter, stub_counter,error_counter, final_verts_labels_list, final_faces_labels_list\n",
    "    \n",
    "    def extract_spine_labels(self,labels_file_location,file_name,clusters,smoothness,\n",
    "                                       **kwargs):\n",
    "        \n",
    "        \n",
    "        smooth_backbone_parameters = kwargs.pop('smooth_backbone_parameters', dict())\n",
    "        stub_threshold = kwargs.pop('stub_threshold', 40)\n",
    "        size_multiplier = kwargs.pop('size_multiplier', 1)\n",
    "        \n",
    "        \n",
    "        status = self.get_spine_classification(labels_file_location,file_name,clusters,\n",
    "                                               smoothness,smooth_backbone_parameters,stub_threshold,size_multiplier)\n",
    "        \n",
    "        if status != \"Success\":\n",
    "            print(f\"spine classification did not execute properly with status {status}\")\n",
    "            \n",
    "        \n",
    "        return status\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_spine_extraction(mesh_file_location,\n",
    "                              file_name,\n",
    "                              **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts the spine meshes from a given dendritic mesh and returns either \n",
    "    just the spine meshes or the spine meshes and the dendritic shaft with the spines removed. \n",
    "  \n",
    "\n",
    "    Parameters: \n",
    "    mesh_file_location (str): location of the dendritic mesh on computer\n",
    "    file_name (str): file name of dendritic mesh on computer\n",
    "    \n",
    "    Optional Parameters:\n",
    "    ---configuring cgal segmentation ---\n",
    "    \n",
    "    clusters (int) : number of clusters to use for CGAL surface mesh segmentation (default = 12)\n",
    "    smoothness (int) : smoothness parameter use for CGAL surface mesh segmentation (default = 0.04)\n",
    "    \n",
    "    #paths to already created cgal and sdf files\n",
    "    cgal_segmentation_path (str) : the path to an already generated cgal segmentation file (default = \"\")\n",
    "    cgal_segmentation_sdf_path (str) : the path to an already generated cgal segmentation sdf file (default = \"\")\n",
    "    \n",
    "    ---configuring output---\n",
    "    \n",
    "    split_up_spines (bool): if True will return array of trimesh objects representing each spine\n",
    "                         if False will return all spines as one mesh (default = True)\n",
    "    shaft_mesh (bool) : if True then returns the shaft mesh with the spines stripped out as well (default=False)\n",
    "    \n",
    "    --- configuring spine extraction ---\n",
    "    error_threshold (int): maximum number of faces a spine group can be in order to not be considered an error\n",
    "        size_multiplier (float) : multiplier that will be applied to all size thresholds \n",
    "                                to make scaling to different deimations easy\n",
    "                                \n",
    "    stub_threshold (int) : number of faces (size) that a spine mesh must include in order to be considered spine (default=50)\n",
    "                            \n",
    "    smooth_backbone_parameters (dict) : dict containing parameters for backbone extraction after cgal segmentation\n",
    "        ---- dictionary can contain the following parameters: ---\n",
    "        max_backbone_threshold (int) :the absolute size if it is greater than this then labeled as a possible backbone\n",
    "        (default = 200)\n",
    "        backbone_threshold (int) :if the label meets the width requirements, these are the size requirements as well in order to be considered possible backbone\n",
    "        (default = 40)\n",
    "        shared_vert_threshold (int): raw number of backbone verts that need to be shared in order for label to possibly be a backbone\n",
    "        (default = 20)\n",
    "        shared_vert_threshold_new (int): raw number of backbone verts that need to be shared in order for label to possibly be a backbone in phase 2\n",
    "        (default = 5)\n",
    "        backbone_width_threshold (float) :#the median sdf/width value the segment has to have in order to be considered a possible backbone \n",
    "        (default = 0.1)\n",
    "        backbone_neighbor_min (int): number of backbones in chain in order for label to keep backbone status\n",
    "        (default = 20)\n",
    "    -------------------------------------\n",
    "    \n",
    "    Returns: \n",
    "    1 or 2 trimesh.mesh objects/lists of objects depending on settings\n",
    "    \n",
    "    if split_up_spines == True (default)\n",
    "        list of trimesh.Mesh: each element in list is trimesh.mesh object representing a single spine_extraction_2.off\n",
    "    else:\n",
    "        trimesh.Mesh: trimesh.mesh object representing all spines\n",
    "    \n",
    "    if shaft_mesh == False (default):\n",
    "         No mesh object \n",
    "    else:\n",
    "        Trimesh.mesh object: representing shaft mesh with all of the spines filtered away\n",
    "        \n",
    "    \n",
    "    Examples:\n",
    "    #returns the spine meshes as one entire mesh\n",
    "    \n",
    "    list_of_spine_meshes = complete_spine_extraction(file_location,file_name)\n",
    "    list_of_spine_meshes,shaft_mesh = complete_spine_extraction(file_location,file_name,shaft_mesh=True)\n",
    "    merged_spine_meshes = complete_spine_extraction(file_location,file_name,split_up_spines=False)\n",
    "    merged_spine_meshes,shaft_mesh = complete_spine_extraction(file_location,file_name,split_up_spines=False,shaft_mesh=True)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    clusters = kwargs.pop('clusters', 12)\n",
    "    smoothness = kwargs.pop('smoothness', 0.04)\n",
    "    \n",
    "    cgal_segmentation_path = kwargs.pop(\"cgal_segmentation_path\",\"\")\n",
    "    cgal_segmentation_sdf_path = kwargs.pop(\"cgal_segmentation_sdf_path\",\"\")\n",
    "\n",
    "    \n",
    "    smooth_backbone_parameters = kwargs.pop('smooth_backbone_parameters', dict())\n",
    "    stub_threshold = kwargs.pop('stub_threshold', 50)\n",
    "    split_up_spines = kwargs.pop('split_up_spines', True)\n",
    "    shaft_mesh = kwargs.pop('shaft_mesh', False)\n",
    "    size_multiplier = kwargs.pop('size_multiplier', 1)\n",
    "    error_threshold = kwargs.pop('error_threshold', 700)\n",
    "    \n",
    "    \n",
    "    #making sure there is no more keyword arguments left that you weren't expecting\n",
    "    if kwargs:\n",
    "        raise TypeError('Unexpected **kwargs: %r' % kwargs)\n",
    "    \n",
    "\n",
    "    #check to see if file exists and if it is an off file\n",
    "    if file_name[-3:] != \"off\":\n",
    "        raise TypeError(\"input file must be a .off \")\n",
    "        return None\n",
    "    if not os.path.isfile(str(Path(mesh_file_location) / Path(file_name))):\n",
    "        raise TypeError(str(Path(mesh_file_location) / Path(file_name)) + \" cannot be found\")\n",
    "        return None\n",
    "    \n",
    "    total_time = time.time()\n",
    "    print(f\"Starting spine extraction for {file_name} with clusters={clusters} and smoothness={smoothness}\")\n",
    "    start_time = time.time()\n",
    "    myClassifier = ClassifyMesh(mesh_file_location,file_name,size_multiplier*error_threshold)\n",
    "    print(f\"Step 1: Trimesh mesh build total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "    #make sure a cgal folder is created, and if not make one\n",
    "    \n",
    "    \n",
    "#     if (os.path.isdir(str(Path(os.getcwd()) / Path(\"cgal\")))) == False:\n",
    "#         os.chdir(str(Path(os.getcwd()) / Path(\"cgal\")))\n",
    "#         os.mkdir(\"cgal\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting CGAL segmentation\")\n",
    "    \n",
    "    #check to see if the cgal files already exist, but if not then generate them\n",
    "\n",
    "    if os.path.isfile(cgal_segmentation_path) and os.path.isfile(cgal_segmentation_sdf_path):\n",
    "        print(\"cgal files already exist so skipping generation\")\n",
    "        \n",
    "    else:\n",
    "        print(\"generating cgal locally because couldn't find files\")\n",
    "        full_file_path = str(Path(mesh_file_location) / Path(file_name))[:-4]\n",
    "        csm.cgal_segmentation(full_file_path,clusters,smoothness)\n",
    "    print(f\"Step 2: CGAL segmentation total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "\n",
    "\n",
    "    #do the cgal processing\n",
    "    #labels_file_location = str(Path(os.getcwd()) / Path(\"cgal\"))\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting Spine Extraction\")\n",
    "    individual_spines = myClassifier.extract_spines(mesh_file_location,file_name,\n",
    "                                                    clusters,\n",
    "                                                    smoothness,\n",
    "                                                    split_up_spines,\n",
    "                                                    shaft_mesh,\n",
    "                                                    smooth_backbone_parameters=smooth_backbone_parameters,\n",
    "                                                    stub_threshold=stub_threshold,\n",
    "                                                   )\n",
    "    print(f\"Step 3: Spine extraction total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "\n",
    "    if individual_spines == None or individual_spines == []:\n",
    "        print(\"no spines were extracted so returning\")\n",
    "        return None\n",
    "    return individual_spines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spine_labels(mesh_file_location,\n",
    "                              file_name,\n",
    "                              **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts the spine meshes from a given dendritic mesh and returns either \n",
    "    just the spine meshes or the spine meshes and the dendritic shaft with the spines removed. \n",
    "  \n",
    "\n",
    "    Parameters: \n",
    "    mesh_file_location (str): location of the dendritic mesh on computer\n",
    "    file_name (str): file name of dendritic mesh on computer\n",
    "    \n",
    "    \n",
    "    Optional Parameters:\n",
    "    ---configuring cgal segmentation ---\n",
    "    \n",
    "    clusters (int) : number of clusters to use for CGAL surface mesh segmentation (default = 12)\n",
    "    smoothness (int) : smoothness parameter use for CGAL surface mesh segmentation (default = 0.04)\n",
    "    \n",
    "    ---configuring output---\n",
    "    \n",
    "    split_up_spines (bool): if True will return array of trimesh objects representing each spine\n",
    "                         if False will return all spines as one mesh (default = True)\n",
    "    shaft_mesh (bool) : if True then returns the shaft mesh with the spines stripped out as well (default=False)\n",
    "    \n",
    "    --- configuring spine extraction ---\n",
    "    error_threshold (int): maximum number of faces (multiplied by size_multiplier) a spine group can be in order to not be considered an error\n",
    "    size_multiplier (float) : multiplier that will be applied to all size thresholds \n",
    "                                to make scaling to different deimations easy\n",
    "    \n",
    "    stub_threshold (int) : number of faces (size) that a spine mesh must include in order to be considered spine (default=50)\n",
    "                            \n",
    "    smooth_backbone_parameters (dict) : dict containing parameters for backbone extraction after cgal segmentation\n",
    "        ---- dictionary can contain the following parameters: ---\n",
    "        max_backbone_threshold (int) :the absolute size if it is greater than this then labeled as a possible backbone\n",
    "        (default = 200)\n",
    "        backbone_threshold (int) :if the label meets the width requirements, these are the size requirements as well in order to be considered possible backbone\n",
    "        (default = 40)\n",
    "        shared_vert_threshold (int): raw number of backbone verts that need to be shared in order for label to possibly be a backbone\n",
    "        (default = 20)\n",
    "        shared_vert_threshold_new (int): raw number of backbone verts that need to be shared in order for label to possibly be a backbone in phase 2\n",
    "        (default = 5)\n",
    "        backbone_width_threshold (float) :#the median sdf/width value the segment has to have in order to be considered a possible backbone \n",
    "        (default = 0.1)\n",
    "        backbone_neighbor_min (int): number of backbones in chain in order for label to keep backbone status\n",
    "        (default = 20)\n",
    "        \n",
    "    head_neck_classify_parameters (dict) : dict containing parameters for spine head, neck extraction once backbone is extracted\n",
    "        ---- dictionary can contain the following parameters: ---\n",
    "        absolute_head_threshold (int) :the absolute size if it is greater than this then labeled as a possible head\n",
    "        (default = 30)\n",
    "        stub_threshold (int) :the absolute size if segment group is less than this then labeled as a stub\n",
    "        (default = 40)\n",
    "        path_threshold (int): the minimum number of faces there needs to be between a head and the dendritic shaft\n",
    "        (default = 40)\n",
    "        split_head_threshold (float): percentage of shared verts with other head/total mesh segment to see if a shared head \n",
    "        , if above this number then shared head (default = 0.35)\n",
    "        \n",
    "        \n",
    "    -------------------------------------\n",
    "    \n",
    "    \n",
    "  \n",
    "    Returns: \n",
    "        np.array holding the face spine classification labels according to following index:\n",
    "            spine_head = 13\n",
    "            spine_neck = 15\n",
    "            spine = 14\n",
    "            error = 10\n",
    "            shaft = 0\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    list_of_spine_meshes = complete_spine_extraction(file_location,file_name)\n",
    "    list_of_spine_meshes,shaft_mesh = complete_spine_extraction(file_location,file_name,shaft_mesh=True)\n",
    "    merged_spine_meshes = complete_spine_extraction(file_location,file_name,split_up_spines=False)\n",
    "    merged_spine_meshes,shaft_mesh = complete_spine_extraction(file_location,file_name,split_up_spines=False,shaft_mesh=True)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    clusters = kwargs.pop('clusters', 12)\n",
    "    smoothness = kwargs.pop('smoothness', 0.04)\n",
    "    \n",
    "    cgal_segmentation_path = kwargs.pop(\"cgal_segmentation_path\",\"\")\n",
    "    cgal_segmentation_sdf_path = kwargs.pop(\"cgal_segmentation_sdf_path\",\"\")\n",
    "\n",
    "    \n",
    "    smooth_backbone_parameters = kwargs.pop('smooth_backbone_parameters', dict())\n",
    "    head_neck_classify_parameters = kwargs.pop('head_neck_classify_parameters', dict())\n",
    "    size_multiplier = kwargs.pop('size_multiplier', 1)\n",
    "    error_threshold = kwargs.pop('error_threshold', 700)\n",
    "    \n",
    "    #getting the paraters for the neck spine classification\n",
    "    absolute_head_threshold = head_neck_classify_parameters.pop('absolute_head_threshold', 30)\n",
    "    stub_threshold = head_neck_classify_parameters.pop('stub_threshold', 40)\n",
    "    path_threshold = head_neck_classify_parameters.pop('path_threshold', 40)\n",
    "    split_head_threshold = head_neck_classify_parameters.pop('split_head_threshold', 0.35)\n",
    "    \n",
    "                                           \n",
    "    \n",
    "    \n",
    "    #making sure there is no more keyword arguments left that you weren't expecting\n",
    "    if kwargs:\n",
    "        raise TypeError('Unexpected **kwargs: %r' % kwargs)\n",
    "    \n",
    "\n",
    "    #check to see if file exists and if it is an off file\n",
    "    if file_name[-3:] != \"off\":\n",
    "        raise TypeError(\"input file must be a .off \")\n",
    "        return None\n",
    "    if not os.path.isfile(str(Path(mesh_file_location) / Path(file_name))):\n",
    "        raise TypeError(str(Path(mesh_file_location) / Path(file_name)) + \" cannot be found\")\n",
    "        return None\n",
    "    \n",
    "    total_time = time.time()\n",
    "    print(f\"Starting spine extraction for {file_name} with clusters={clusters} and smoothness={smoothness}\")\n",
    "    start_time = time.time()\n",
    "    myClassifier = ClassifyMesh(mesh_file_location,file_name,size_multiplier*error_threshold)\n",
    "    print(f\"Step 1: Trimesh mesh build total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "    #make sure a cgal folder is created, and if not make one\n",
    "    \n",
    "    \n",
    "#     if (os.path.isdir(str(Path(os.getcwd()) / Path(\"cgal\")))) == False:\n",
    "#         os.chdir(str(Path(os.getcwd()) / Path(\"cgal\")))\n",
    "#         os.mkdir(\"cgal\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting CGAL segmentation\")\n",
    "    \n",
    "    if os.path.isfile(cgal_segmentation_path) and os.path.isfile(cgal_segmentation_sdf_path):\n",
    "        print(\"cgal files already exist so skipping generation\")\n",
    "        \n",
    "    else:\n",
    "        full_file_path = str(Path(mesh_file_location) / Path(file_name))[:-4]\n",
    "        csm.cgal_segmentation(full_file_path,clusters,smoothness)\n",
    "    print(f\"Step 2: CGAL segmentation total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "    \n",
    "    \n",
    "    #do the cgal processing\n",
    "    #labels_file_location = str(Path(os.getcwd()) / Path(\"cgal\"))\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting Spine Extraction\")\n",
    "    status = myClassifier.extract_spine_labels(mesh_file_location,file_name,\n",
    "                                                    clusters,\n",
    "                                                    smoothness,\n",
    "                                                   smooth_backbone_parameters=smooth_backbone_parameters,\n",
    "                                                   stub_threshold=stub_threshold*size_multiplier,\n",
    "                                                   size_multiplier=size_multiplier\n",
    "                                                   )\n",
    "    print(f\"Step 3: Spine extraction total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "    \n",
    "    if status != \"Success\":\n",
    "        print(\"no spines were extracted so returning\")\n",
    "        return None\n",
    "    \n",
    "    #function call that will retrieve the spine head and neck labels\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStep 3: Starting Spine Classification\")\n",
    "    head_counter,neck_counter, spine_counter, stub_counter,error_counter,final_verts_labels_list, final_faces_labels_list = myClassifier.automatic_spine_classification_vp3(\n",
    "                                            absolute_head_threshold = absolute_head_threshold*size_multiplier,\n",
    "                                           stub_threshold = stub_threshold*size_multiplier,\n",
    "                                           path_threshold = path_threshold*size_multiplier,\n",
    "                                            split_head_threshold=split_head_threshold)\n",
    "    print(f\"\\nStep 3: Finshed Spine Classification: {time.time()-start_time}\")\n",
    "    \n",
    "\n",
    "    print(\"head_counter = \" + str(head_counter))\n",
    "    print(\"neck_counter = \" + str(neck_counter))\n",
    "    print(\"spine_counter = \" + str(spine_counter))\n",
    "    print(\"stub_counter = \" + str(stub_counter))\n",
    "    print(\"error_counter = \" + str(error_counter))\n",
    "    \n",
    "    \n",
    "    print(f\"Total time ---- {np.round(time.time() - total_time,5)} seconds\")\n",
    "    return final_faces_labels_list,final_verts_labels_list,head_counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to use for writing off data\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def write_csv_int(output_location,output_file,value_list):\n",
    "    with open(output_location + \"/\" + output_file,\"w\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile,delimiter=\",\")\n",
    "        for i in value_list:\n",
    "            csv_writer.writerow([int(i)])\n",
    "            \n",
    "\n",
    "def write_csv_float(output_location,output_file,value_list):\n",
    "    with open(output_location + \"/\" + output_file,\"w\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile,delimiter=\",\")\n",
    "        for i in value_list:\n",
    "            csv_writer.writerow([float(i)])\n",
    "            \n",
    "def write_Whole_Neuron_Off_file(location,file_name,vertices=[], triangles=[]):\n",
    "    #primary_key = dict(segmentation=1, segment_id=segment_id, decimation_ratio=0.35)\n",
    "    #vertices, triangles = (mesh_Table_35 & primary_key).fetch1('vertices', 'triangles')\n",
    "    \n",
    "    num_vertices = (len(vertices))\n",
    "    num_faces = len(triangles)\n",
    "    \n",
    "    #get the current file location\n",
    "    \n",
    "    file_loc = Path(location)\n",
    "    filename = Path(file_name)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #print(file_loc)\n",
    "    #print(path_and_filename)\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(num_vertices) + \" \" + str(num_faces) + \" 0\\n\" )\n",
    "    \n",
    "    \n",
    "    #iterate through and write all of the vertices in the file\n",
    "    for verts in vertices:\n",
    "        f.write(str(verts[0]) + \" \" + str(verts[1]) + \" \" + str(verts[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing verts\")\n",
    "        \n",
    "    for faces in triangles:\n",
    "        f.write(\"3 \" + str(faces[0]) + \" \" + str(faces[1]) + \" \" + str(faces[2])+\"\\n\")\n",
    "    \n",
    "    print(\"Done writing OFF file\")\n",
    "    #f.write(\"end\")\n",
    "    \n",
    "    return str(path_and_filename),str(filename),str(file_loc)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "#places where to save\n",
    "location = \"dendrite_branches\"\n",
    "import datajoint as dj\n",
    "pinky = dj.create_virtual_module(\"pinky\",\"microns_pinky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #places where to save\n",
    "# location = \"dendrite_branches\"\n",
    "# import datajoint as dj\n",
    "# pinky = dj.create_virtual_module(\"pinky\",\"microns_pinky\")\n",
    "\n",
    "# #download the mesh and the cgal files\n",
    "# segment_type = \"Apical\"\n",
    "# segment_id = 648518346349499701\n",
    "# component_index = 0\n",
    "# clusters=12\n",
    "# smoothness=0.04\n",
    "# decimation_ratio=0.35\n",
    "\n",
    "# output_file_no_ext = str(segment_id) + \"_\" + str(segment_type) + \"_\" + str(component_index)\n",
    "\n",
    "\n",
    "# #get the indices of the component\n",
    "# component_mesh_key = dict(compartment_type=segment_type,segmentation=3,\n",
    "#                        segment_id=segment_id,\n",
    "#                        component_index=component_index,\n",
    "#                         decimation_ratio= decimation_ratio\n",
    "#                        )\n",
    "\n",
    "# labels = (pinky.ComponentLabelFinal & component_mesh_key).fetch1(\"labeled_triangles\")\n",
    "\n",
    "# write_csv_int(location,output_file_no_ext + \"_labels\",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len original component vertices = 201282\n",
      "Len new component vertices = 201282\n",
      "Done writing OFF file\n"
     ]
    }
   ],
   "source": [
    "#places where to save\n",
    "location = \"dendrite_branches\"\n",
    "\n",
    "#download the mesh and the cgal files\n",
    "segment_type = \"Apical\"\n",
    "segment_id = 648518346349499701\n",
    "component_index = 0\n",
    "clusters=12\n",
    "smoothness=0.04\n",
    "decimation_ratio=0.35\n",
    "\n",
    "output_file_no_ext = str(segment_id) + \"_\" + str(segment_type) + \"_\" + str(component_index)\n",
    "\n",
    "#get the original mesh\n",
    "original_mesh_key = dict(segmentation=3,\n",
    "                       segment_id=segment_id,\n",
    "                        decimation_ratio= decimation_ratio\n",
    "                       )\n",
    "\n",
    "verts,faces = (pinky.PymeshfixDecimatedExcitatoryStitchedMesh & original_mesh_key).fetch1(\"vertices\",\n",
    "                                                                                         \"triangles\")\n",
    "\n",
    "\n",
    "#get the indices of the component\n",
    "component_mesh_key = dict(compartment_type=segment_type,segmentation=3,\n",
    "                       segment_id=segment_id,\n",
    "                       component_index=component_index,\n",
    "                        decimation_ratio= decimation_ratio\n",
    "                       )\n",
    "\n",
    "component_vertices, component_faces = (pinky.CompartmentFinal.ComponentFinal & component_mesh_key).fetch1(\"vertex_indices\",\n",
    "                                                                                              \"triangle_indices\")\n",
    "#use the mesh indices to get the submesh\n",
    "new_mesh = trimesh.Trimesh()\n",
    "new_mesh.vertices = verts\n",
    "new_mesh.faces = faces\n",
    "\n",
    "#get the submesh according to the verties\n",
    "component_mesh = new_mesh.submesh([component_faces],append=True)\n",
    "\n",
    "#print the number of vertices to make sure they are the same\n",
    "print(\"Len original component vertices = \" + str(len(component_vertices)))\n",
    "print(\"Len new component vertices = \" + str(len(component_mesh.vertices)))\n",
    "\n",
    "\n",
    "#write the mesh as an off file\n",
    "write_Whole_Neuron_Off_file(location,output_file_no_ext,\n",
    "                            component_mesh.vertices,\n",
    "                            component_mesh.faces)\n",
    "\n",
    "auto_segment_key = dict(compartment_type=segment_type,segmentation=3,\n",
    "                       segment_id=segment_id,\n",
    "                       component_index=component_index,\n",
    "                        decimation_ratio= decimation_ratio,\n",
    "                        clusters = clusters,\n",
    "                        smoothness = smoothness\n",
    "                       )\n",
    "\n",
    "#get the segmentation\n",
    "seg_group,sdf_group = (pinky.ComponentAutoSegmentFinal & auto_segment_key).fetch1(\"seg_group\",\"sdf\")\n",
    "cgal_segmentation_path = location + \"/\" + output_file_no_ext+\"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \".csv\"\n",
    "cgal_segmentation_sdf_path = location + \"/\" + output_file_no_ext+\"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \"_sdf.csv\"\n",
    "\n",
    "write_csv_int(location,output_file_no_ext+\"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \".csv\",seg_group)\n",
    "write_csv_float(location,output_file_no_ext+\"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \"_sdf.csv\",sdf_group)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_file = str(Path(labels_file_location) / Path(file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \".csv\" ))  \n",
    "# sdf_file = str(Path(labels_file_location) / Path(file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \"_sdf.csv\" ))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cgal files already exist so skipping generation\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(cgal_segmentation_path) and os.path.isfile(cgal_segmentation_sdf_path):\n",
    "    print(\"cgal files already exist so skipping generation\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting spine extraction for 648518346349499701_Apical_0.off with clusters=12 and smoothness=0.04\n",
      "Step 1: Trimesh mesh build total time ---- 3.12527 seconds\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Step 2: CGAL segmentation total time ---- 205.42576 seconds\n",
      "\n",
      "Starting Spine Extraction\n",
      "\n",
      "backbone Parameters\n",
      "max_backbone_threshold = 200,                             backbone_threshold = 40,                             shared_vert_threshold = 20,                             shared_vert_threshold_new = 5                              backbone_width_threshold = 0.1,                              backbone_neighbor_min = 7,                             size_multiplier = 1\n",
      "\n",
      "stub_threshold = 40\n",
      "getting cgal data--- 1.53053 seconds ---\n",
      "smoothing round 1\n",
      "smoothing round 2\n",
      "smoothing round 3\n",
      "smoothing backbone--- 75.44619 seconds ---\n",
      "removing 99 labels with stub threshold 40\n",
      "---removing stubs: 1.64104 seconds ---\n",
      "Total spine extraction --- 78.6192 seconds ---\n",
      "Step 3: Spine extraction total time ---- 78.61963 seconds\n",
      "\n",
      "Step 3: Starting Spine Classification\n",
      "inside auto_spine_classification\n",
      "About to iterate through labels\n",
      "str(-1 in final_spine_labels) = True\n",
      "done classifying labels: 509.9477729797363\n",
      "Starting Relabeling final faces and vertices\n",
      "Counter(final_spine_labels) = Counter({-1: 210770, -2: 87427, -3: 52350, -4: 44774, -6: 7488})\n",
      "Done relabeling final faces: 2.3860936164855957\n",
      "\n",
      "Step 3: Finshed Spine Classification: 512.4907295703888\n",
      "head_counter = 725\n",
      "neck_counter = 725\n",
      "spine_counter = 336\n",
      "stub_counter = 0\n",
      "error_counter = 5\n",
      "Total time ---- 799.66277 seconds\n",
      "final_faces_labels_list = [0. 0. 0. ... 0. 0. 0.]\n",
      "final_verts_labels_list = [0. 0. 0. ... 0. 0. 0.]\n",
      "head_counter = 725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mesh_file_location = \"/notebooks/Pass_2_Excitatory_9_Auto_Spine_Classification/dendrite_branches\"\n",
    "\n",
    "clusters = 12\n",
    "smoothness = 0.04\n",
    "\n",
    "file_name = output_file_no_ext + \".off\"\n",
    "# part_2_spines,final_faces_labels_list = complete_spine_extraction(mesh_file_location,file_name,\n",
    "#                                                 clusters=clusters,\n",
    "#                                                 smoothness=smoothness,\n",
    "#                        shaft_mesh = True)\n",
    "\n",
    "# smooth_backbone_parameters (dict) : dict containing parameters for backbone extraction after cgal segmentation\n",
    "#         ---- dictionary can contain the following parameters: ---\n",
    "#         max_backbone_threshold (int) :the absolute size if it is greater than this then labeled as a possible backbone\n",
    "#         (default = 200)\n",
    "#         backbone_threshold (int) :if the label meets the width requirements, these are the size requirements as well in order to be considered possible backbone\n",
    "#         (default = 40)\n",
    "#         shared_vert_threshold (int): raw number of backbone verts that need to be shared in order for label to possibly be a backbone\n",
    "#         (default = 20)\n",
    "#         shared_vert_threshold_new (int): raw number of backbone verts that need to be shared in order for label to possibly be a backbone in phase 2\n",
    "#         (default = 5)\n",
    "#         backbone_width_threshold (float) :#the median sdf/width value the segment has to have in order to be considered a possible backbone \n",
    "#         (default = 0.1)\n",
    "#         backbone_neighbor_min (int): number of backbones in chain in order for label to keep backbone status\n",
    "#         (default = 20)\n",
    "\n",
    "#these parameters are different because the number \n",
    "###### final parameters\n",
    "smooth_backbone_parameters = dict(max_backbone_threshold=200,\n",
    "                                  backbone_threshold=40,\n",
    "                                  shared_vert_threshold=20,\n",
    "                                 shared_vert_threshold_new=5,\n",
    "                                 backbone_width_threshold=0.10,\n",
    "                                 backbone_neighbor_min=7)\n",
    "\n",
    "\n",
    "final_faces_labels_list,final_verts_labels_list,head_counter = generate_spine_labels(mesh_file_location,file_name,\n",
    "                                                clusters=clusters,\n",
    "                                                smoothness=smoothness,\n",
    "                                               smooth_backbone_parameters=smooth_backbone_parameters,\n",
    "                                               error_threshold=700 )\n",
    "\n",
    "print(\"final_faces_labels_list = \" + str(final_faces_labels_list))\n",
    "print(\"final_verts_labels_list = \" + str(final_verts_labels_list))\n",
    "print(\"head_counter = \" + str(head_counter))\n",
    "# myClassifier2 = ClassifyMesh(mesh_file_location,file_name)\n",
    "# myClassifier2.get_spine_classification(labels_file_location,file_name,clusters,smoothness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648518346349499701_Apical_0_final_spines.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "mesh_file_location = \"/notebooks/Pass_2_Excitatory_9_Auto_Spine_Classification/dendrite_branches\"\n",
    "#export the labels to a file and add the label\n",
    "output_file = file_name[:-4] + \"_final_spines.csv\"\n",
    "print(output_file)\n",
    "with open(mesh_file_location + \"/\" + output_file,\"w\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile,delimiter=\",\")\n",
    "    for i in final_faces_labels_list:\n",
    "        csv_writer.writerow([int(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import time\n",
    "import pymeshfix\n",
    "import os\n",
    "import trimesh\n",
    "from pathlib import Path\n",
    "\n",
    "import cgal_Segmentation_Module as csm\n",
    "\n",
    "#for supressing the output\n",
    "import contextlib\n",
    "\n",
    "#for fixing the space issue with the CGAL:readoff\n",
    "import csv\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "ta3p100 = dj.create_virtual_module(\"ta3p100\",\"microns_ta3p100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def write_Whole_Neuron_Off_file(neuron_ID,vertices=[], triangles=[]):\n",
    "    #primary_key = dict(segmentation=1, segment_id=segment_id, decimation_ratio=0.35)\n",
    "    #vertices, triangles = (mesh_Table_35 & primary_key).fetch1('vertices', 'triangles')\n",
    "    \n",
    "    num_vertices = (len(vertices))\n",
    "    num_faces = len(triangles)\n",
    "    \n",
    "    #get the current file location\n",
    "    \n",
    "    file_loc = pathlib.Path.cwd() / \"temp\"\n",
    "    filename = str(neuron_ID)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #print(file_loc)\n",
    "    #print(path_and_filename)\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(num_vertices) + \" \" + str(num_faces) + \" 0\\n\" )\n",
    "    \n",
    "    \n",
    "    #iterate through and write all of the vertices in the file\n",
    "    for verts in vertices:\n",
    "        f.write(str(verts[0]) + \" \" + str(verts[1]) + \" \" + str(verts[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing verts\")\n",
    "        \n",
    "    for faces in triangles:\n",
    "        f.write(\"3 \" + str(faces[0]) + \" \" + str(faces[1]) + \" \" + str(faces[2])+\"\\n\")\n",
    "    \n",
    "    print(\"Done writing OFF file\")\n",
    "    #f.write(\"end\")\n",
    "    \n",
    "    return str(path_and_filename)#,str(filename),str(file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that will handle the whole neuron segmentation:\n",
    "class WholeNeuronClassifier(object):\n",
    "    def __init__(self,mesh_file_location=\"\",file_name=\"\",joincomp=True,remove_smallest_components=False,key=dict()):\n",
    "        \"\"\"\n",
    "        imports mesh from off file and runs the pymeshfix algorithm to get of any unwanted portions of mesh\n",
    "        (particularly used to get rid of basketball like debris that is sometimes inside soma)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        full_path = str(Path(mesh_file_location) / Path(file_name))\n",
    "        self.mesh = trimesh.load_mesh(full_path)\n",
    "        \n",
    "        #get the vertices to faces lookup table\n",
    "        \n",
    "        original_start_time = time.time() \n",
    "        start_time = time.time()\n",
    "        if os.path.isfile(full_path):\n",
    "            print(\"Loading mesh from \" + str(full_path))\n",
    "    \n",
    "            my_mesh = trimesh.load_mesh(full_path)\n",
    "            vertices = my_mesh.vertices\n",
    "            faces = my_mesh.faces\n",
    "\n",
    "        elif \"segment_id\" in key.keys():\n",
    "\n",
    "            print(\"Loading mesh from datajoint- id: \" + str(key[\"segment_id\"]))\n",
    "            segment_id = key[\"segment_id\"]\n",
    "            decimation_ratio = key.pop(\"decimation_ratio\",0.35)\n",
    "            segmentation = key.pop(\"segmentation\",2)\n",
    "\n",
    "            primary_key = dict(segmentation=segmentation,decimation_ratio=decimation_ratio,segment_id=segment_id)\n",
    "            neuron_data = (ta3p100.CleansedMesh & primary_key).fetch1()\n",
    "\n",
    "            print(neuron_data)\n",
    "            vertices = neuron_data['vertices']#.astype(dtype=np.int32)\n",
    "            faces = neuron_data['triangles']#.astype(dtype=np.uint32)\n",
    "            \n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"No valid key or filename given\")\n",
    "            return \"failure\"\n",
    "        \n",
    "        self.vertices = vertices\n",
    "        self.faces = faces\n",
    "        \n",
    "        \"\"\"Ignoring for now so don't have to wait for pymeshfix to run\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"Starting pymeshfix algorithm\")\n",
    "        meshfix = pymeshfix.MeshFix(vertices,faces)\n",
    "        verbose=False\n",
    "        meshfix.repair(verbose,joincomp,remove_smallest_components)\n",
    "        print(f\"Finished pymeshfix algorithm: {time.time() - start_time}\")\n",
    "        \n",
    "        self.verts = meshfix.v\n",
    "        self.faces = meshfix.f\n",
    "\n",
    "        trimesh_object = trimesh.Trimesh()\n",
    "        trimesh_object.faces = self.faces\n",
    "        trimesh_object.vertices = self.vertices\n",
    "        self.mesh = trimesh_object\n",
    "        self.mesh_file_location = mesh_file_location\n",
    "        self.file_name = file_name\n",
    "    \n",
    "    def generate_verts_to_face_dictionary(self,labels_list=[]):\n",
    "        \"\"\"\n",
    "        Generates 2 dictionary mapping for vertices:\n",
    "        1) verts_to_Face: maps each vertex to all the faces it touches\n",
    "        2) verts_to_Label: maps each vertex to all the unique face labels it touches\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if len(labels_list) <= 1:\n",
    "            #print(\"len(labels_list) <= 1\")\n",
    "            labels_list = self.labels_list\n",
    "        \n",
    "        verts_to_Face = {i:[] for i,vertex in enumerate(self.vertices)}\n",
    "        verts_to_Label = {i:[] for i,vertex in enumerate(self.vertices)}\n",
    "\n",
    "\n",
    "        for i,verts in enumerate(self.faces):\n",
    "            \n",
    "            for vertex in verts:\n",
    "                verts_to_Face[vertex].append(i)\n",
    "\n",
    "        #use the verts to face to create the verts to label dictionary\n",
    "        for vert,face_list in verts_to_Face.items():\n",
    "            diff_labels = [labels_list[fc] for fc in face_list]\n",
    "            #print(list(set(diff_labels)))\n",
    "            verts_to_Label[vert] = list(set(diff_labels))\n",
    "            \n",
    "        self.verts_to_Face = verts_to_Face\n",
    "        self.verts_to_Label = verts_to_Label\n",
    "        \n",
    "        #print(\"inside generate verts_to_face\")\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    #write the faces and vertices to an off file\n",
    "    def export_self_mesh(self,file_path_and_name):\n",
    "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "            self.mesh.export(file_path_and_name)\n",
    "    \n",
    "    #Step 2\n",
    "    def load_cgal_segmentation(self,clusters=3,smoothness=0.20):\n",
    "        \"\"\"\n",
    "        Runs the cgal surface mesh segmentation on the mesh object and writes it to a temporary file\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #have to write the new mesh to an off file\n",
    "#         new_mesh_file_path_and_name = str(Path(self.mesh_file_location) /\n",
    "#                                             Path(self.file_name[:-4] + \"_fixed.off\"))\n",
    "        \n",
    "#         self.export_self_mesh(new_mesh_file_path_and_name)\n",
    "#         #add an extra end line to the off file\n",
    "#         with open(new_mesh_file_path_and_name,'a') as fd:\n",
    "#             fd.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "    \n",
    "        path_and_filename = write_Whole_Neuron_Off_file(self.file_name[:-4] + \"_fixed\",self.vertices,self.faces)\n",
    "        \n",
    "    \n",
    "        \"\"\"skip the segmentation for now\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"Starting CGAL segmentation algorithm\")\n",
    "\n",
    "        print(\"\\nStarting CGAL segmentation\")\n",
    "        result = csm.cgal_segmentation(path_and_filename,clusters,smoothness)\n",
    "        print(result)\n",
    "        print(f\"Finished CGAL segmentation algorithm: {time.time() - start_time}\")\n",
    "        \n",
    "        self.clusters = clusters\n",
    "        self.smoothness = smoothness\n",
    "#         self.labels_file = str(Path(self.mesh_file_location) / Path(self.file_name[:-4] + \"_fixed\" + \"-cgal_\" + str(np.round(clusters,2)) + \"_\" + \"{:.2f}\".format(smoothness) + \".csv\" ))  \n",
    "#         self.sdf_file = str(Path(self.mesh_file_location) / Path(self.file_name[:-4] + \"_fixed\" + \"-cgal_\" + str(clusters) + \"_\" + \"{:.2f}\".format(smoothness) + \"_sdf.csv\" ))  \n",
    "#         #print(f\"Step 2: CGAL segmentation total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "        self.labels_file = path_and_filename + \"-cgal_\" + str(np.round(clusters,2)) + \"_\" + \"{:.2f}\".format(smoothness) + \".csv\" \n",
    "        self.sdf_file = path_and_filename + \"-cgal_\" + str(clusters) + \"_\" + \"{:.2f}\".format(smoothness) + \"_sdf.csv\" \n",
    "        return\n",
    "        \n",
    "        \n",
    "    #used for when not pulling from datajoint\n",
    "    def get_cgal_data_and_label_local_optomized(self):\n",
    "        \"\"\"\n",
    "        Loads the cgal segmentation stored in the temporary file into the object\n",
    "        And remaps the labels from CGAL file to numberical \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        labels_file = self.labels_file\n",
    "        sdf_file = self.sdf_file\n",
    "        \n",
    "        #reads int the cgal labels for all of the faces\n",
    "        triangles_labels = np.zeros(len(self.mesh.faces)).astype(\"int64\")\n",
    "        with open(labels_file) as csvfile:\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                triangles_labels[i] = int(row[0])\n",
    "\n",
    "\n",
    "        #converts the cgal labels into a list that\n",
    "        # starts at 0\n",
    "        # progresses in order for all unique labels (so no numbers are skipped and don't have corresponding face)\n",
    "        verts_raw = self.mesh.vertices\n",
    "        faces_raw = self.mesh.faces\n",
    "        #gets a list of the unique labels\n",
    "        unique_segments = list(Counter(triangles_labels).keys())\n",
    "        segmentation_length = len(unique_segments) \n",
    "        unique_index_dict = {unique_segments[x]:x for x in range(0,segmentation_length )}\n",
    "        \n",
    "        labels_list = np.zeros(len(triangles_labels)).astype(\"int64\")\n",
    "        for i,tri in enumerate(triangles_labels):\n",
    "\n",
    "            #assembles the label list that represents all of the faces\n",
    "            labels_list[i] = int(unique_index_dict[tri])\n",
    "\n",
    "        #write thses new labels to a file\n",
    "        with open(labels_file[:-4] + \"_revised.csv\",mode=\"w\") as csvfile:\n",
    "            csv_writer = csv.writer(csvfile,delimiter=',')\n",
    "            for i in labels_list:\n",
    "                csv_writer.writerow([i])\n",
    "\n",
    "        #print(\"done with cgal_segmentation\")\n",
    "\n",
    "        #----------------------now return a dictionary of the sdf values like in the older function get_sdf_dictionary\n",
    "        #get the sdf values and store in sdf_labels\n",
    "        sdf_labels = np.zeros(len(labels_list)).astype(\"float\")\n",
    "        with open(sdf_file) as csvfile:\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                sdf_labels[i] = float(row[0])\n",
    "\n",
    "        \n",
    "        sdf_temp_dict = {}\n",
    "        for i in range(0,segmentation_length):\n",
    "            sdf_temp_dict[i] = []\n",
    "        \n",
    "        \n",
    "        #iterate through the labels_list\n",
    "        for i,label in enumerate(labels_list):\n",
    "            sdf_temp_dict[label].append(sdf_labels[i])\n",
    "        #print(sdf_temp_dict)\n",
    "\n",
    "        #now calculate the stats on the sdf values for each label\n",
    "        sdf_final_dict = {}\n",
    "        \n",
    "        for dict_key,value in sdf_temp_dict.items():\n",
    "\n",
    "            sdf_final_dict[dict_key] = dict(median=np.median(value),mean=np.mean(value),max=np.amax(value))\n",
    "\n",
    "\n",
    "        self.sdf_final_dict = sdf_final_dict\n",
    "        self.labels_list = labels_list\n",
    "        self.labels_list_counter = Counter(labels_list)\n",
    "    \n",
    "#         adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "        \n",
    "#         self.adjacency_labels_col1, self.adjacency_labels_col2 = adjacency_labels.T\n",
    "        #generate the vertices labels\n",
    "        self.generate_verts_to_face_dictionary(labels_list)\n",
    "        \n",
    "        return \n",
    "        \n",
    "    \n",
    "    #Step 3\n",
    "    def get_highest_sdf_part(self,size_threshold=3000,exclude_label=None):\n",
    "        \"\"\"\n",
    "        Based ont the sdf data and the labels data,\n",
    "        Finds the label with the highest median,\n",
    "            label with highest max,\n",
    "            label with highest mean sdf value\n",
    "        \n",
    "        *** but only for those that meet the certain threshold ***\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        high_median_val = 0\n",
    "        high_median = -1\n",
    "        high_mean_val = 0\n",
    "        high_mean = -1\n",
    "        high_max_val = 0\n",
    "        high_max = -1\n",
    "\n",
    "\n",
    "\n",
    "        #gets all of the labels\n",
    "        my_list = Counter(self.labels_list)\n",
    "        my_list_keys = list(my_list.keys())\n",
    "        if exclude_label != None:\n",
    "            my_list_keys.remove(exclude_label)\n",
    "\n",
    "        #OPTOMIZE\n",
    "        \n",
    "        for x in my_list_keys:\n",
    "            \n",
    "            if self.sdf_final_dict[x][\"median\"] > high_median_val and my_list[x] > size_threshold:\n",
    "                high_median = x\n",
    "                high_median_val = self.sdf_final_dict[x][\"median\"]\n",
    "            if self.sdf_final_dict[x][\"mean\"] > high_mean_val  and my_list[x] > size_threshold:\n",
    "                high_mean = x\n",
    "                high_mean_val = self.sdf_final_dict[x][\"mean\"]\n",
    "            if self.sdf_final_dict[x][\"max\"] > high_max_val  and my_list[x] > size_threshold:\n",
    "                high_max = x\n",
    "                high_max_val = self.sdf_final_dict[x][\"max\"]\n",
    "\n",
    "\n",
    "        self.highest_vals= [high_median,high_median_val,high_mean,high_mean_val,high_max,high_max_val]\n",
    "        \n",
    "        self.high_median = self.highest_vals[0]\n",
    "        return self.high_median\n",
    "    \n",
    "\n",
    "    #Step 3\n",
    "    def get_graph_structure(self):\n",
    "        \"\"\"\n",
    "        For each unique label gets:\n",
    "        1) all neighbors\n",
    "        2) number of faces belonging to that label\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        connections = {label_name:[] for label_name in self.labels_list_counter.keys()}\n",
    "        mesh_Number = {label_name:number for label_name,number in self.labels_list_counter.items()}\n",
    "        #label_vert_stats = {label_name:[300000,-300000] for label_name in Counter(labels_list).keys()}\n",
    "\n",
    "        #verts to label curently is the has every vertex and the labels it is toughing in a list\n",
    "        for verts,total_labels in self.verts_to_Label.items():\n",
    "            if len(total_labels) > 1: #if more than one label\n",
    "                for face in total_labels:\n",
    "                    for fc in [v for v in total_labels if v != face]:\n",
    "                        if fc not in connections[face]:\n",
    "                            connections[face].append(fc)\n",
    "\n",
    "        self.connections = connections\n",
    "        self.mesh_Number = mesh_Number\n",
    "        \n",
    "\n",
    "        return \n",
    "    \n",
    "    #Step 4\n",
    "    def find_Soma_Caps(self,soma_index,min_width=0.23,max_faces=6000,max_n_connection=6,large_sized_convex=3):\n",
    "        \"\"\"\n",
    "        Will identify and relabel soma extensions that are created when using clusters of size 4 or higher\n",
    "        \n",
    "        \"\"\"\n",
    "        #get the soma neighbors\n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        \n",
    "        total_soma_caps = []\n",
    "        for i in soma_neighbors:\n",
    "            soma_cap = True\n",
    "            \n",
    "            #collect the mesh of the cap\n",
    "            submesh = self.mesh.submesh(np.where(self.labels_list == i))[0]\n",
    "\n",
    "            mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "            n_faces = len(submesh.faces)\n",
    "            width_data = self.sdf_final_dict[i]\n",
    "            width_data_median = self.sdf_final_dict[i][\"median\"]\n",
    "            n_connections = len(self.connections[i])\n",
    "          \n",
    "            \n",
    "            if width_data[\"median\"] < min_width or n_faces>max_faces or n_connections>max_n_connection: \n",
    "                soma_cap = False\n",
    "            \n",
    "            #use the convex data if size is really big:\n",
    "            if n_faces > 1500:\n",
    "                if mean_convex > 5:\n",
    "                    #print(f\" {i} Doesn't meet second pass\")\n",
    "                    soma_cap = False\n",
    "            \n",
    "            if soma_cap == True:\n",
    "                total_soma_caps.append(i)\n",
    "            \n",
    "\n",
    "        #for all the soma caps replace the labels list with soma_index and recompute neighbors and connections:\n",
    "        if len(total_soma_caps) > 0:\n",
    "            print(f\"Found {len(total_soma_caps)} soma caps and replacing labels: {total_soma_caps}\")\n",
    "            start_time = time.time()\n",
    "            self.labels_list[np.where(np.isin(self.labels_list,total_soma_caps))] = soma_index\n",
    "            \n",
    "            \n",
    "#             #write thses new labels to a file\n",
    "#             with open(self.labels_file[:-4] + \"_revised.csv\",mode=\"w\") as csvfile:\n",
    "#                 csv_writer = csv.writer(csvfile,delimiter=',')\n",
    "#                 for i in self.labels_list:\n",
    "#                     csv_writer.writerow([i])\n",
    "            #call the functions to recompute the connections/neighbors and others (but don't need to generate SDF labels again)\n",
    "            self.labels_list_counter = Counter(self.labels_list)\n",
    "\n",
    "            #generate the vertices labels\n",
    "            self.generate_verts_to_face_dictionary(self.labels_list)\n",
    "\n",
    "            self.get_graph_structure()\n",
    "            print(f\"done replacing soma cap labels : {time.time() - start_time}\")\n",
    "            \n",
    "    #Step 5\n",
    "    def find_Apical(self,soma_index,apical_mesh_threshold=2000,\n",
    "                                        apical_height_threshold=5000,\n",
    "                                           apical_sdf_threshold = 0.09):\n",
    "        \"\"\"Returns the index of the most likely apical \n",
    "        1) calculate the height of 70% up the soma\n",
    "        2) find all the neighbors of the soma using verts_to_Label\n",
    "        3) filter out the neighbors that go below that\n",
    "        4) filter away the neighbors that don't meet minimum number of face, height change and sdf median\n",
    "        5) If multiple, pick the one that has the most number of neighbors\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Soma Index = \" + str(soma_index))\n",
    "        print(\"Soma Connections = \" + str(self.connections[soma_index]))\n",
    "        mesh_Threshold = apical_mesh_threshold\n",
    "        height_Threshold =apical_height_threshold\n",
    "        sdf_Threshold = apical_sdf_threshold\n",
    "        #1) calculate the height of 70% up the soma (but have to adjust because the negative direction of y is \n",
    "        #direction of the apical), this new method gets the height of the first 30% of the somae which is actually\n",
    "        # the top 30% of the soma once it is flipped in the right orientation\n",
    "\n",
    "        soma_verts = self.vertices[self.faces[np.where(self.labels_list == soma_index)].ravel()][:,1]\n",
    "        soma_y_min = np.min(soma_verts)\n",
    "        soma_y_max = np.max(soma_verts)\n",
    "        self.soma_y_min = soma_y_min\n",
    "        self.soma_y_max = soma_y_max\n",
    "#         print(\"soma_y_max =\"  + str(soma_y_max))\n",
    "#         print(\"soma_y_min =\"  + str(soma_y_min))\n",
    "        \n",
    "        \n",
    "        soma_70_percent = (soma_y_max - soma_y_min)*0.3 +  soma_y_min\n",
    "        #print(\"soma_70_percent = \" + str(soma_70_percent))\n",
    "        \n",
    "        #2) find all the neighbors of the soma using verts_to_Label\n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        \n",
    "        #3) filter out the neighbors that go below that\n",
    "\n",
    "        possible_Axons_filter_1 = [label for label in soma_neighbors \n",
    "                            if np.max(self.vertices[self.faces[np.where(self.labels_list == label)].ravel()][:,1]) < soma_70_percent]\n",
    "\n",
    "        #4) filter away the neighbors that don't meet minimum number of face, height change and sdf median\n",
    "        print(\"possible_Axons_filter_1 = \" + str(possible_Axons_filter_1))\n",
    "        possible_Axons_filter_2 = [lab for lab in possible_Axons_filter_1 if \n",
    "                                        self.mesh_Number[lab] > mesh_Threshold and \n",
    "        np.max(self.vertices[self.faces[np.where(self.labels_list == lab)].ravel()][:,1]) - np.min(self.vertices[self.faces[np.where(self.labels_list == lab)].ravel()][:,1]) > height_Threshold and\n",
    "                                        self.sdf_final_dict[lab][\"median\"] > sdf_Threshold]\n",
    "        print(\"possible_Axons_filter_2 = \" + str(possible_Axons_filter_2))\n",
    "        if len(possible_Axons_filter_2) <= 0:\n",
    "            return \"None\"\n",
    "        elif len(possible_Axons_filter_2) == 1:\n",
    "            return possible_Axons_filter_2[0]\n",
    "        else:\n",
    "            #find the one with the most neighbors \n",
    "            ##### MIGHT WANT TO ADD IN WHERE FINDS THE THICKEST WIDTH !\n",
    "            current_apical = possible_Axons_filter_2[0]\n",
    "            current_apical_neighbors = len(self.connections[possible_Axons_filter_2[0]])\n",
    "            for i in range(1,len(possible_Axons_filter_2)):\n",
    "                if len(self.connections[possible_Axons_filter_2[i]]) > current_apical_neighbors:\n",
    "                    current_apical = possible_Axons_filter_2[i]\n",
    "                    current_apical_neighbors = len(self.connections[possible_Axons_filter_2[i]])\n",
    "\n",
    "            return current_apical\n",
    "    \n",
    "    \n",
    "    #step 6\n",
    "    def classify_whole_neuron(self,possible_Apical,soma_index,\n",
    "                             classifier_cilia_threshold=1000,\n",
    "                             classifier_stub_threshold=200,\n",
    "                             classifier_non_dendrite_convex_threshold = 26.5,\n",
    "                             classifier_axon_std_dev_threshold = 69,\n",
    "                             classifier_stub_threshold_apical = 700):\n",
    "        \"\"\"\n",
    "        Will use the soma index and apical index to label the rest of the segmentation portions\n",
    "        with the appropriate category: Apical, Soma stub, cilia, basal, dendrite, axon, etc.\n",
    "        \n",
    "        Parameteres:\n",
    "        classifier_cilia_threshold #maximum size of cilia\n",
    "        classifier_stub_threshold # minimum size of appndage of soma to not be considered stub and merged with the soma\n",
    "        classifier_non_dendrite_convex_threshold #must be above this value to be axon, cilia or error\n",
    "        \n",
    "        classifier_stub_threshold_apical #the minimum size threshold for apical appendage not to be merged with apical\n",
    "        \"\"\"\n",
    "        \n",
    "        #check to see if no soma index\n",
    "        if soma_index < 0:\n",
    "            self.whole_neuron_labels ={lb:\"unsure\" for lb in self.connections.keys()}\n",
    "            return\n",
    "        \n",
    "        #creates dictionary with unique labels whose value will store their final label\n",
    "        whole_neuron_labels ={lb:\"unsure\" for lb in self.connections.keys()}\n",
    "        whole_neuron_labels[soma_index] = \"soma\"\n",
    "\n",
    "        #create a networkx graph based on connections\n",
    "        G=nx.Graph(self.connections)\n",
    "\n",
    "        \n",
    "        #removes the soma from the list of nodes, but not actually remove it from the graph\n",
    "        node_list = list(G.nodes)\n",
    "        if(soma_index in node_list):\n",
    "            node_list.remove(soma_index)\n",
    "        else:\n",
    "            #didn't find soma\n",
    "            return []\n",
    "\n",
    "        \n",
    "        #finds the shortest path from any label to the soma\n",
    "        shortest_paths = {}\n",
    "        for node in node_list:\n",
    "            shortest_paths[node] = [k for k in nx.shortest_path(G,node,soma_index)]\n",
    "\n",
    "        #find the direct neighbors of the soma\n",
    "        soma_branches = dict()\n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        \n",
    "        \n",
    "        #print(\"soma_neighbors = \" + str(soma_neighbors))\n",
    "        \n",
    "        #assemble each of these compartments into groups\n",
    "        for node,path in shortest_paths.items():\n",
    "            if possible_Apical not in path:\n",
    "                specific_soma_neighbor = (set(path).intersection(set(soma_neighbors))).pop()\n",
    "                \n",
    "                if specific_soma_neighbor not in soma_branches.keys():\n",
    "                    soma_branches[specific_soma_neighbor] = []\n",
    "                soma_branches[specific_soma_neighbor].append(node)\n",
    "        \n",
    "\n",
    "        #print(\"soma_branches = \" + str(soma_branches))\n",
    "        #have groups of branches and assmble them into trimesh objects\n",
    "        branches_submeshes = {}\n",
    "        for group,group_list in soma_branches.items():\n",
    "            total_indices = []\n",
    "            for g in group_list:\n",
    "                face_indices = np.where(self.labels_list == g)\n",
    "                total_indices += face_indices\n",
    "            \n",
    "            #create a trimesh submshesh\n",
    "            branches_submeshes[group] = self.mesh.submesh(total_indices,append=True)\n",
    "        \n",
    "        \n",
    "        #iterate through meshes and assign certain labels to these guys\n",
    "        ## define certain thresholds for determining label\n",
    "        cilia_threshold = classifier_cilia_threshold #maximum size of cilia\n",
    "        stub_threshold = classifier_stub_threshold # minimum size of appndage of soma to not be considered stub and merged with the soma\n",
    "        non_dendrite_convex_threshold = classifier_non_dendrite_convex_threshold #must be above this value to be axon, cilia or error\n",
    "\n",
    "        \n",
    "        #Calculate the soma 30% that axon must be lower than\n",
    "\n",
    "        \n",
    "        soma_height = self.soma_y_max - self.soma_y_min\n",
    "        \n",
    "        soma_lower_30 = self.soma_y_max - 0.3*soma_height\n",
    "#         print(\"self.soma_y_max = \" + str(self.soma_y_max))\n",
    "#         print(\"self.soma_y_min = \" + str(self.soma_y_min))\n",
    "#         print(\"soma_lower_30 = \" + str(soma_lower_30))\n",
    "        \n",
    "        \n",
    "        for neighbor,submesh in branches_submeshes.items():\n",
    "            \n",
    "            #get the number of faces\n",
    "            total_faces = len(submesh.faces)\n",
    "            #print(f\"total_faces  = {total_faces}\")\n",
    "            \n",
    "            if total_faces < stub_threshold:\n",
    "                print(f\"{neighbor} = stub soma\")\n",
    "                for x in soma_branches[neighbor]:\n",
    "                    \n",
    "                    whole_neuron_labels[x] = \"soma\"\n",
    "            else:\n",
    "            \n",
    "                mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "                #print(f\"total_faces  = {mean_convex}\")\n",
    "                if mean_convex > non_dendrite_convex_threshold:\n",
    "                    #print(\"neighbor inside cilia check = \" + str(neighbor))\n",
    "                    #classify according to size\n",
    "\n",
    "                    if total_faces < cilia_threshold:\n",
    "                        print(f\"{neighbor} = cilia\")\n",
    "                        for x in soma_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"cilia\"\n",
    "                    else:\n",
    "                        print(f\"{neighbor} = error\")\n",
    "                        for x in soma_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"error\"\n",
    "                else: #try to see if there is any axon\n",
    "                    #calculate the standard deviation\n",
    "                    #print(\"neighbor inside axon check = \" + str(neighbor))\n",
    "                    std_dev_convex = np.std((trimesh.convex.adjacency_projections(submesh)))\n",
    "                    \n",
    "                    if std_dev_convex < classifier_axon_std_dev_threshold:\n",
    "                        #find the minimum y heght of neighbor\n",
    "                        neighbor_y_min = np.min(self.vertices[self.faces[np.where(self.labels_list == neighbor)].ravel()][:,1])\n",
    "                        ###Don't need the maximum anymore\n",
    "                        ##neighbor_y_max = np.max(self.vertices[self.faces[np.where(self.labels_list == neighbor)].ravel()][:,1])\n",
    "                        #print(\"neighbor_y_min = \" + str(neighbor_y_min))\n",
    "                        #print(\"neighbor_y_max = \" + str(neighbor_y_max))\n",
    "                        \n",
    "                        if neighbor_y_min > soma_lower_30:\n",
    "                            #make sure that it doesn't go higher than 40% soma height\n",
    "                            print(f\"{neighbor} = axon\")\n",
    "                            for x in soma_branches[neighbor]:\n",
    "                                whole_neuron_labels[x] = \"axon\"\n",
    "                                \n",
    "                        else:\n",
    "                            print(f\"MET AXON THRESHOLD CRITERIA but not low enough on soma for neighbor = {neighbor}\")\n",
    "                            \n",
    "        \n",
    "        # checks if apical is present or not, and if not then just labels everything else basal\n",
    "        if possible_Apical == \"None\":\n",
    "            #label everything as basal if don't know\n",
    "            for k,vals in whole_neuron_labels.items():\n",
    "                if k != soma_index and vals == \"unsure\":\n",
    "                    #whole_neuron_labels[k] = \"basal\"\n",
    "                    pass\n",
    "            self.whole_neuron_labels = whole_neuron_labels\n",
    "            \n",
    "            return \n",
    "                    \n",
    "            \n",
    "        #return branches_submeshes\n",
    "        \n",
    "        \"\"\" 4-29 added edition that will prevent small spines off of apical \n",
    "        from being considered oblique branches\n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        #find the direct neighbors of the soma\n",
    "        apical_branches = dict()\n",
    "        \n",
    "        apical_neighbors = self.connections[possible_Apical]\n",
    "        apical_neighbors.remove(soma_index)\n",
    "        \n",
    "        \n",
    "        #assemble each of these compartments into groups\n",
    "        for node,path in shortest_paths.items():\n",
    "            if possible_Apical in path and node != possible_Apical: #make sure only those obliques and not actual apical\n",
    "                \n",
    "                specific_apical_neighbor = (set(path).intersection(set(apical_neighbors))).pop()\n",
    "                \n",
    "                if specific_apical_neighbor not in apical_branches.keys():\n",
    "                    apical_branches[specific_apical_neighbor] = []\n",
    "                apical_branches[specific_apical_neighbor].append(node)\n",
    "        \n",
    "\n",
    "        #print(\"apical_branches = \" + str(apical_branches))\n",
    "        #have groups of branches and assmble them into trimesh objects\n",
    "        branches_submeshes_apical = {}\n",
    "        for group,group_list in apical_branches.items():\n",
    "            total_indices = []\n",
    "            for g in group_list:\n",
    "                face_indices = np.where(self.labels_list == g)\n",
    "                total_indices += face_indices\n",
    "            \n",
    "            #create a trimesh submshesh\n",
    "            \n",
    "            branches_submeshes_apical[group] = self.mesh.submesh(total_indices,append=True)\n",
    "        #return branches_submeshes_apical\n",
    "        \n",
    "        #iterate through meshes and assign certain labels to these guys\n",
    "        \n",
    "        \n",
    "        for neighbor,submesh in branches_submeshes_apical.items():\n",
    "            \n",
    "            #get the number of faces\n",
    "            total_faces = len(submesh.faces)\n",
    "            #print(f\"total_faces  = {total_faces}\")\n",
    "            \n",
    "            if total_faces < classifier_stub_threshold_apical:\n",
    "                print(f\"{neighbor} = stub apical\")\n",
    "                for x in apical_branches[neighbor]:\n",
    "                    \n",
    "                    whole_neuron_labels[x] = \"apical\"\n",
    "            else:\n",
    "            \n",
    "                mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "                #print(f\"total_faces  = {mean_convex}\")\n",
    "                if mean_convex > non_dendrite_convex_threshold:\n",
    "                    #classify according to size\n",
    "\n",
    "                    print(f\"{neighbor} = error\")\n",
    "                    for x in apical_branches[neighbor]:\n",
    "                        whole_neuron_labels[x] = \"error\"\n",
    "                else: #try to see if there is any axon like objects off of apical --> if so then error\n",
    "                    #calculate the standard deviation\n",
    "                    std_dev_convex = np.std((trimesh.convex.adjacency_projections(submesh)))\n",
    "                    if std_dev_convex < classifier_axon_std_dev_threshold:\n",
    "                        for x in apical_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"error\"\n",
    "\n",
    "\n",
    "        for label_name, path in shortest_paths.items():\n",
    "            if label_name == possible_Apical: #labels the possible apical as apical\n",
    "                whole_neuron_labels[label_name] = \"apical\"\n",
    "            else:\n",
    "                if possible_Apical in path:\n",
    "                    #if has apical on path and not the apical itself, soma or other label --> label oblique\n",
    "                    for jj in path: \n",
    "                        if jj != possible_Apical and jj != soma_index and whole_neuron_labels[jj] == \"unsure\":\n",
    "                            whole_neuron_labels[jj] = \"oblique\" \n",
    "                else:\n",
    "                    #if NO apical on path and not the apical itself, soma or other label --> label oblique\n",
    "                    for jj in path:\n",
    "                        if jj != possible_Apical and jj != soma_index and whole_neuron_labels[jj] == \"unsure\":\n",
    "                            whole_neuron_labels[jj] = \"basal\" \n",
    "\n",
    "        #return the final list of labels:\n",
    "        self.whole_neuron_labels = whole_neuron_labels\n",
    "        return\n",
    "    \n",
    "    #Step 7\n",
    "    def label_whole_neuron(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        iterates through all of faces and labels them accoring\n",
    "        to the labels assigned to the cgal generic labels\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #instead of going to datajoint for labels\n",
    "        #this just have it locally so don't rely on datajoint\n",
    "\n",
    "        apical_index = 2\n",
    "        basal_index = 3\n",
    "        oblique_index = 4\n",
    "        soma_index = 5\n",
    "        cilia_index = 12\n",
    "        error_index = 10 \n",
    "        axon_index=6\n",
    "\n",
    "\n",
    "        self.final_faces_labels_list = np.zeros(len(self.faces))\n",
    "        \n",
    "\n",
    "        unknown_counter = 0\n",
    "\n",
    "        for i,lab in enumerate(self.labels_list):\n",
    "            #get the category according to the dictionary\n",
    "            cat = self.whole_neuron_labels[lab]\n",
    "            if cat == \"apical\":\n",
    "                self.final_faces_labels_list[i] = apical_index\n",
    "            elif cat == \"basal\":\n",
    "                self.final_faces_labels_list[i] = basal_index\n",
    "            elif cat == \"oblique\":\n",
    "                self.final_faces_labels_list[i] = oblique_index\n",
    "            elif cat == \"soma\":\n",
    "                self.final_faces_labels_list[i] = soma_index\n",
    "            elif cat == \"cilia\":\n",
    "                self.final_faces_labels_list[i] = cilia_index\n",
    "            elif cat == \"axon\":\n",
    "                self.final_faces_labels_list[i] = axon_index\n",
    "            elif cat == \"error\":\n",
    "                self.final_faces_labels_list[i] = error_index\n",
    "            else:\n",
    "                #if wasn't labeled anything just assing it a random color based on cgal assignment\n",
    "                self.output_faces_labels_list[i] = 18 + (int(lab))\n",
    "\n",
    "        \n",
    "    def generate_output_lists(self):\n",
    "        \"\"\"\n",
    "        Will generate the final faces and vertices labels for the classification\n",
    "        \"\"\"\n",
    "\n",
    "        output_faces_list = self.final_faces_labels_list\n",
    "        \n",
    "\n",
    "        #generate the vertices labels\n",
    "        self.generate_verts_to_face_dictionary(output_faces_list)\n",
    "\n",
    "        output_verts_list = [int(self.verts_to_Label[v][0]) for v in self.verts_to_Label]\n",
    "\n",
    "        self.output_verts_labels_list = output_verts_list\n",
    "        return self.final_faces_labels_list, self.output_verts_labels_list \n",
    "    \n",
    "\n",
    "\n",
    "    def return_branches(self,return_cilia=False,\n",
    "                        return_soma=False,\n",
    "                        return_axon=False\n",
    "                        ,return_size_threshold=200):\n",
    "        all_components = dict()\n",
    "        \n",
    "        apical_index = 2\n",
    "        basal_index = 3\n",
    "        oblique_index = 4\n",
    "        soma_index = 5\n",
    "        cilia_index = 12\n",
    "        error_index = 10 \n",
    "        axon_index=6\n",
    "        \n",
    "        basal_indexes = np.where(self.final_faces_labels_list == basal_index)[0]\n",
    "        oblique_indexes = np.where(self.final_faces_labels_list == oblique_index)[0]\n",
    "        apical_indexes = np.where(self.final_faces_labels_list == apical_index)[0]\n",
    "        #axon_indexes = np.where(self.final_faces_labels_list == axon_index)\n",
    "        spine_indexes = [np.concatenate([basal_indexes,oblique_indexes,apical_indexes])]\n",
    "        \n",
    "        if spine_indexes[0].size > 0:\n",
    "            #gets all of the dendritic branches\n",
    "            spine_meshes_whole = self.mesh.submesh(spine_indexes,append=True)\n",
    "\n",
    "            split_up_spines = True\n",
    "            #decides if passing back spines as one whole mesh or seperate meshes\n",
    "            if split_up_spines==True:\n",
    "                individual_spines = []\n",
    "                temp_spines = spine_meshes_whole.split(only_watertight=False)\n",
    "                for spine in temp_spines:\n",
    "                    if len(spine.faces) >= return_size_threshold:\n",
    "                        individual_spines.append(spine)\n",
    "            else:\n",
    "                individual_spines = spine_meshes_whole\n",
    "                    \n",
    "        else:\n",
    "            \n",
    "            individual_spines = None\n",
    "        \n",
    "        \n",
    "        all_components[\"dendrites\"] = individual_spines\n",
    "        \n",
    "        \n",
    "        #will also pass back the cilia,axon or soma based on the parameters of the mesh with the extracted spines\n",
    "        if return_cilia==True:\n",
    "            shaft_indexes = np.where(np.array(self.final_faces_labels_list) == cilia_index) \n",
    "            if shaft_indexes[0].size > 0:\n",
    "                shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "                all_components[\"cilia\"] = shaft_mesh_whole\n",
    "            else:\n",
    "                all_components[\"cilia\"] = None\n",
    "        \n",
    "        #will also pass back the cilia,axon or soma based on the parameters of the mesh with the extracted spines\n",
    "        if return_soma==True:\n",
    "            shaft_indexes = np.where(np.array(self.final_faces_labels_list) == soma_index)\n",
    "            if shaft_indexes[0].size > 0:\n",
    "                shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "                all_components[\"soma\"] = shaft_mesh_whole\n",
    "            else:\n",
    "                all_components[\"soma\"] = None\n",
    "            \n",
    "        #will also pass back the cilia,axon or soma based on the parameters of the mesh with the extracted spines\n",
    "        if return_axon==True:\n",
    "            shaft_indexes = np.where(np.array(self.final_faces_labels_list) == axon_index) \n",
    "            if shaft_indexes[0].size > 0:\n",
    "                shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "                all_components[\"axon\"] = shaft_mesh_whole\n",
    "            else:\n",
    "                all_components[\"axon\"] = None\n",
    "        \n",
    "        \n",
    "        return all_components\n",
    "            \n",
    "\n",
    "    def clean_files(self):\n",
    "        #clean the files \n",
    "        \n",
    "        #1) new mesh file\n",
    "        #2) cgal files (sdf and labels)\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_branches_whole_neuron(mesh_file_location,\n",
    "                              file_name,\n",
    "                              **kwargs):\n",
    "    \"\"\"\n",
    "    Extracts the meshes of all dendritic branches (optionally soma, axon, cilia meshes)\n",
    "    from a full neuron mesh \n",
    "    \n",
    "    Parameters:\n",
    "    mesh_file_location (str): location of the dendritic mesh on computer\n",
    "    file_name (str): file name of dendritic mesh on computer\n",
    "    \n",
    "    ---configuring cgal segmentation ---\n",
    "    \n",
    "    clusters (int) : number of clusters to use for CGAL surface mesh segmentation (default = 12)\n",
    "    smoothness (int) : smoothness parameter use for CGAL surface mesh segmentation (default = 0.04)\n",
    "    \n",
    "    ---configuring pymeshfix options ---\n",
    "\n",
    "    joincomp : bool, optional (default = True)\n",
    "       Attempts to join nearby open components.\n",
    "\n",
    "    remove_smallest_components : bool, optional (default = False)\n",
    "        Remove all but the largest isolated component from the mesh\n",
    "        before beginning the repair process.  Default True\n",
    "\n",
    "    \n",
    "    ---configuring output---\n",
    "    * if any of the below settings are set to true then will return a dictionary storing \n",
    "    the lists for each mesh category (dendrite,cilia,soma,axon) only for those present that flag is set True\n",
    "    The dendritic branches will always be returned\n",
    "    \n",
    "    return_cilia (bool) : if true will return cilia mesh inside returned dictionary (default = False)\n",
    "    return_soma (bool) : if true will return soma mesh inside returned dictionary (default = False)\n",
    "    return_axon (bool) : if true will return axon mesh inside returned dictionary (default = False)\n",
    "    \n",
    "    whole_neuron_labels (bool)  = face labels for entire neuron mesh indicating the classification using following indices (default = False)\n",
    "        apical_index = 2\n",
    "        basal_index = 3\n",
    "        oblique_index = 4\n",
    "        soma_index = 5\n",
    "        cilia_index = 12\n",
    "        error_index = 10 \n",
    "        axon_index=6\n",
    "    \n",
    "    --- configuring branch extraction parameters ---\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    global_start = time.time()\n",
    "    \n",
    "    #Step 1: Mesh importing and Pymeshfix parameters\n",
    "    joincomp = kwargs.pop('joincomp', False)\n",
    "    remove_smallest_components = kwargs.pop('remove_smallest_components', True)\n",
    "    \n",
    "    #Step 2: CGAL segmentation parameters\n",
    "    clusters = kwargs.pop('clusters', 4)\n",
    "    smoothness = kwargs.pop('smoothness', 0.30)\n",
    "    \n",
    "    #step 3: Soma identification parameters\n",
    "    size_multiplier = kwargs.pop('size_multiplier', 1)\n",
    "    soma_size_threshold = kwargs.pop(\"soma_size_threshold\",3000)\n",
    "    \n",
    "    #step 4: finding soma cap parameters\n",
    "    soma_cap_min_width= kwargs.pop('soma_cap_min_width', 0.23) \n",
    "    soma_cap_max_faces= kwargs.pop('soma_cap_max_faces', 6000) \n",
    "    soma_cap_max_n_connections= kwargs.pop('soma_cap_max_n_connections', 6) \n",
    "    soma_cap_conex_threshold= kwargs.pop('soma_cap_conex_threshold', 3) \n",
    "    \n",
    "    #Step 5: Apical Identifying Parameters\n",
    "    apical_mesh_threshold= kwargs.pop('apical_mesh_threshold', 2000)\n",
    "    apical_height_threshold= kwargs.pop('apical_height_threshold', 5000) \n",
    "    apical_sdf_threshold = kwargs.pop('apical_sdf_threshold', 0.09)\n",
    "    \n",
    "    #Step 6: Classifying Entire \n",
    "    classifier_cilia_threshold=kwargs.pop('classifier_cilia_threshold', 1000) #maximum size of cilia\n",
    "    classifier_stub_threshold=kwargs.pop('classifier_stub_threshold', 200) # minimum size of appndage of soma to not be considered stub and merged with the soma\n",
    "    classifier_non_dendrite_convex_threshold = kwargs.pop('classifier_non_dendrite_convex_threshold', 26.5) #must be above this value to be axon, cilia or error\n",
    "    classifier_axon_std_dev_threshold = kwargs.pop('classifier_axon_std_dev_threshold', 69) #standard deviation of convex measurements for which axon branches are under this threshold\n",
    "    classifier_stub_threshold_apical = kwargs.pop('classifier_stub_threshold_apical', 700) #the minimum size threshold for apical appendage not to be merged with apical\n",
    "    \n",
    "    #Step 9: Output Configuration Parameters\n",
    "    return_cilia=kwargs.pop('return_cilia', False)\n",
    "    return_soma=kwargs.pop('return_soma', False)\n",
    "    return_axon=kwargs.pop('return_axon', False)\n",
    "    return_size_threshold=kwargs.pop('return_size_threshold', 200)\n",
    "    \n",
    "    \n",
    "    #making sure there is no more keyword arguments left that you weren't expecting\n",
    "    if kwargs:\n",
    "        raise TypeError('Unexpected **kwargs: %r' % kwargs)\n",
    "    \n",
    "\n",
    "    \n",
    "    #check to see if file exists and if it is an off file\n",
    "    if file_name[-3:] != \"off\":\n",
    "        raise TypeError(\"input file must be a .off \")\n",
    "        return None\n",
    "    if not os.path.isfile(str(Path(mesh_file_location) / Path(file_name))):\n",
    "        raise TypeError(str(Path(mesh_file_location) / Path(file_name)) + \" cannot be found\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"1) Starting: Mesh importing and Pymesh fix\")\n",
    "    classifier = WholeNeuronClassifier(mesh_file_location,file_name,joincomp,remove_smallest_components)\n",
    "    print(f\"1) Finished: Mesh importing and Pymesh fix: {time.time() - start_time}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"2) Staring: Generating CGAL segmentation for neuron\")\n",
    "    classifier.load_cgal_segmentation(clusters,smoothness)\n",
    "    #retrieves the cgal data from the file\n",
    "    \n",
    "\n",
    "    #check to see if files exist\n",
    "    for f in [classifier.labels_file,classifier.sdf_file]:\n",
    "            if not os.path.isfile(f):\n",
    "                print(\"CGAL segmentation files weren't generated\")\n",
    "                raise ValueError(\"CGAL segmentation files weren't generated\")\n",
    "                return \"Failure\"\n",
    "    \n",
    "    classifier.get_cgal_data_and_label_local_optomized()\n",
    "    print(f\"2) Finished: Generating CGAL segmentation for neuron: {time.time() - start_time}\")\n",
    "    \n",
    "\n",
    "    #get the highest values of sdf\n",
    "    start_time = time.time()\n",
    "    print(\"3) Staring: Generating Graph Structure and Identifying Soma\")\n",
    "    soma_index = classifier.get_highest_sdf_part(size_multiplier*soma_size_threshold)\n",
    "    print(\"soma_index = \" + str(soma_index))\n",
    "\n",
    "    #create a graph structure and stats for the whole neuron\n",
    "    classifier.get_graph_structure()\n",
    "    print(f\"3) Finished: Generating Graph Structure and Identifying Soma: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    #gets the caps of the somas created from segmenting into 4 clusters\n",
    "    start_time = time.time()\n",
    "    print(\"4) Staring: Finding Soma Extensions\")\n",
    "    classifier.find_Soma_Caps(soma_index,soma_cap_min_width,\n",
    "                              soma_cap_max_faces*size_multiplier,\n",
    "                              soma_cap_max_n_connections,\n",
    "                              soma_cap_conex_threshold)\n",
    "    \n",
    "    print(f\"4) Finished: Finding Soma Extensions: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"5) Staring: Finding Apical Index\")\n",
    "    #send data to function that will find the Apical\n",
    "    possible_Apical = classifier.find_Apical(soma_index,apical_mesh_threshold*size_multiplier,\n",
    "                                            apical_height_threshold,\n",
    "                                            apical_sdf_threshold)\n",
    "    print(\"possible_Apical = \" + str(possible_Apical))\n",
    "    print(f\"5) Finished: Finding Apical Index: {time.time() - start_time}\")\n",
    "\n",
    "    #use the apical label and the soma label to classify the rest as basal or oblique and return a dictionary that has the mapping of label to compartment type\n",
    "    #but only classifies the cgal labels and not each individual face\n",
    "    start_time = time.time()\n",
    "    print(\"6) Staring: Classifying Entire Neuron\")\n",
    "    classifier.classify_whole_neuron(possible_Apical,soma_index,\n",
    "                                                classifier_cilia_threshold,\n",
    "                                                classifier_stub_threshold,\n",
    "                                                classifier_non_dendrite_convex_threshold,\n",
    "                                                classifier_axon_std_dev_threshold,\n",
    "                                                classifier_stub_threshold_apical\n",
    "                                               )\n",
    "    \n",
    "    #print unique list of labels found\n",
    "    print(\"Total Labels found = \" + str(set(classifier.whole_neuron_labels.values())))\n",
    "    print(f\"6) Finished: Classifying Entire Neuron: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    #label the neurons according to classification\n",
    "    #############NEED TO ADD STEP THAT CALCULATES THE LABELS OF THE VERTICES ##################\n",
    "    start_time = time.time()\n",
    "    print(\"7) Staring: Transfering Segmentation Labels to Face Labels\")\n",
    "    classifier.label_whole_neuron()\n",
    "    print(f\"7) Finished: Transfering Segmentation Labels to Face Labels: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "\n",
    "    #####need to map the final_faces_labels_list to all successive numbers and get vertices\n",
    "    start_time = time.time()\n",
    "    print(\"8) Staring: Generating final Vertex and Face Labels\")\n",
    "    output_faces_list, output_verts_list = classifier.generate_output_lists()\n",
    "    print(f\"8) Finished: Generating final Vertex and Face Labels: {time.time() - start_time}\")\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"9) Staring: Generating Returning Branches\")\n",
    "    dendritic_branches = classifier.return_branches(return_cilia,\n",
    "                                                    return_soma,\n",
    "                                                    return_axon,\n",
    "                                                    return_size_threshold)\n",
    "    print(f\"9) Finished: Generating Returning Branches: {time.time() - start_time}\")\n",
    "    return dendritic_branches\n",
    "    \n",
    "    dendrites_segments = dendritic_branches.pop(\"dendrites\",None)\n",
    "    cilia_segments = dendritic_branches.pop(\"cilia\",None)\n",
    "    soma_segments = dendritic_branches.pop(\"soma\",None)\n",
    "    axon_segments = dendritic_branches.pop(\"axon\",None)\n",
    "                                      \n",
    "    size_one = np.array(5).shape\n",
    "    \n",
    "    if dendrites_segments == None:\n",
    "        dendrites_number = 0\n",
    "    elif np.asarray(dendrites_segments).shape == size_one:\n",
    "        dendrites_number = 1\n",
    "    else:\n",
    "        dendrites_number = len(dendrites_segments)\n",
    "    print(f\"Returning: \\n{dendrites_number} dendritic branches\")\n",
    "    \n",
    "    if return_cilia == True:\n",
    "        if cilia_segments == None:\n",
    "            cilia_number = 0\n",
    "        elif np.asarray(cilia_segments).shape == size_one:\n",
    "            cilia_number = 1\n",
    "        else:\n",
    "            cilia_number = len(cilia_segments)\n",
    "        print(f\" {cilia_number} cilia\")\n",
    "    if return_soma == True:\n",
    "        if soma_segments == None:\n",
    "            soma_number = 0\n",
    "        elif np.asarray(soma_segments).shape == size_one:\n",
    "            soma_number = 1\n",
    "        else:\n",
    "            soma_number = len(soma_segments)\n",
    "        print(f\" {soma_number} soma\")\n",
    "    if return_axon == True:\n",
    "        if axon_segments == None:\n",
    "            axon_number = 0\n",
    "        elif np.asarray(axon_segments).shape == size_one:\n",
    "            axon_number = 1\n",
    "        else:\n",
    "            axon_number = len(axon_segments)\n",
    "        print(f\" {axon_number} axon\")\n",
    "    \n",
    "          \n",
    "    print(f\"Total time: {time.time() - global_start }\")\n",
    "\n",
    "    dendritic_branches[\"dendrites\"] =  dendrites_segments\n",
    "    dendritic_branches[\"cilia\"] =  cilia_segments\n",
    "    dendritic_branches[\"soma\"] =  soma_segments\n",
    "    dendritic_branches[\"axon\"] =  axon_segments\n",
    "\n",
    "    \n",
    "    if return_cilia == False and return_soma == False and return_axon == False:\n",
    "          return dendritic_branches[\"dendrites\"]\n",
    "    else:\n",
    "          return dendritic_branches\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     #passed on the output configurations, return the data\n",
    "#     if \n",
    "\n",
    "\n",
    "\n",
    "#output the faces_labels to a file to test\n",
    "# with open(str(segment_id) + \"_classification.csv\") as file:\n",
    "    \n",
    "#     csv_writer = csv\n",
    "    \n",
    "#             with open(labels_file) as csvfile:\n",
    "#             #print(\"inside labels file\")\n",
    "\n",
    "#             for i,row in enumerate(csv.reader(csvfile)):\n",
    "#                 triangles_labels[i] = int(row[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other neurons tried:\n",
    "#segment_id = 648518346349473045\n",
    "#segment_id = 648518346349386137\n",
    "#segment_id = 648518346349472574\n",
    "#segment_id = 648518346349473044\n",
    "#segment_id = 648518346349386137\n",
    "segment_id = 648518346349473044\n",
    "\n",
    "mesh_file_location = \"./temp\"\n",
    "file_name = \"neuron_\" + str(segment_id) + \".off\"\n",
    "dendritic_branches = extract_branches_whole_neuron(mesh_file_location,file_name,\n",
    "                            return_cilia = True,\n",
    "                            return_soma = True,\n",
    "                            return_axon = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "new_mesh = trimesh.load_mesh(\"./temp/neuron_648518346349473044.off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymeshfix\n",
    "print(\"starting Trimesh\")\n",
    "start_time = time.time()\n",
    "meshfix = pymeshfix.MeshFix(new_mesh.vertices,new_mesh.faces)\n",
    "verbose=False\n",
    "meshfix.repair(verbose,joincomp=False,remove_smallest_components=True)\n",
    "print(f\"Total time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(meshfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_Whole_Neuron_Off_file(\"648518346349473044_pymesh\",meshfix.v,\n",
    "                           meshfix.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendritic_branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendritic_branches[\"dendrites\"][0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendritic_branches[\"cilia\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendritic_branches[\"axon\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = np.array([23,24,25,62,25,26,27,28,27,28,29])\n",
    "\n",
    "apical_index = 2\n",
    "basal_index = 3\n",
    "oblique_index = 4\n",
    "soma_index = 5\n",
    "cilia_index = 12\n",
    "error_index = 10 \n",
    "axon_index=6\n",
    "\n",
    "basal_indexes = np.where(labels_list == basal_index)[0]\n",
    "oblique_indexes = np.where(labels_list == oblique_index)[0]\n",
    "apical_indexes = np.where(labels_list == apical_index)[0]\n",
    "#axon_indexes = np.where(self.final_faces_labels_list == axon_index)\n",
    "spine_indexes = [np.concatenate([basal_indexes,oblique_indexes,apical_indexes])]\n",
    "spine_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spine_indexes[0].size > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import time\n",
    "import pymeshfix\n",
    "import os\n",
    "import trimesh\n",
    "from pathlib import Path\n",
    "\n",
    "import cgal_Segmentation_Module as csm\n",
    "\n",
    "#for supressing the output\n",
    "import contextlib\n",
    "\n",
    "#for fixing the space issue with the CGAL:readoff\n",
    "import csv\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def write_Whole_Neuron_Off_file(neuron_ID,vertices=[], triangles=[]):\n",
    "    #primary_key = dict(segmentation=1, segment_id=segment_id, decimation_ratio=0.35)\n",
    "    #vertices, triangles = (mesh_Table_35 & primary_key).fetch1('vertices', 'triangles')\n",
    "    \n",
    "    num_vertices = (len(vertices))\n",
    "    num_faces = len(triangles)\n",
    "    \n",
    "    #get the current file location\n",
    "    \n",
    "    file_loc = pathlib.Path.cwd() / \"temp\"\n",
    "    filename = str(neuron_ID)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #print(file_loc)\n",
    "    #print(path_and_filename)\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(num_vertices) + \" \" + str(num_faces) + \" 0\\n\" )\n",
    "    \n",
    "    \n",
    "    #iterate through and write all of the vertices in the file\n",
    "    for verts in vertices:\n",
    "        f.write(str(verts[0]) + \" \" + str(verts[1]) + \" \" + str(verts[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing verts\")\n",
    "        \n",
    "    for faces in triangles:\n",
    "        f.write(\"3 \" + str(faces[0]) + \" \" + str(faces[1]) + \" \" + str(faces[2])+\"\\n\")\n",
    "    \n",
    "    print(\"Done writing OFF file\")\n",
    "    #f.write(\"end\")\n",
    "    \n",
    "    return str(path_and_filename)#,str(filename),str(file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that will handle the whole neuron segmentation:\n",
    "class WholeNeuronClassifier(object):\n",
    "    def __init__(self,mesh_file_location=\"\",file_name=\"\",joincomp=True,remove_smallest_components=False,key=dict()):\n",
    "        \"\"\"\n",
    "        imports mesh from off file and runs the pymeshfix algorithm to get of any unwanted portions of mesh\n",
    "        (particularly used to get rid of basketball like debris that is sometimes inside soma)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        full_path = str(Path(mesh_file_location) / Path(file_name))\n",
    "        self.mesh = trimesh.load_mesh(full_path)\n",
    "        \n",
    "        #get the vertices to faces lookup table\n",
    "        \n",
    "        original_start_time = time.time() \n",
    "        start_time = time.time()\n",
    "        if os.path.isfile(full_path):\n",
    "            print(\"Loading mesh from \" + str(full_path))\n",
    "    \n",
    "            my_mesh = trimesh.load_mesh(full_path)\n",
    "            vertices = my_mesh.vertices\n",
    "            faces = my_mesh.faces\n",
    "\n",
    "        elif \"segment_id\" in key.keys():\n",
    "\n",
    "            print(\"Loading mesh from datajoint- id: \" + str(key[\"segment_id\"]))\n",
    "            segment_id = key[\"segment_id\"]\n",
    "            decimation_ratio = key.pop(\"decimation_ratio\",0.35)\n",
    "            segmentation = key.pop(\"segmentation\",2)\n",
    "\n",
    "            primary_key = dict(segmentation=segmentation,decimation_ratio=decimation_ratio,segment_id=segment_id)\n",
    "            neuron_data = (ta3p100.CleansedMesh & primary_key).fetch1()\n",
    "\n",
    "            print(neuron_data)\n",
    "            vertices = neuron_data['vertices']#.astype(dtype=np.int32)\n",
    "            faces = neuron_data['triangles']#.astype(dtype=np.uint32)\n",
    "            \n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"No valid key or filename given\")\n",
    "            return \"failure\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.vertices = vertices\n",
    "        self.faces = faces\n",
    "        \n",
    "        #pymeshfix step\n",
    "        start_time = time.time()\n",
    "        print(\"Starting pymeshfix algorithm\")\n",
    "        meshfix = pymeshfix.MeshFix(vertices,faces)\n",
    "        verbose=False\n",
    "        meshfix.repair(verbose,joincomp,remove_smallest_components)\n",
    "        print(f\"Finished pymeshfix algorithm: {time.time() - start_time}\")\n",
    "\n",
    "      \n",
    "    \n",
    "        self.vertices = meshfix.v\n",
    "        self.faces = meshfix.f\n",
    "        \n",
    "#         #------To load local pymeshfix mesh so don't have to wait for it to do it again----#\n",
    "#         print(\"Loading local pymeshfixed mesh\")\n",
    "#         temp_mesh = trimesh.load_mesh(\"temp/neuron_\" + str(648518346341393609) + \"_fixed.off\")\n",
    "#         self.vertices = temp_mesh.vertices\n",
    "#         self.faces = temp_mesh.faces\n",
    "#         #------End of local pymesh import----#\n",
    "        \n",
    "        \n",
    "\n",
    "        trimesh_object = trimesh.Trimesh()\n",
    "        trimesh_object.faces = self.faces\n",
    "        trimesh_object.vertices = self.vertices\n",
    "        self.mesh = trimesh_object\n",
    "        self.mesh_file_location = mesh_file_location\n",
    "        self.file_name = file_name\n",
    "    \n",
    "    def generate_verts_to_face_dictionary(self,labels_list=[]):\n",
    "        \"\"\"\n",
    "        Generates 2 dictionary mapping for vertices:\n",
    "        1) verts_to_Face: maps each vertex to all the faces it touches\n",
    "        2) verts_to_Label: maps each vertex to all the unique face labels it touches\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if len(labels_list) <= 1:\n",
    "            #print(\"len(labels_list) <= 1\")\n",
    "            labels_list = self.labels_list\n",
    "        \n",
    "        verts_to_Face = {i:[] for i,vertex in enumerate(self.vertices)}\n",
    "        verts_to_Label = {i:[] for i,vertex in enumerate(self.vertices)}\n",
    "\n",
    "\n",
    "        for i,verts in enumerate(self.faces):\n",
    "            \n",
    "            for vertex in verts:\n",
    "                verts_to_Face[vertex].append(i)\n",
    "\n",
    "        #use the verts to face to create the verts to label dictionary\n",
    "        for vert,face_list in verts_to_Face.items():\n",
    "            diff_labels = [labels_list[fc] for fc in face_list]\n",
    "            #print(list(set(diff_labels)))\n",
    "            verts_to_Label[vert] = list(set(diff_labels))\n",
    "            \n",
    "        self.verts_to_Face = verts_to_Face\n",
    "        self.verts_to_Label = verts_to_Label\n",
    "        \n",
    "        #print(\"inside generate verts_to_face\")\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    #write the faces and vertices to an off file\n",
    "    def export_self_mesh(self,file_path_and_name):\n",
    "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "            self.mesh.export(file_path_and_name)\n",
    "    \n",
    "    #Step 2\n",
    "    def load_cgal_segmentation(self,clusters=3,smoothness=0.20):\n",
    "        \"\"\"\n",
    "        Runs the cgal surface mesh segmentation on the mesh object and writes it to a temporary file\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #have to write the new mesh to an off file\n",
    "#         new_mesh_file_path_and_name = str(Path(self.mesh_file_location) /\n",
    "#                                             Path(self.file_name[:-4] + \"_fixed.off\"))\n",
    "        \n",
    "#         self.export_self_mesh(new_mesh_file_path_and_name)\n",
    "#         #add an extra end line to the off file\n",
    "#         with open(new_mesh_file_path_and_name,'a') as fd:\n",
    "#             fd.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "    \n",
    "        path_and_filename = write_Whole_Neuron_Off_file(self.file_name[:-4] + \"_fixed\",self.vertices,self.faces)\n",
    "        \n",
    "    \n",
    "        \"\"\"skip the segmentation for now\"\"\"\n",
    "        start_time = time.time()\n",
    "        cgal_Flag = True\n",
    "        \n",
    "        print(\"\\nStarting CGAL segmentation\")\n",
    "        if cgal_Flag == True:\n",
    "            result = csm.cgal_segmentation(path_and_filename,clusters,smoothness)\n",
    "            print(result)\n",
    "        print(f\"Finished CGAL segmentation algorithm: {time.time() - start_time}\")\n",
    "        \n",
    "        self.clusters = clusters\n",
    "        self.smoothness = smoothness\n",
    "#         self.labels_file = str(Path(self.mesh_file_location) / Path(self.file_name[:-4] + \"_fixed\" + \"-cgal_\" + str(np.round(clusters,2)) + \"_\" + \"{:.2f}\".format(smoothness) + \".csv\" ))  \n",
    "#         self.sdf_file = str(Path(self.mesh_file_location) / Path(self.file_name[:-4] + \"_fixed\" + \"-cgal_\" + str(clusters) + \"_\" + \"{:.2f}\".format(smoothness) + \"_sdf.csv\" ))  \n",
    "#         #print(f\"Step 2: CGAL segmentation total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "        self.labels_file = path_and_filename + \"-cgal_\" + str(np.round(clusters,2)) + \"_\" + \"{:.2f}\".format(smoothness) + \".csv\" \n",
    "        self.sdf_file = path_and_filename + \"-cgal_\" + str(clusters) + \"_\" + \"{:.2f}\".format(smoothness) + \"_sdf.csv\" \n",
    "        return\n",
    "        \n",
    "        \n",
    "    #used for when not pulling from datajoint\n",
    "    def get_cgal_data_and_label_local_optomized(self):\n",
    "        \"\"\"\n",
    "        Loads the cgal segmentation stored in the temporary file into the object\n",
    "        And remaps the labels from CGAL file to numberical \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        labels_file = self.labels_file\n",
    "        sdf_file = self.sdf_file\n",
    "        \n",
    "        #reads int the cgal labels for all of the faces\n",
    "        triangles_labels = np.zeros(len(self.mesh.faces)).astype(\"int64\")\n",
    "        with open(labels_file) as csvfile:\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                triangles_labels[i] = int(row[0])\n",
    "\n",
    "\n",
    "        #converts the cgal labels into a list that\n",
    "        # starts at 0\n",
    "        # progresses in order for all unique labels (so no numbers are skipped and don't have corresponding face)\n",
    "        verts_raw = self.mesh.vertices\n",
    "        faces_raw = self.mesh.faces\n",
    "        #gets a list of the unique labels\n",
    "        unique_segments = list(Counter(triangles_labels).keys())\n",
    "        segmentation_length = len(unique_segments) \n",
    "        unique_index_dict = {unique_segments[x]:x for x in range(0,segmentation_length )}\n",
    "        \n",
    "        labels_list = np.zeros(len(triangles_labels)).astype(\"int64\")\n",
    "        for i,tri in enumerate(triangles_labels):\n",
    "\n",
    "            #assembles the label list that represents all of the faces\n",
    "            labels_list[i] = int(unique_index_dict[tri])\n",
    "\n",
    "        #write thses new labels to a file\n",
    "        with open(labels_file[:-4] + \"_revised.csv\",mode=\"w\") as csvfile:\n",
    "            csv_writer = csv.writer(csvfile,delimiter=',')\n",
    "            for i in labels_list:\n",
    "                csv_writer.writerow([i])\n",
    "\n",
    "        #print(\"done with cgal_segmentation\")\n",
    "\n",
    "        #----------------------now return a dictionary of the sdf values like in the older function get_sdf_dictionary\n",
    "        #get the sdf values and store in sdf_labels\n",
    "        sdf_labels = np.zeros(len(labels_list)).astype(\"float\")\n",
    "        with open(sdf_file) as csvfile:\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                sdf_labels[i] = float(row[0])\n",
    "\n",
    "        \n",
    "        sdf_temp_dict = {}\n",
    "        for i in range(0,segmentation_length):\n",
    "            sdf_temp_dict[i] = []\n",
    "        \n",
    "        \n",
    "        #iterate through the labels_list\n",
    "        for i,label in enumerate(labels_list):\n",
    "            sdf_temp_dict[label].append(sdf_labels[i])\n",
    "        #print(sdf_temp_dict)\n",
    "\n",
    "        #now calculate the stats on the sdf values for each label\n",
    "        sdf_final_dict = {}\n",
    "        \n",
    "        for dict_key,value in sdf_temp_dict.items():\n",
    "\n",
    "            sdf_final_dict[dict_key] = dict(median=np.median(value),mean=np.mean(value),max=np.amax(value))\n",
    "\n",
    "\n",
    "        self.sdf_final_dict = sdf_final_dict\n",
    "        self.labels_list = labels_list\n",
    "        self.labels_list_counter = Counter(labels_list)\n",
    "    \n",
    "#         adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "        \n",
    "#         self.adjacency_labels_col1, self.adjacency_labels_col2 = adjacency_labels.T\n",
    "        #generate the vertices labels\n",
    "        self.generate_verts_to_face_dictionary(labels_list)\n",
    "        \n",
    "        return \n",
    "        \n",
    "    \n",
    "    #Step 3\n",
    "    def get_highest_sdf_part(self,size_threshold=3000,exclude_label=None):\n",
    "        \"\"\"\n",
    "        Based ont the sdf data and the labels data,\n",
    "        Finds the label with the highest median,\n",
    "            label with highest max,\n",
    "            label with highest mean sdf value\n",
    "        \n",
    "        *** but only for those that meet the certain threshold ***\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        high_median_val = 0\n",
    "        high_median = -1\n",
    "        high_mean_val = 0\n",
    "        high_mean = -1\n",
    "        high_max_val = 0\n",
    "        high_max = -1\n",
    "\n",
    "\n",
    "\n",
    "        #gets all of the labels\n",
    "        my_list = Counter(self.labels_list)\n",
    "        my_list_keys = list(my_list.keys())\n",
    "        if exclude_label != None:\n",
    "            my_list_keys.remove(exclude_label)\n",
    "\n",
    "        #OPTOMIZE\n",
    "        \n",
    "        for x in my_list_keys:\n",
    "            \n",
    "            if self.sdf_final_dict[x][\"median\"] > high_median_val and my_list[x] > size_threshold:\n",
    "                high_median = x\n",
    "                high_median_val = self.sdf_final_dict[x][\"median\"]\n",
    "            if self.sdf_final_dict[x][\"mean\"] > high_mean_val  and my_list[x] > size_threshold:\n",
    "                high_mean = x\n",
    "                high_mean_val = self.sdf_final_dict[x][\"mean\"]\n",
    "            if self.sdf_final_dict[x][\"max\"] > high_max_val  and my_list[x] > size_threshold:\n",
    "                high_max = x\n",
    "                high_max_val = self.sdf_final_dict[x][\"max\"]\n",
    "\n",
    "\n",
    "        self.highest_vals= [high_median,high_median_val,high_mean,high_mean_val,high_max,high_max_val]\n",
    "        \n",
    "        self.high_median = self.highest_vals[0]\n",
    "        return self.high_median\n",
    "    \n",
    "\n",
    "    #Step 3\n",
    "    def get_graph_structure(self):\n",
    "        \"\"\"\n",
    "        For each unique label gets:\n",
    "        1) all neighbors\n",
    "        2) number of faces belonging to that label\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        connections = {label_name:[] for label_name in self.labels_list_counter.keys()}\n",
    "        mesh_Number = {label_name:number for label_name,number in self.labels_list_counter.items()}\n",
    "        #label_vert_stats = {label_name:[300000,-300000] for label_name in Counter(labels_list).keys()}\n",
    "\n",
    "        #verts to label curently is the has every vertex and the labels it is toughing in a list\n",
    "        for verts,total_labels in self.verts_to_Label.items():\n",
    "            if len(total_labels) > 1: #if more than one label\n",
    "                for face in total_labels:\n",
    "                    for fc in [v for v in total_labels if v != face]:\n",
    "                        if fc not in connections[face]:\n",
    "                            connections[face].append(fc)\n",
    "\n",
    "        self.connections = connections\n",
    "        self.mesh_Number = mesh_Number\n",
    "        \n",
    "\n",
    "        return \n",
    "    \n",
    "    #Step 4\n",
    "    def find_Soma_Caps(self,soma_index,min_width=0.23,max_faces=6000,max_n_connection=6,large_extension_size=1500,large_extension_convex_max=3):\n",
    "        \"\"\"\n",
    "        Will identify and relabel soma extensions that are created when using clusters of size 4 or higher\n",
    "        \n",
    "        \"\"\"\n",
    "        #get the soma neighbors\n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        \n",
    "        total_soma_caps = []\n",
    "        for i in soma_neighbors:\n",
    "            soma_cap = True\n",
    "            \n",
    "            #collect the mesh of the cap\n",
    "            submesh = self.mesh.submesh(np.where(self.labels_list == i))[0]\n",
    "\n",
    "            mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "            n_faces = len(submesh.faces)\n",
    "            width_data = self.sdf_final_dict[i]\n",
    "            width_data_median = self.sdf_final_dict[i][\"median\"]\n",
    "            n_connections = len(self.connections[i])\n",
    "          \n",
    "            \n",
    "            if width_data[\"median\"] < min_width or n_faces>max_faces or n_connections>max_n_connection: \n",
    "                soma_cap = False\n",
    "            \n",
    "            #use the convex data if size is really big:\n",
    "            if n_faces > large_extension_size:\n",
    "                if mean_convex > 5:\n",
    "                    #print(f\" {i} Doesn't meet second pass\")\n",
    "                    soma_cap = False\n",
    "            \n",
    "            if soma_cap == True:\n",
    "                total_soma_caps.append(i)\n",
    "            \n",
    "\n",
    "        #for all the soma caps replace the labels list with soma_index and recompute neighbors and connections:\n",
    "        if len(total_soma_caps) > 0:\n",
    "            print(f\"Found {len(total_soma_caps)} soma caps and replacing labels: {total_soma_caps}\")\n",
    "            start_time = time.time()\n",
    "            self.labels_list[np.where(np.isin(self.labels_list,total_soma_caps))] = soma_index\n",
    "            \n",
    "            \n",
    "#             #write thses new labels to a file\n",
    "#             with open(self.labels_file[:-4] + \"_revised.csv\",mode=\"w\") as csvfile:\n",
    "#                 csv_writer = csv.writer(csvfile,delimiter=',')\n",
    "#                 for i in self.labels_list:\n",
    "#                     csv_writer.writerow([i])\n",
    "            #call the functions to recompute the connections/neighbors and others (but don't need to generate SDF labels again)\n",
    "            self.labels_list_counter = Counter(self.labels_list)\n",
    "\n",
    "            #generate the vertices labels\n",
    "            self.generate_verts_to_face_dictionary(self.labels_list)\n",
    "\n",
    "            self.get_graph_structure()\n",
    "            print(f\"done replacing soma cap labels : {time.time() - start_time}\")\n",
    "            \n",
    "    #Step 5\n",
    "    def find_Apical(self,soma_index,apical_mesh_threshold=2000,\n",
    "                                        apical_height_threshold=5000,\n",
    "                                           apical_sdf_threshold = 0.09):\n",
    "        \"\"\"Returns the index of the most likely apical \n",
    "        1) calculate the height of 70% up the soma\n",
    "        2) find all the neighbors of the soma using verts_to_Label\n",
    "        3) filter out the neighbors that go below that\n",
    "        4) filter away the neighbors that don't meet minimum number of face, height change and sdf median\n",
    "        5) If multiple, pick the one that has the most number of neighbors\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Soma Index = \" + str(soma_index))\n",
    "        print(\"Soma Connections = \" + str(self.connections[soma_index]))\n",
    "        mesh_Threshold = apical_mesh_threshold\n",
    "        height_Threshold =apical_height_threshold\n",
    "        sdf_Threshold = apical_sdf_threshold\n",
    "        #1) calculate the height of 70% up the soma (but have to adjust because the negative direction of y is \n",
    "        #direction of the apical), this new method gets the height of the first 30% of the somae which is actually\n",
    "        # the top 30% of the soma once it is flipped in the right orientation\n",
    "\n",
    "        soma_verts = self.vertices[self.faces[np.where(self.labels_list == soma_index)].ravel()][:,1]\n",
    "        soma_y_min = np.min(soma_verts)\n",
    "        soma_y_max = np.max(soma_verts)\n",
    "        self.soma_y_min = soma_y_min\n",
    "        self.soma_y_max = soma_y_max\n",
    "#         print(\"soma_y_max =\"  + str(soma_y_max))\n",
    "#         print(\"soma_y_min =\"  + str(soma_y_min))\n",
    "        \n",
    "        \n",
    "        soma_80_percent = (soma_y_max - soma_y_min)*0.2 +  soma_y_min\n",
    "        #print(\"soma_70_percent = \" + str(soma_70_percent))\n",
    "        \n",
    "        #2) find all the neighbors of the soma using verts_to_Label\n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        \n",
    "        #3) filter out the neighbors that go below that\n",
    "\n",
    "        possible_Axons_filter_1 = [label for label in soma_neighbors \n",
    "                            if np.max(self.vertices[self.faces[np.where(self.labels_list == label)].ravel()][:,1]) < soma_80_percent]\n",
    "\n",
    "        #4) filter away the neighbors that don't meet minimum number of face, height change and sdf median\n",
    "        print(\"possible_Axons_filter_1 = \" + str(possible_Axons_filter_1))\n",
    "        possible_Axons_filter_2 = [lab for lab in possible_Axons_filter_1 if \n",
    "                                        self.mesh_Number[lab] > mesh_Threshold and \n",
    "        np.max(self.vertices[self.faces[np.where(self.labels_list == lab)].ravel()][:,1]) - np.min(self.vertices[self.faces[np.where(self.labels_list == lab)].ravel()][:,1]) > height_Threshold and\n",
    "                                        self.sdf_final_dict[lab][\"median\"] > sdf_Threshold]\n",
    "        print(\"possible_Axons_filter_2 = \" + str(possible_Axons_filter_2))\n",
    "        if len(possible_Axons_filter_2) <= 0:\n",
    "            return \"None\"\n",
    "        elif len(possible_Axons_filter_2) == 1:\n",
    "            return possible_Axons_filter_2[0]\n",
    "        else:\n",
    "#             #find the one with the most neighbors\n",
    "#             current_apical = possible_Axons_filter_2[0]\n",
    "#             current_apical_neighbors = len(self.connections[possible_Axons_filter_2[0]])\n",
    "#             for i in range(1,len(possible_Axons_filter_2)):\n",
    "#                 if len(self.connections[possible_Axons_filter_2[i]]) > current_apical_neighbors:\n",
    "#                     current_apical = possible_Axons_filter_2[i]\n",
    "#                     current_apical_neighbors = len(self.connections[possible_Axons_filter_2[i]])\n",
    "                    \n",
    "            #--> revised now does the one with the highest thickness\n",
    "            ##### MIGHT WANT TO ADD IN WHERE FINDS THE THICKEST WIDTH !\n",
    "            \n",
    "            current_apical = possible_Axons_filter_2[0]\n",
    "            current_apical_width = self.sdf_final_dict[current_apical][\"median\"]\n",
    "            \n",
    "            \n",
    "            for i in range(1,len(possible_Axons_filter_2)):\n",
    "                if self.sdf_final_dict[possible_Axons_filter_2[i]][\"median\"] > current_apical_width:\n",
    "                    current_apical = possible_Axons_filter_2[i]\n",
    "                    current_apical_width = self.sdf_final_dict[possible_Axons_filter_2[i]][\"median\"]\n",
    "\n",
    "            return current_apical\n",
    "    \n",
    "    \n",
    "    #step 6\n",
    "    def classify_whole_neuron(self,possible_Apical,soma_index,\n",
    "                             classifier_cilia_threshold=1000,\n",
    "                             classifier_stub_threshold=200,\n",
    "                             classifier_non_dendrite_convex_threshold = 27.5,\n",
    "                             classifier_axon_std_dev_threshold = 69,\n",
    "                             classifier_stub_threshold_apical = 700):\n",
    "        \"\"\"\n",
    "        Will use the soma index and apical index to label the rest of the segmentation portions\n",
    "        with the appropriate category: Apical, Soma stub, cilia, basal, dendrite, axon, etc.\n",
    "        \n",
    "        Parameteres:\n",
    "        classifier_cilia_threshold #maximum size of cilia\n",
    "        classifier_stub_threshold # minimum size of appndage of soma to not be considered stub and merged with the soma\n",
    "        classifier_non_dendrite_convex_threshold #must be above this value to be axon, cilia or error\n",
    "        \n",
    "        classifier_stub_threshold_apical #the minimum size threshold for apical appendage not to be merged with apical\n",
    "        \"\"\"\n",
    "        \n",
    "        #check to see if no soma index\n",
    "        if soma_index < 0:\n",
    "            self.whole_neuron_labels ={lb:\"unsure\" for lb in self.connections.keys()}\n",
    "            return\n",
    "        \n",
    "        #creates dictionary with unique labels whose value will store their final label\n",
    "        whole_neuron_labels ={lb:\"unsure\" for lb in self.connections.keys()}\n",
    "        whole_neuron_labels[soma_index] = \"soma\"\n",
    "\n",
    "        #create a networkx graph based on connections\n",
    "        G=nx.Graph(self.connections)\n",
    "\n",
    "        \n",
    "        #removes the soma from the list of nodes, but not actually remove it from the graph\n",
    "        node_list = list(G.nodes)\n",
    "        if(soma_index in node_list):\n",
    "            node_list.remove(soma_index)\n",
    "        else:\n",
    "            #didn't find soma\n",
    "            return []\n",
    "\n",
    "        \n",
    "        #finds the shortest path from any label to the soma\n",
    "        shortest_paths = {}\n",
    "        for node in node_list:\n",
    "            shortest_paths[node] = [k for k in nx.shortest_path(G,node,soma_index)]\n",
    "\n",
    "        #find the direct neighbors of the soma\n",
    "        soma_branches = dict()\n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        \n",
    "        \n",
    "        #print(\"soma_neighbors = \" + str(soma_neighbors))\n",
    "        \n",
    "        #assemble each of these compartments into groups\n",
    "        for node,path in shortest_paths.items():\n",
    "            if possible_Apical not in path:\n",
    "                specific_soma_neighbor = (set(path).intersection(set(soma_neighbors))).pop()\n",
    "                \n",
    "                if specific_soma_neighbor not in soma_branches.keys():\n",
    "                    soma_branches[specific_soma_neighbor] = []\n",
    "                soma_branches[specific_soma_neighbor].append(node)\n",
    "        \n",
    "\n",
    "        #print(\"soma_branches = \" + str(soma_branches))\n",
    "        #have groups of branches and assmble them into trimesh objects\n",
    "        branches_submeshes = {}\n",
    "        for group,group_list in soma_branches.items():\n",
    "            total_indices = []\n",
    "            for g in group_list:\n",
    "                face_indices = np.where(self.labels_list == g)\n",
    "                total_indices += face_indices\n",
    "            \n",
    "            #create a trimesh submshesh\n",
    "            branches_submeshes[group] = self.mesh.submesh(total_indices,append=True)\n",
    "        \n",
    "        \n",
    "        #iterate through meshes and assign certain labels to these guys\n",
    "        ## define certain thresholds for determining label\n",
    "        cilia_threshold = classifier_cilia_threshold #maximum size of cilia\n",
    "        stub_threshold = classifier_stub_threshold # minimum size of appndage of soma to not be considered stub and merged with the soma\n",
    "        non_dendrite_convex_threshold = classifier_non_dendrite_convex_threshold #must be above this value to be axon, cilia or error\n",
    "\n",
    "        \n",
    "        #Calculate the soma 30% that axon must be lower than\n",
    "\n",
    "        \n",
    "        soma_height = self.soma_y_max - self.soma_y_min\n",
    "        \n",
    "        soma_lower_30 = self.soma_y_max - 0.3*soma_height\n",
    "#         print(\"self.soma_y_max = \" + str(self.soma_y_max))\n",
    "#         print(\"self.soma_y_min = \" + str(self.soma_y_min))\n",
    "#         print(\"soma_lower_30 = \" + str(soma_lower_30))\n",
    "        \n",
    "        \n",
    "        for neighbor,submesh in branches_submeshes.items():\n",
    "            \n",
    "            #get the number of faces\n",
    "            total_faces = len(submesh.faces)\n",
    "            #print(f\"total_faces  = {total_faces}\")\n",
    "            \n",
    "            if total_faces < stub_threshold:\n",
    "                print(f\"{neighbor} = stub soma\")\n",
    "                for x in soma_branches[neighbor]:\n",
    "                    \n",
    "                    whole_neuron_labels[x] = \"soma\"\n",
    "            else:\n",
    "            \n",
    "                mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "                #print(f\"total_faces  = {mean_convex}\")\n",
    "                if mean_convex > non_dendrite_convex_threshold:\n",
    "                    #print(\"neighbor inside cilia check = \" + str(neighbor))\n",
    "                    #classify according to size\n",
    "\n",
    "                    if total_faces < cilia_threshold:\n",
    "                        print(f\"{neighbor} = cilia\")\n",
    "                        for x in soma_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"cilia\"\n",
    "                    else:\n",
    "                        print(f\"{neighbor} = error\")\n",
    "                        for x in soma_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"error\"\n",
    "                else: #try to see if there is any axon\n",
    "                    #calculate the standard deviation\n",
    "                    #print(\"neighbor inside axon check = \" + str(neighbor))\n",
    "                    std_dev_convex = np.std((trimesh.convex.adjacency_projections(submesh)))\n",
    "                    \n",
    "                    if std_dev_convex < classifier_axon_std_dev_threshold:\n",
    "                        #find the minimum y heght of neighbor\n",
    "                        neighbor_y_min = np.min(self.vertices[self.faces[np.where(self.labels_list == neighbor)].ravel()][:,1])\n",
    "                        ###Don't need the maximum anymore\n",
    "                        ##neighbor_y_max = np.max(self.vertices[self.faces[np.where(self.labels_list == neighbor)].ravel()][:,1])\n",
    "                        #print(\"neighbor_y_min = \" + str(neighbor_y_min))\n",
    "                        #print(\"neighbor_y_max = \" + str(neighbor_y_max))\n",
    "                        \n",
    "                        if neighbor_y_min > soma_lower_30:\n",
    "                            #make sure that it doesn't go higher than 40% soma height\n",
    "                            print(f\"{neighbor} = axon\")\n",
    "                            for x in soma_branches[neighbor]:\n",
    "                                whole_neuron_labels[x] = \"axon\"\n",
    "                                \n",
    "                        else:\n",
    "                            print(f\"MET AXON THRESHOLD CRITERIA but not low enough on soma for neighbor = {neighbor}\")\n",
    "                            \n",
    "        \n",
    "        # checks if apical is present or not, and if not then just labels everything else basal\n",
    "        if possible_Apical == \"None\":\n",
    "            #label everything as basal if don't know\n",
    "            for k,vals in whole_neuron_labels.items():\n",
    "                if k != soma_index and vals == \"unsure\":\n",
    "                    #whole_neuron_labels[k] = \"basal\"\n",
    "                    pass\n",
    "            self.whole_neuron_labels = whole_neuron_labels\n",
    "            \n",
    "            return \n",
    "                    \n",
    "            \n",
    "        #return branches_submeshes\n",
    "        \n",
    "        \"\"\" 4-29 added edition that will prevent small spines off of apical \n",
    "        from being considered oblique branches\n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        #find the direct neighbors of the soma\n",
    "        apical_branches = dict()\n",
    "        \n",
    "        apical_neighbors = self.connections[possible_Apical]\n",
    "        apical_neighbors.remove(soma_index)\n",
    "        \n",
    "        \n",
    "        #assemble each of these compartments into groups\n",
    "        for node,path in shortest_paths.items():\n",
    "            if possible_Apical in path and node != possible_Apical: #make sure only those obliques and not actual apical\n",
    "                \n",
    "                specific_apical_neighbor = (set(path).intersection(set(apical_neighbors))).pop()\n",
    "                \n",
    "                if specific_apical_neighbor not in apical_branches.keys():\n",
    "                    apical_branches[specific_apical_neighbor] = []\n",
    "                apical_branches[specific_apical_neighbor].append(node)\n",
    "        \n",
    "\n",
    "        #print(\"apical_branches = \" + str(apical_branches))\n",
    "        #have groups of branches and assmble them into trimesh objects\n",
    "        branches_submeshes_apical = {}\n",
    "        for group,group_list in apical_branches.items():\n",
    "            total_indices = []\n",
    "            for g in group_list:\n",
    "                face_indices = np.where(self.labels_list == g)\n",
    "                total_indices += face_indices\n",
    "            \n",
    "            #create a trimesh submshesh\n",
    "            \n",
    "            branches_submeshes_apical[group] = self.mesh.submesh(total_indices,append=True)\n",
    "        #return branches_submeshes_apical\n",
    "        \n",
    "        #iterate through meshes and assign certain labels to these guys\n",
    "        \n",
    "        \n",
    "        for neighbor,submesh in branches_submeshes_apical.items():\n",
    "            \n",
    "            #get the number of faces\n",
    "            total_faces = len(submesh.faces)\n",
    "            #print(f\"total_faces  = {total_faces}\")\n",
    "            \n",
    "            if total_faces < classifier_stub_threshold_apical:\n",
    "                print(f\"{neighbor} = stub apical\")\n",
    "                for x in apical_branches[neighbor]:\n",
    "                    \n",
    "                    whole_neuron_labels[x] = \"apical\"\n",
    "            else:\n",
    "            \n",
    "                mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "                #print(f\"total_faces  = {mean_convex}\")\n",
    "                if mean_convex > non_dendrite_convex_threshold:\n",
    "                    #classify according to size\n",
    "\n",
    "                    print(f\"{neighbor} = error\")\n",
    "                    for x in apical_branches[neighbor]:\n",
    "                        whole_neuron_labels[x] = \"error\"\n",
    "                else: #try to see if there is any axon like objects off of apical --> if so then error\n",
    "                    #calculate the standard deviation\n",
    "                    std_dev_convex = np.std((trimesh.convex.adjacency_projections(submesh)))\n",
    "                    if std_dev_convex < classifier_axon_std_dev_threshold:\n",
    "                        for x in apical_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"error\"\n",
    "\n",
    "\n",
    "        for label_name, path in shortest_paths.items():\n",
    "            if label_name == possible_Apical: #labels the possible apical as apical\n",
    "                whole_neuron_labels[label_name] = \"apical\"\n",
    "            else:\n",
    "                if possible_Apical in path:\n",
    "                    #if has apical on path and not the apical itself, soma or other label --> label oblique\n",
    "                    for jj in path: \n",
    "                        if jj != possible_Apical and jj != soma_index and whole_neuron_labels[jj] == \"unsure\":\n",
    "                            whole_neuron_labels[jj] = \"oblique\" \n",
    "                else:\n",
    "                    #if NO apical on path and not the apical itself, soma or other label --> label oblique\n",
    "                    for jj in path:\n",
    "                        if jj != possible_Apical and jj != soma_index and whole_neuron_labels[jj] == \"unsure\":\n",
    "                            whole_neuron_labels[jj] = \"basal\" \n",
    "\n",
    "        #return the final list of labels:\n",
    "        self.whole_neuron_labels = whole_neuron_labels\n",
    "        return\n",
    "    \n",
    "    #Step 7\n",
    "    def label_whole_neuron(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        iterates through all of faces and labels them accoring\n",
    "        to the labels assigned to the cgal generic labels\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #instead of going to datajoint for labels\n",
    "        #this just have it locally so don't rely on datajoint\n",
    "\n",
    "        apical_index = 2\n",
    "        basal_index = 3\n",
    "        oblique_index = 4\n",
    "        soma_index = 5\n",
    "        cilia_index = 12\n",
    "        error_index = 10 \n",
    "        axon_index=6\n",
    "\n",
    "\n",
    "        self.final_faces_labels_list = np.zeros(len(self.faces))\n",
    "        \n",
    "\n",
    "        unknown_counter = 0\n",
    "\n",
    "        for i,lab in enumerate(self.labels_list):\n",
    "            #get the category according to the dictionary\n",
    "            cat = self.whole_neuron_labels[lab]\n",
    "            if cat == \"apical\":\n",
    "                self.final_faces_labels_list[i] = apical_index\n",
    "            elif cat == \"basal\":\n",
    "                self.final_faces_labels_list[i] = basal_index\n",
    "            elif cat == \"oblique\":\n",
    "                self.final_faces_labels_list[i] = oblique_index\n",
    "            elif cat == \"soma\":\n",
    "                self.final_faces_labels_list[i] = soma_index\n",
    "            elif cat == \"cilia\":\n",
    "                self.final_faces_labels_list[i] = cilia_index\n",
    "            elif cat == \"axon\":\n",
    "                self.final_faces_labels_list[i] = axon_index\n",
    "            elif cat == \"error\":\n",
    "                self.final_faces_labels_list[i] = error_index\n",
    "            else:\n",
    "                #if wasn't labeled anything just assing it a random color based on cgal assignment\n",
    "                self.final_faces_labels_list[i] = 18 + (int(lab))\n",
    "\n",
    "        \n",
    "    def generate_output_lists(self):\n",
    "        \"\"\"\n",
    "        Will generate the final faces and vertices labels for the classification\n",
    "        \"\"\"\n",
    "\n",
    "        output_faces_list = self.final_faces_labels_list\n",
    "        \n",
    "\n",
    "        #generate the vertices labels\n",
    "        self.generate_verts_to_face_dictionary(output_faces_list)\n",
    "\n",
    "        output_verts_list = [int(self.verts_to_Label[v][0]) for v in self.verts_to_Label]\n",
    "\n",
    "        self.output_verts_labels_list = output_verts_list\n",
    "        return self.final_faces_labels_list, self.output_verts_labels_list \n",
    "    \n",
    "\n",
    "\n",
    "    def return_branches(self,return_cilia=False,\n",
    "                        return_soma=False,\n",
    "                        return_axon=False,\n",
    "                        return_error=False\n",
    "                        ,return_size_threshold=200):\n",
    "        all_components = dict()\n",
    "        \n",
    "        apical_index = 2\n",
    "        basal_index = 3\n",
    "        oblique_index = 4\n",
    "        soma_index = 5\n",
    "        cilia_index = 12\n",
    "        error_index = 10 \n",
    "        axon_index=6\n",
    "        \n",
    "        basal_indexes = np.where(self.final_faces_labels_list == basal_index)[0]\n",
    "        oblique_indexes = np.where(self.final_faces_labels_list == oblique_index)[0]\n",
    "        apical_indexes = np.where(self.final_faces_labels_list == apical_index)[0]\n",
    "        #axon_indexes = np.where(self.final_faces_labels_list == axon_index)\n",
    "        spine_indexes = [np.concatenate([basal_indexes,oblique_indexes,apical_indexes])]\n",
    "        \n",
    "        if spine_indexes[0].size > 0:\n",
    "            #gets all of the dendritic branches\n",
    "            spine_meshes_whole = self.mesh.submesh(spine_indexes,append=True)\n",
    "\n",
    "            split_up_spines = True\n",
    "            #decides if passing back spines as one whole mesh or seperate meshes\n",
    "            if split_up_spines==True:\n",
    "                individual_spines = []\n",
    "                temp_spines = spine_meshes_whole.split(only_watertight=False)\n",
    "                for spine in temp_spines:\n",
    "                    if len(spine.faces) >= return_size_threshold:\n",
    "                        individual_spines.append(spine)\n",
    "            else:\n",
    "                individual_spines = spine_meshes_whole\n",
    "                    \n",
    "        else:\n",
    "            \n",
    "            individual_spines = None\n",
    "        \n",
    "        if individual_spines == []:\n",
    "            individual_spines = None\n",
    "        \n",
    "        \n",
    "        all_components[\"dendrites\"] = individual_spines\n",
    "        \n",
    "        \n",
    "        #will also pass back the cilia,axon or soma based on the parameters of the mesh with the extracted spines\n",
    "        if return_cilia==True:\n",
    "            shaft_indexes = np.where(np.array(self.final_faces_labels_list) == cilia_index) \n",
    "            if shaft_indexes[0].size > 0:\n",
    "                shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "                all_components[\"cilia\"] = shaft_mesh_whole\n",
    "            else:\n",
    "                all_components[\"cilia\"] = None\n",
    "        \n",
    "        #will also pass back the cilia,axon or soma based on the parameters of the mesh with the extracted spines\n",
    "        if return_soma==True:\n",
    "            shaft_indexes = np.where(np.array(self.final_faces_labels_list) == soma_index)\n",
    "            if shaft_indexes[0].size > 0:\n",
    "                shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "                all_components[\"soma\"] = shaft_mesh_whole\n",
    "            else:\n",
    "                all_components[\"soma\"] = None\n",
    "            \n",
    "        #will also pass back the cilia,axon or soma based on the parameters of the mesh with the extracted spines\n",
    "        if return_axon==True:\n",
    "            shaft_indexes = np.where(np.array(self.final_faces_labels_list) == axon_index) \n",
    "            if shaft_indexes[0].size > 0:\n",
    "                shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "                all_components[\"axon\"] = shaft_mesh_whole\n",
    "            else:\n",
    "                all_components[\"axon\"] = None\n",
    "        \n",
    "        \n",
    "        if return_error==True:\n",
    "            shaft_indexes = np.where(np.array(self.final_faces_labels_list) == error_index) \n",
    "            if shaft_indexes[0].size > 0:\n",
    "                #gets all of the dendritic branches\n",
    "                spine_meshes_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "\n",
    "                split_up_spines = True\n",
    "                #decides if passing back spines as one whole mesh or seperate meshes\n",
    "                if split_up_spines==True:\n",
    "                    individual_error = []\n",
    "                    temp_spines = spine_meshes_whole.split(only_watertight=False)\n",
    "                    for spine in temp_spines:\n",
    "                        individual_error.append(spine)\n",
    "                else:\n",
    "                    individual_error = spine_meshes_whole\n",
    "\n",
    "            else:\n",
    "\n",
    "                individual_error = None\n",
    "\n",
    "            if individual_error == []:\n",
    "                individual_error = None\n",
    "        \n",
    "        \n",
    "            all_components[\"error\"] = individual_error\n",
    "        \n",
    "        return all_components\n",
    "            \n",
    "\n",
    "    def clean_files(self):\n",
    "        #clean the files \n",
    "        \n",
    "        #1) new mesh file\n",
    "        #2) cgal files (sdf and labels)\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_branches_whole_neuron(mesh_file_location,\n",
    "                              file_name,\n",
    "                              **kwargs):\n",
    "    \"\"\"\n",
    "    Extracts the meshes of all dendritic branches (optionally soma, axon, cilia meshes)\n",
    "    from a full neuron mesh  (Assumes meshes have been decimated to 35% original size but if not then scaling \n",
    "    can be adjusted using size_multiplier argument)\n",
    "    \n",
    "    Parameters:\n",
    "    mesh_file_location (str): location of the dendritic mesh on computer\n",
    "    file_name (str): file name of dendritic mesh on computer\n",
    "    size_multiplier (float): multiplying factor to help scale all size thresholds in case of up/downsampling of faces (default = 1)\n",
    "    \n",
    "        Option kwargs parameters\n",
    "        \n",
    "    --- Step 1: Mesh importing and Pymeshfix parameters ---\n",
    "    \n",
    "    joincomp : bool, optional (default = True)\n",
    "       Attempts to join nearby open components.\n",
    "\n",
    "    remove_smallest_components : bool, optional (default = False)\n",
    "        Remove all but the largest isolated component from the mesh\n",
    "        before beginning the repair process.  Default True\n",
    "        \n",
    "    --- Step 2: CGAL segmentation parameters ---\n",
    "\n",
    "    clusters (int) : number of clusters to use for CGAL surface mesh segmentation (default = 4)\n",
    "    smoothness (int) : smoothness parameter use for CGAL surface mesh segmentation (default = 0.30)\n",
    "    \n",
    "    --- Step 3: Soma identification parameters ---\n",
    "    \n",
    "    soma_size_threshold (int) : Minimum number of faces (multiplied by size_multipler) of segment to be classified as soma (default = 3000)\n",
    "    \n",
    "    --- Step 4: Findin Soma extensions parameters --- \n",
    "    #if clusters > 3, then will try to relabel small stubs off of soma as soma (helps with identifying axons)\n",
    "    soma_cap_min_width (float): Minimum width size to be categorized as soma extension (default = 0.23) \n",
    "    soma_cap_max_faces (int): Maximum number of faces (multiplied by size_multipler) to be categorized as soma extension (default = 6000)\n",
    "    soma_cap_max_n_connections (int): Maximum number of neighbors to be considered soma extension(default = 6)\n",
    "    large_extension_size (int): Maximum number of faces (multiplied by size_multipler) to be considered a possible large soma extension segment\n",
    "    large_extension_convex_max (float): Maximum value for the mean of the convex adjacency projections for large segments to be considered soma extension (default = 3.0) \n",
    "    \n",
    "    --- Step 5: Apical Identifying Parameters --- \n",
    "    apical_mesh_threshold (int) : Minimum size of segment (multiplied by size_multipler) to be considered possible apical (default = 2000)\n",
    "    apical_height_threshold (int) : Minimum height of bounding box of segment to be considered possible apical (default = 5000) \n",
    "    apical_sdf_threshold (float) : Minimum width of segment to be considered possible apical (default = 0.09)\n",
    "    \n",
    "    --- Step 6: Classifying Entire Mesh Parameters ---\n",
    "    classifier_cilia_threshold (int): Maximum size of segment (multiplied by size_multipler) to be considered possible cilia (default = 1000) \n",
    "    classifier_stub_threshold (int): minimum size of appndage of soma (multiplied by size_multipler) to not be considered stub and merged with the soma (default = 200) \n",
    "    classifier_non_dendrite_convex_threshold (float) : Segment must be above this mean convex value to be considered a possible axon, cilia or error(default = 26.5) \n",
    "    classifier_axon_std_dev_threshold (float): standard deviation of convex measurements for which axon branches are under this threshold (default = 69.0) \n",
    "    classifier_stub_threshold_apical (int) = the minimum size threshold (multiplied by size_multipler) for apical appendage not to be merged with apical(default = 700) \n",
    "    \n",
    "    \n",
    "    ---Step 9: Output Configuration Parameters ---\n",
    "    * if any of the below settings are set to true then will return a dictionary storing \n",
    "    the lists for each mesh category (dendrite,cilia,soma,axon) only for those present that flag is set True\n",
    "    The dendritic branches will always be returned\n",
    "    \n",
    "    return_cilia (bool) : if true will return cilia mesh inside returned dictionary (default = False)\n",
    "    return_soma (bool) : if true will return soma mesh inside returned dictionary (default = False)\n",
    "    return_axon (bool) : if true will return axon mesh inside returned dictionary (default = False)\n",
    "    return_error (bool) : if true will return error mesh inside returned dictionary (default = False)\n",
    "    return_size_threshold (int): Minimum size (multiplied by size_multipler) of dendrite piece to be returned (default = 200)\n",
    "    \n",
    "        -------------------------------------\n",
    "  \n",
    "    Returns: \n",
    "    if return_cilia,return_soma,return_axon,return_error are all set to false: \n",
    "        return  lists of trimesh.mesh/None based on the number of dendrite branches found\n",
    "    if Any of the return_cilia,return_soma,return_axon,return_error are set to true: \n",
    "        returns dictionary containing 4 keys: dendrites,soma,cilia,axon\n",
    "        For each value will return  lists of object (for dendrtiess), trimesh.mesh objects (for other compartments) or None based on the number of that compartment found\n",
    "\n",
    "    Examples:\n",
    "    #returns just simple list of dendrite meshes\n",
    "    list_of_dendrite_meshes = extract_branches_whole_neuron(file_location,file_name)\n",
    "    \n",
    "    #returns dendrite meshes and an available soma mesh\n",
    "    compartment_meshes= complete_spine_extraction(file_location,file_name,return_soma=True)\n",
    "    soma_mesh = compartment_meshes[\"soma\"]\n",
    "    dendrite_mesh_list = compartment_meshes[\"dendrites\"]\n",
    "    \n",
    "    #retruns dendrite meshes but adjusts for not downsampling meshes to 35% original as default settings assume\n",
    "    list_of_dendrite_meshes = extract_branches_whole_neuron(file_location,file_name,size_multiplier=1/0.35)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    global_start = time.time()\n",
    "    \n",
    "    #Step 1: Mesh importing and Pymeshfix parameters\n",
    "    joincomp = kwargs.pop('joincomp', False)\n",
    "    remove_smallest_components = kwargs.pop('remove_smallest_components', True)\n",
    "    \n",
    "    #Step 2: CGAL segmentation parameters\n",
    "    clusters = kwargs.pop('clusters', 4)\n",
    "    smoothness = kwargs.pop('smoothness', 0.30)\n",
    "    \n",
    "    #step 3: Soma identification parameters\n",
    "    size_multiplier = kwargs.pop('size_multiplier', 1)\n",
    "    soma_size_threshold = kwargs.pop(\"soma_size_threshold\",3000)\n",
    "    \n",
    "    #step 4: finding soma extensions parameters\n",
    "    soma_cap_min_width= kwargs.pop('soma_cap_min_width', 0.23) \n",
    "    soma_cap_max_faces= kwargs.pop('soma_cap_max_faces', 6000) \n",
    "    soma_cap_max_n_connections= kwargs.pop('soma_cap_max_n_connections', 6) \n",
    "    large_extension_size = kwargs.pop('large_extension_size', 1500) \n",
    "    large_extension_convex_max= kwargs.pop('soma_cap_conex_threshold', 3) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Step 5: Apical Identifying Parameters\n",
    "    apical_mesh_threshold= kwargs.pop('apical_mesh_threshold', 2000)\n",
    "    apical_height_threshold= kwargs.pop('apical_height_threshold', 5000) \n",
    "    apical_sdf_threshold = kwargs.pop('apical_sdf_threshold', 0.09)\n",
    "    \n",
    "    #Step 6: Classifying Entire Mesh parameters\n",
    "    classifier_cilia_threshold=kwargs.pop('classifier_cilia_threshold', 1000) #maximum size of cilia\n",
    "    classifier_stub_threshold=kwargs.pop('classifier_stub_threshold', 200) # minimum size of appndage of soma to not be considered stub and merged with the soma\n",
    "    classifier_non_dendrite_convex_threshold = kwargs.pop('classifier_non_dendrite_convex_threshold', 27.5) #must be above this value to be axon, cilia or error\n",
    "    classifier_axon_std_dev_threshold = kwargs.pop('classifier_axon_std_dev_threshold', 69) #standard deviation of convex measurements for which axon branches are under this threshold\n",
    "    classifier_stub_threshold_apical = kwargs.pop('classifier_stub_threshold_apical', 700) #the minimum size threshold for apical appendage not to be merged with apical\n",
    "    \n",
    "    #Step 9: Output Configuration Parameters\n",
    "    return_cilia=kwargs.pop('return_cilia', False)\n",
    "    return_soma=kwargs.pop('return_soma', False)\n",
    "    return_axon=kwargs.pop('return_axon', False)\n",
    "    return_error=kwargs.pop('return_error', False)\n",
    "    return_size_threshold=kwargs.pop('return_size_threshold', 200)\n",
    "    \n",
    "    \n",
    "    #making sure there is no more keyword arguments left that you weren't expecting\n",
    "    if kwargs:\n",
    "        raise TypeError('Unexpected **kwargs: %r' % kwargs)\n",
    "    \n",
    "\n",
    "    \n",
    "    #check to see if file exists and if it is an off file\n",
    "    if file_name[-3:] != \"off\":\n",
    "        raise TypeError(\"input file must be a .off \")\n",
    "        return None\n",
    "    if not os.path.isfile(str(Path(mesh_file_location) / Path(file_name))):\n",
    "        raise TypeError(str(Path(mesh_file_location) / Path(file_name)) + \" cannot be found\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"1) Starting: Mesh importing and Pymesh fix\")\n",
    "    classifier = WholeNeuronClassifier(mesh_file_location,file_name,joincomp,remove_smallest_components)\n",
    "    print(f\"1) Finished: Mesh importing and Pymesh fix: {time.time() - start_time}\")\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"2) Staring: Generating CGAL segmentation for neuron\")\n",
    "    classifier.load_cgal_segmentation(clusters,smoothness)\n",
    "    #retrieves the cgal data from the file\n",
    "    \n",
    "\n",
    "    #check to see if files exist\n",
    "    for f in [classifier.labels_file,classifier.sdf_file]:\n",
    "            if not os.path.isfile(f):\n",
    "                print(\"CGAL segmentation files weren't generated\")\n",
    "                raise ValueError(\"CGAL segmentation files weren't generated\")\n",
    "                return \"Failure\"\n",
    "    \n",
    "    classifier.get_cgal_data_and_label_local_optomized()\n",
    "    print(f\"2) Finished: Generating CGAL segmentation for neuron: {time.time() - start_time}\")\n",
    "    \n",
    "\n",
    "    #get the highest values of sdf\n",
    "    start_time = time.time()\n",
    "    print(\"3) Staring: Generating Graph Structure and Identifying Soma\")\n",
    "    soma_index = classifier.get_highest_sdf_part(size_multiplier*soma_size_threshold)\n",
    "    print(\"soma_index = \" + str(soma_index))\n",
    "\n",
    "    #create a graph structure and stats for the whole neuron\n",
    "    classifier.get_graph_structure()\n",
    "    print(f\"3) Finished: Generating Graph Structure and Identifying Soma: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    #gets the caps of the somas created from segmenting into 4 clusters\n",
    "    if clusters > 3:\n",
    "        start_time = time.time()\n",
    "        print(\"4) Staring: Finding Soma Extensions\")\n",
    "        classifier.find_Soma_Caps(soma_index,soma_cap_min_width,\n",
    "                                  soma_cap_max_faces*size_multiplier,\n",
    "                                  soma_cap_max_n_connections,\n",
    "                                  large_extension_size=large_extension_size*size_multiplier,\n",
    "                                  large_extension_convex_max=large_extension_convex_max\n",
    "                                 )\n",
    "\n",
    "        print(f\"4) Finished: Finding Soma Extensions: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"5) Staring: Finding Apical Index\")\n",
    "    #send data to function that will find the Apical\n",
    "    possible_Apical = classifier.find_Apical(soma_index,apical_mesh_threshold*size_multiplier,\n",
    "                                            apical_height_threshold,\n",
    "                                            apical_sdf_threshold)\n",
    "    print(\"possible_Apical = \" + str(possible_Apical))\n",
    "    print(f\"5) Finished: Finding Apical Index: {time.time() - start_time}\")\n",
    "\n",
    "    #use the apical label and the soma label to classify the rest as basal or oblique and return a dictionary that has the mapping of label to compartment type\n",
    "    #but only classifies the cgal labels and not each individual face\n",
    "    start_time = time.time()\n",
    "    print(\"6) Staring: Classifying Entire Neuron\")\n",
    "    classifier.classify_whole_neuron(possible_Apical,soma_index,\n",
    "                                                classifier_cilia_threshold*size_multiplier,\n",
    "                                                classifier_stub_threshold*size_multiplier,\n",
    "                                                classifier_non_dendrite_convex_threshold,\n",
    "                                                classifier_axon_std_dev_threshold,\n",
    "                                                classifier_stub_threshold_apical*size_multiplier\n",
    "                                               )\n",
    "    \n",
    "    #print unique list of labels found\n",
    "    print(\"Total Labels found = \" + str(set(classifier.whole_neuron_labels.values())))\n",
    "    print(f\"6) Finished: Classifying Entire Neuron: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    #label the neurons according to classification\n",
    "    #############NEED TO ADD STEP THAT CALCULATES THE LABELS OF THE VERTICES ##################\n",
    "    start_time = time.time()\n",
    "    print(\"7) Staring: Transfering Segmentation Labels to Face Labels\")\n",
    "    classifier.label_whole_neuron()\n",
    "    print(f\"7) Finished: Transfering Segmentation Labels to Face Labels: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "\n",
    "    #####need to map the final_faces_labels_list to all successive numbers and get vertices\n",
    "    start_time = time.time()\n",
    "    print(\"8) Staring: Generating final Vertex and Face Labels\")\n",
    "    output_faces_list, output_verts_list = classifier.generate_output_lists()\n",
    "    print(f\"8) Finished: Generating final Vertex and Face Labels: {time.time() - start_time}\")\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"9) Staring: Generating Returning Branches\")\n",
    "    dendritic_branches = classifier.return_branches(return_cilia,\n",
    "                                                    return_soma,\n",
    "                                                    return_axon,\n",
    "                                                    return_error,\n",
    "                                                    return_size_threshold*size_multiplier)\n",
    "    print(f\"9) Finished: Generating Returning Branches: {time.time() - start_time}\")\n",
    "    \n",
    "    \n",
    "    dendrites_segments = dendritic_branches.pop(\"dendrites\",None)\n",
    "    cilia_segments = dendritic_branches.pop(\"cilia\",None)\n",
    "    soma_segments = dendritic_branches.pop(\"soma\",None)\n",
    "    axon_segments = dendritic_branches.pop(\"axon\",None)\n",
    "    error_segments = dendritic_branches.pop(\"error\",None)\n",
    "                                      \n",
    "    size_one = np.array(5).shape\n",
    "    \n",
    "    if dendrites_segments == None:\n",
    "        dendrites_number = 0\n",
    "    elif np.asarray(dendrites_segments).shape == size_one:\n",
    "        dendrites_number = 1\n",
    "    else:\n",
    "        dendrites_number = len(dendrites_segments)\n",
    "    print(f\"Returning: \\n{dendrites_number} dendritic branches\")\n",
    "    \n",
    "    if return_cilia == True:\n",
    "        if cilia_segments == None:\n",
    "            cilia_number = 0\n",
    "        elif np.asarray(cilia_segments).shape == size_one:\n",
    "            cilia_number = 1\n",
    "        else:\n",
    "            cilia_number = len(cilia_segments)\n",
    "        print(f\" {cilia_number} cilia\")\n",
    "    if return_soma == True:\n",
    "        if soma_segments == None:\n",
    "            soma_number = 0\n",
    "        elif np.asarray(soma_segments).shape == size_one:\n",
    "            soma_number = 1\n",
    "        else:\n",
    "            soma_number = len(soma_segments)\n",
    "        print(f\" {soma_number} soma\")\n",
    "    if return_axon == True:\n",
    "        if axon_segments == None:\n",
    "            axon_number = 0\n",
    "        elif np.asarray(axon_segments).shape == size_one:\n",
    "            axon_number = 1\n",
    "        else:\n",
    "            axon_number = len(axon_segments)\n",
    "        print(f\" {axon_number} axon\")\n",
    "    if return_error == True:\n",
    "        if error_segments == None:\n",
    "            error_number = 0\n",
    "        elif np.asarray(error_segments).shape == size_one:\n",
    "            axon_number = 1\n",
    "        else:\n",
    "            error_number = len(error_segments)\n",
    "        print(f\" {error_number} errors\")\n",
    "    \n",
    "          \n",
    "    print(f\"Total time: {time.time() - global_start }\")\n",
    "\n",
    "    dendritic_branches[\"dendrites\"] =  dendrites_segments\n",
    "    dendritic_branches[\"cilia\"] =  cilia_segments\n",
    "    dendritic_branches[\"soma\"] =  soma_segments\n",
    "    dendritic_branches[\"axon\"] =  axon_segments\n",
    "    dendritic_branches[\"error\"] =  error_segments\n",
    "\n",
    "    \n",
    "    if return_cilia == False and return_soma == False and return_axon == False and return_error == False:\n",
    "          return dendritic_branches[\"dendrites\"],output_faces_list,classifier\n",
    "    else:\n",
    "          return dendritic_branches,output_faces_list,classifier\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     #passed on the output configurations, return the data\n",
    "#     if \n",
    "\n",
    "\n",
    "\n",
    "#output the faces_labels to a file to test\n",
    "# with open(str(segment_id) + \"_classification.csv\") as file:\n",
    "    \n",
    "#     csv_writer = csv\n",
    "    \n",
    "#             with open(labels_file) as csvfile:\n",
    "#             #print(\"inside labels file\")\n",
    "\n",
    "#             for i,row in enumerate(csv.reader(csvfile)):\n",
    "#                 triangles_labels[i] = int(row[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta3p100 = dj.create_virtual_module(\"ta3p100\",\"microns_ta3p100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([648518346341366885, 648518346341371119, 648518346341388820,\n",
       "       648518346341393609, 648518346342797556, 648518346342806978,\n",
       "       648518346349468264, 648518346349470171, 648518346349471156,\n",
       "       648518346349471500, 648518346349471562, 648518346349471565,\n",
       "       648518346349471910, 648518346349472574, 648518346349472601,\n",
       "       648518346349473044, 648518346349473160, 648518346349473583,\n",
       "       648518346349473597, 648518346349473781, 648518346349473804,\n",
       "       648518346349473808, 648518346349473811, 648518346349473813,\n",
       "       648518346349473816, 648518346349473819, 648518346349473821,\n",
       "       648518346349473830, 648518346349473832, 648518346349473833,\n",
       "       648518346349473835, 648518346349473838, 648518346349473844,\n",
       "       648518346349473847, 648518346349473848, 648518346349474146,\n",
       "       648518346349474703, 648518346349475035, 648518346349475101,\n",
       "       648518346349475132, 648518346349475150, 648518346349475425,\n",
       "       648518346349475510, 648518346349475518, 648518346349475520,\n",
       "       648518346349475521, 648518346349475522, 648518346349475523,\n",
       "       648518346349475524, 648518346349475525, 648518346349475526,\n",
       "       648518346349475530, 648518346349475531, 648518346349475536,\n",
       "       648518346349475540, 648518346349476159, 648518346349476185,\n",
       "       648518346349476961, 648518346349477981, 648518346349478054,\n",
       "       648518346349478118, 648518346349478183, 648518346349478197,\n",
       "       648518346349478248, 648518346349478330, 648518346349478348,\n",
       "       648518346349478380, 648518346349478399, 648518346349478431,\n",
       "       648518346349478473, 648518346349478700, 648518346349478718,\n",
       "       648518346349478785, 648518346349478830, 648518346349478860,\n",
       "       648518346349478913, 648518346349479056, 648518346349479094,\n",
       "       648518346349479127, 648518346349479254, 648518346349479478,\n",
       "       648518346349479479, 648518346349479573, 648518346349479706,\n",
       "       648518346349479776, 648518346349479837, 648518346349479981,\n",
       "       648518346349480499, 648518346349481014, 648518346349481574,\n",
       "       648518346349482196, 648518346349482312, 648518346349482676,\n",
       "       648518346349483124, 648518346349483228, 648518346349483956,\n",
       "       648518346349484607, 648518346349484832, 648518346349485007,\n",
       "       648518346349485701, 648518346349485870, 648518346349486885,\n",
       "       648518346349486929, 648518346349487499, 648518346349487734,\n",
       "       648518346349488659, 648518346349490527, 648518346349490614,\n",
       "       648518346349490624, 648518346349490654, 648518346349490749,\n",
       "       648518346349490796, 648518346349491045, 648518346349491311,\n",
       "       648518346349491736, 648518346349491811, 648518346349491953,\n",
       "       648518346349491984, 648518346349492078, 648518346349492097,\n",
       "       648518346349492197, 648518346349492662, 648518346349493106,\n",
       "       648518346349493117, 648518346349493260, 648518346349493354,\n",
       "       648518346349493653, 648518346349493733, 648518346349493856,\n",
       "       648518346349493881, 648518346349493981, 648518346349494072,\n",
       "       648518346349494087, 648518346349494194, 648518346349494515,\n",
       "       648518346349494539, 648518346349494577, 648518346349494969,\n",
       "       648518346349495174, 648518346349495181, 648518346349495243,\n",
       "       648518346349495341, 648518346349495452, 648518346349495481,\n",
       "       648518346349495660, 648518346349495846, 648518346349495935,\n",
       "       648518346349495971, 648518346349496058, 648518346349496103,\n",
       "       648518346349496245, 648518346349496278, 648518346349496405,\n",
       "       648518346349496554, 648518346349497094, 648518346349497151,\n",
       "       648518346349497168, 648518346349497759, 648518346349498051,\n",
       "       648518346349498116, 648518346349498239, 648518346349498286,\n",
       "       648518346349498482, 648518346349498566, 648518346349498632,\n",
       "       648518346349498835, 648518346349499005, 648518346349499085,\n",
       "       648518346349499150, 648518346349499186, 648518346349499297,\n",
       "       648518346349499369, 648518346349499581, 648518346349499624,\n",
       "       648518346349499636, 648518346349499669, 648518346349499679,\n",
       "       648518346349499680, 648518346349499689, 648518346349499701,\n",
       "       648518346349499759, 648518346349499773, 648518346349499783,\n",
       "       648518346349499803, 648518346349499824, 648518346349499828,\n",
       "       648518346349499851, 648518346349499852, 648518346349499876,\n",
       "       648518346349499896, 648518346349499910, 648518346349499928,\n",
       "       648518346349499939, 648518346349500049, 648518346349500120,\n",
       "       648518346349500126, 648518346349500138, 648518346349500139,\n",
       "       648518346349500155, 648518346349500162, 648518346349500181,\n",
       "       648518346349500277, 648518346349500320, 648518346349500324,\n",
       "       648518346349500341, 648518346349500431, 648518346349500440,\n",
       "       648518346349500486, 648518346349500627, 648518346349500657,\n",
       "       648518346349500672, 648518346349500725, 648518346349500742,\n",
       "       648518346349500939, 648518346349500954, 648518346349501011,\n",
       "       648518346349501175, 648518346349501216, 648518346349501481,\n",
       "       648518346349501517, 648518346349501587, 648518346349501597,\n",
       "       648518346349501625, 648518346349501746, 648518346349501787,\n",
       "       648518346349502049, 648518346349502203, 648518346349502434,\n",
       "       648518346349502613, 648518346349502767, 648518346349502920,\n",
       "       648518346349503011, 648518346349503086, 648518346349503140,\n",
       "       648518346349503236, 648518346349503453, 648518346349503473,\n",
       "       648518346349503588, 648518346349503591, 648518346349503592,\n",
       "       648518346349503643, 648518346349503717, 648518346349503766,\n",
       "       648518346349503767, 648518346349503773, 648518346349503924,\n",
       "       648518346349504065, 648518346349504130, 648518346349504185,\n",
       "       648518346349504273, 648518346349504324, 648518346349504442,\n",
       "       648518346349504521, 648518346349504565, 648518346349504618,\n",
       "       648518346349504754, 648518346349504826, 648518346349504955,\n",
       "       648518346349505061, 648518346349505144, 648518346349505250,\n",
       "       648518346349505261, 648518346349505450, 648518346349505512,\n",
       "       648518346349505592, 648518346349505640, 648518346349505696,\n",
       "       648518346349505739, 648518346349505826, 648518346349505911,\n",
       "       648518346349506008, 648518346349506092, 648518346349506159,\n",
       "       648518346349506425, 648518346349506596, 648518346349506617,\n",
       "       648518346349506684, 648518346349506748, 648518346349506770,\n",
       "       648518346349506778, 648518346349506803, 648518346349506944,\n",
       "       648518346349507015, 648518346349507087, 648518346349507092,\n",
       "       648518346349507103, 648518346349507175, 648518346349507226,\n",
       "       648518346349507351, 648518346349507355, 648518346349507537,\n",
       "       648518346349507717, 648518346349507788, 648518346349507963,\n",
       "       648518346349507964, 648518346349507984])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ta3p100.CleansedMesh() & \"n_vertices>100000\").fetch(\"segment_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********working on 648518346342797556 *****************\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "Loading mesh from neurons/neuron_648518346342797556.off\n",
      "Starting pymeshfix algorithm\n",
      "Finished pymeshfix algorithm: 72.37338423728943\n",
      "1) Finished: Mesh importing and Pymesh fix: 78.76498699188232\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "1\n",
      "Finished CGAL segmentation algorithm: 419.7427628040314\n",
      "2) Finished: Generating CGAL segmentation for neuron: 432.99543595314026\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "soma_index = 1\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.17552852630615234\n",
      "4) Staring: Finding Soma Extensions\n",
      "Found 3 soma caps and replacing labels: [8, 675, 676]\n",
      "done replacing soma cap labels : 7.151614665985107\n",
      "4) Finished: Finding Soma Extensions: 7.612066268920898\n",
      "5) Staring: Finding Apical Index\n",
      "Soma Index = 1\n",
      "Soma Connections = [96, 53, 52, 7, 2, 673, 180, 674, 591, 672, 363]\n",
      "possible_Axons_filter_1 = [7, 674, 672]\n",
      "possible_Axons_filter_2 = [7]\n",
      "possible_Apical = 7\n",
      "5) Finished: Finding Apical Index: 0.030881404876708984\n",
      "6) Staring: Classifying Entire Neuron\n",
      "180 = cilia\n",
      "363 = stub soma\n",
      "591 = stub soma\n",
      "672 = stub soma\n",
      "673 = stub soma\n",
      "674 = stub soma\n",
      "6 = stub apical\n",
      "21 = error\n",
      "22 = stub apical\n",
      "24 = stub apical\n",
      "42 = stub apical\n",
      "55 = stub apical\n",
      "71 = stub apical\n",
      "72 = stub apical\n",
      "76 = stub apical\n",
      "79 = stub apical\n",
      "89 = stub apical\n",
      "92 = stub apical\n",
      "108 = error\n",
      "110 = stub apical\n",
      "114 = stub apical\n",
      "120 = stub apical\n",
      "123 = stub apical\n",
      "130 = stub apical\n",
      "142 = error\n",
      "156 = stub apical\n",
      "158 = stub apical\n",
      "629 = stub apical\n",
      "163 = stub apical\n",
      "166 = stub apical\n",
      "169 = stub apical\n",
      "170 = stub apical\n",
      "181 = stub apical\n",
      "182 = stub apical\n",
      "183 = stub apical\n",
      "184 = stub apical\n",
      "185 = error\n",
      "186 = stub apical\n",
      "187 = stub apical\n",
      "188 = stub apical\n",
      "189 = stub apical\n",
      "190 = stub apical\n",
      "191 = stub apical\n",
      "192 = stub apical\n",
      "193 = stub apical\n",
      "194 = stub apical\n",
      "195 = stub apical\n",
      "196 = stub apical\n",
      "197 = stub apical\n",
      "198 = stub apical\n",
      "199 = stub apical\n",
      "200 = stub apical\n",
      "201 = stub apical\n",
      "202 = stub apical\n",
      "205 = stub apical\n",
      "206 = stub apical\n",
      "207 = stub apical\n",
      "208 = stub apical\n",
      "209 = stub apical\n",
      "210 = stub apical\n",
      "211 = stub apical\n",
      "212 = stub apical\n",
      "213 = stub apical\n",
      "216 = stub apical\n",
      "217 = stub apical\n",
      "218 = stub apical\n",
      "219 = stub apical\n",
      "220 = stub apical\n",
      "221 = stub apical\n",
      "222 = stub apical\n",
      "223 = stub apical\n",
      "224 = stub apical\n",
      "225 = stub apical\n",
      "226 = stub apical\n",
      "227 = stub apical\n",
      "610 = stub apical\n",
      "611 = stub apical\n",
      "613 = stub apical\n",
      "623 = stub apical\n",
      "624 = stub apical\n",
      "625 = stub apical\n",
      "626 = stub apical\n",
      "627 = stub apical\n",
      "628 = stub apical\n",
      "630 = stub apical\n",
      "631 = stub apical\n",
      "632 = stub apical\n",
      "633 = stub apical\n",
      "634 = stub apical\n",
      "635 = stub apical\n",
      "636 = stub apical\n",
      "637 = stub apical\n",
      "638 = stub apical\n",
      "639 = stub apical\n",
      "640 = stub apical\n",
      "641 = stub apical\n",
      "643 = stub apical\n",
      "644 = stub apical\n",
      "645 = stub apical\n",
      "646 = stub apical\n",
      "647 = stub apical\n",
      "648 = stub apical\n",
      "649 = stub apical\n",
      "650 = stub apical\n",
      "651 = stub apical\n",
      "652 = stub apical\n",
      "653 = stub apical\n",
      "655 = stub apical\n",
      "656 = stub apical\n",
      "658 = stub apical\n",
      "659 = stub apical\n",
      "660 = stub apical\n",
      "661 = stub apical\n",
      "662 = stub apical\n",
      "663 = stub apical\n",
      "Total Labels found = {'oblique', 'apical', 'error', 'cilia', 'basal', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 1.467151403427124\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.2939586639404297\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 6.510622501373291\n",
      "9) Staring: Generating Returning Branches\n",
      "9) Finished: Generating Returning Branches: 1.0501694679260254\n",
      "Returning: \n",
      "5 dendritic branches\n",
      " 1 cilia\n",
      " 1 soma\n",
      " 0 axon\n",
      " 4 errors\n",
      "Total time: 528.9019553661346\n",
      "*********working on 648518346342806978 *****************\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "Loading mesh from neurons/neuron_648518346342806978.off\n",
      "Starting pymeshfix algorithm\n",
      "Finished pymeshfix algorithm: 31.5351345539093\n",
      "1) Finished: Mesh importing and Pymesh fix: 35.007123708724976\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "1\n",
      "Finished CGAL segmentation algorithm: 247.6111524105072\n",
      "2) Finished: Generating CGAL segmentation for neuron: 257.39505314826965\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "soma_index = 5\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.14054036140441895\n",
      "4) Staring: Finding Soma Extensions\n",
      "Found 17 soma caps and replacing labels: [86, 321, 224, 271, 342, 273, 343, 340, 128, 270, 327, 272, 336, 328, 326, 341, 335]\n",
      "done replacing soma cap labels : 5.032841444015503\n",
      "4) Finished: Finding Soma Extensions: 5.537003040313721\n",
      "5) Staring: Finding Apical Index\n",
      "Soma Index = 5\n",
      "Soma Connections = [73, 28, 338, 9, 45, 325, 42, 14, 274, 136, 339, 337]\n",
      "possible_Axons_filter_1 = [28, 274]\n",
      "possible_Axons_filter_2 = [28]\n",
      "possible_Apical = 28\n",
      "5) Finished: Finding Apical Index: 0.022521495819091797\n",
      "6) Staring: Classifying Entire Neuron\n",
      "42 = cilia\n",
      "136 = stub soma\n",
      "325 = cilia\n",
      "274 = stub soma\n",
      "337 = stub soma\n",
      "338 = stub soma\n",
      "339 = stub soma\n",
      "4 = error\n",
      "55 = error\n",
      "Total Labels found = {'oblique', 'apical', 'error', 'cilia', 'basal', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 0.9260845184326172\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.2778294086456299\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 4.940727710723877\n",
      "9) Staring: Generating Returning Branches\n",
      "9) Finished: Generating Returning Branches: 0.651491641998291\n",
      "Returning: \n",
      "4 dendritic branches\n",
      " 1 cilia\n",
      " 1 soma\n",
      " 0 axon\n",
      " 2 errors\n",
      "Total time: 304.9000835418701\n",
      "*********working on 648518346349468264 *****************\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "Loading mesh from neurons/neuron_648518346349468264.off\n",
      "Starting pymeshfix algorithm\n",
      "Finished pymeshfix algorithm: 34.574706077575684\n",
      "1) Finished: Mesh importing and Pymesh fix: 38.24586296081543\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "1\n",
      "Finished CGAL segmentation algorithm: 314.4477233886719\n",
      "2) Finished: Generating CGAL segmentation for neuron: 325.5268135070801\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "soma_index = 1\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.13098478317260742\n",
      "4) Staring: Finding Soma Extensions\n",
      "Found 2 soma caps and replacing labels: [236, 16]\n",
      "done replacing soma cap labels : 6.349349737167358\n",
      "4) Finished: Finding Soma Extensions: 6.654047250747681\n",
      "5) Staring: Finding Apical Index\n",
      "Soma Index = 1\n",
      "Soma Connections = [109, 70, 24, 22, 9, 2, 217, 235, 234, 71, 233]\n",
      "possible_Axons_filter_1 = [22]\n",
      "possible_Axons_filter_2 = [22]\n",
      "possible_Apical = 22\n",
      "5) Finished: Finding Apical Index: 0.031433820724487305\n",
      "6) Staring: Classifying Entire Neuron\n",
      "109 = axon\n",
      "71 = stub soma\n",
      "233 = stub soma\n",
      "234 = stub soma\n",
      "235 = stub soma\n",
      "10 = stub apical\n",
      "12 = error\n",
      "23 = stub apical\n",
      "35 = stub apical\n",
      "75 = stub apical\n",
      "80 = stub apical\n",
      "81 = stub apical\n",
      "85 = stub apical\n",
      "90 = stub apical\n",
      "99 = stub apical\n",
      "113 = stub apical\n",
      "117 = stub apical\n",
      "119 = stub apical\n",
      "123 = stub apical\n",
      "131 = stub apical\n",
      "132 = stub apical\n",
      "133 = stub apical\n",
      "134 = stub apical\n",
      "135 = stub apical\n",
      "136 = stub apical\n",
      "137 = stub apical\n",
      "138 = stub apical\n",
      "143 = stub apical\n",
      "155 = stub apical\n",
      "157 = stub apical\n",
      "158 = stub apical\n",
      "164 = stub apical\n",
      "173 = stub apical\n",
      "178 = stub apical\n",
      "180 = stub apical\n",
      "183 = stub apical\n",
      "187 = stub apical\n",
      "201 = stub apical\n",
      "218 = stub apical\n",
      "221 = stub apical\n",
      "222 = stub apical\n",
      "223 = stub apical\n",
      "224 = stub apical\n",
      "225 = stub apical\n",
      "226 = stub apical\n",
      "227 = stub apical\n",
      "228 = stub apical\n",
      "252 = stub apical\n",
      "254 = stub apical\n",
      "255 = stub apical\n",
      "256 = stub apical\n",
      "257 = stub apical\n",
      "260 = stub apical\n",
      "261 = stub apical\n",
      "262 = stub apical\n",
      "263 = stub apical\n",
      "265 = stub apical\n",
      "266 = stub apical\n",
      "267 = stub apical\n",
      "268 = stub apical\n",
      "269 = stub apical\n",
      "270 = stub apical\n",
      "271 = stub apical\n",
      "272 = stub apical\n",
      "273 = stub apical\n",
      "274 = stub apical\n",
      "275 = stub apical\n",
      "277 = stub apical\n",
      "278 = stub apical\n",
      "279 = stub apical\n",
      "280 = stub apical\n",
      "281 = stub apical\n",
      "390 = stub apical\n",
      "392 = stub apical\n",
      "393 = stub apical\n",
      "394 = stub apical\n",
      "395 = stub apical\n",
      "396 = stub apical\n",
      "397 = stub apical\n",
      "398 = stub apical\n",
      "399 = stub apical\n",
      "400 = stub apical\n",
      "401 = stub apical\n",
      "402 = stub apical\n",
      "403 = stub apical\n",
      "404 = stub apical\n",
      "405 = stub apical\n",
      "406 = stub apical\n",
      "407 = stub apical\n",
      "408 = stub apical\n",
      "Total Labels found = {'axon', 'oblique', 'apical', 'error', 'basal', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 1.1937072277069092\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.2704436779022217\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 5.443347215652466\n",
      "9) Staring: Generating Returning Branches\n",
      "9) Finished: Generating Returning Branches: 0.774507999420166\n",
      "Returning: \n",
      "6 dendritic branches\n",
      " 0 cilia\n",
      " 1 soma\n",
      " 1 axon\n",
      " 1 errors\n",
      "Total time: 378.2728362083435\n",
      "*********working on 648518346349471156 *****************\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "Loading mesh from neurons/neuron_648518346349471156.off\n",
      "Starting pymeshfix algorithm\n",
      "Finished pymeshfix algorithm: 41.94371056556702\n",
      "1) Finished: Mesh importing and Pymesh fix: 46.030518531799316\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "1\n",
      "Finished CGAL segmentation algorithm: 224.51112484931946\n",
      "2) Finished: Generating CGAL segmentation for neuron: 234.19008135795593\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "soma_index = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.11751556396484375\n",
      "4) Staring: Finding Soma Extensions\n",
      "Found 1 soma caps and replacing labels: [175]\n",
      "done replacing soma cap labels : 4.99504280090332\n",
      "4) Finished: Finding Soma Extensions: 5.2541282176971436\n",
      "5) Staring: Finding Apical Index\n",
      "Soma Index = 0\n",
      "Soma Connections = [61, 56, 55, 119, 9, 68, 59, 149, 208, 93, 207, 206]\n",
      "possible_Axons_filter_1 = [56, 9, 93]\n",
      "possible_Axons_filter_2 = []\n",
      "possible_Apical = None\n",
      "5) Finished: Finding Apical Index: 0.018798828125\n",
      "6) Staring: Classifying Entire Neuron\n",
      "119 = error\n",
      "208 = cilia\n",
      "93 = stub soma\n",
      "149 = stub soma\n",
      "206 = stub soma\n",
      "207 = stub soma\n",
      "Total Labels found = {'unsure', 'error', 'soma', 'cilia'}\n",
      "6) Finished: Classifying Entire Neuron: 0.9409172534942627\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.28469347953796387\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 4.848282337188721\n",
      "9) Staring: Generating Returning Branches\n",
      "9) Finished: Generating Returning Branches: 0.20264363288879395\n",
      "Returning: \n",
      "0 dendritic branches\n",
      " 1 cilia\n",
      " 1 soma\n",
      " 0 axon\n",
      " 1 errors\n",
      "Total time: 291.88911056518555\n"
     ]
    }
   ],
   "source": [
    "#Other neurons tried:\n",
    "#segment_id = 648518346349473045\n",
    "#segment_id = 648518346349386137\n",
    "#segment_id = 648518346349472574\n",
    "#segment_id = 648518346349473044\n",
    "#segment_id = 648518346349386137\n",
    "segment_id = 648518346349473044 #worked well for this one\n",
    "#segment_id = 648518346349386137\n",
    "segment_id = 648518346349472574\n",
    "segment_id = 648518346341393609\n",
    "\n",
    "# mesh_file_location = \"./neurons\"\n",
    "# file_name = \"neuron_\" + str(segment_id) + \".off\"\n",
    "# dendritic_branches,output_faces_list,classifier = extract_branches_whole_neuron(mesh_file_location,file_name,\n",
    "#                             return_cilia = True,\n",
    "#                             return_soma = True,\n",
    "#                             return_axon = True,\n",
    "#                             return_error = True)\n",
    "\n",
    "to_test_segments = [648518346342797556,648518346342806978,648518346349468264,648518346349471156]\n",
    "\n",
    "\n",
    "segment_results = dict()\n",
    "for segment_id in to_test_segments:\n",
    "    print(f\"*********working on {segment_id} *****************\")\n",
    "    mesh_file_location = \"./neurons\"\n",
    "    file_name = \"neuron_\" + str(segment_id) + \".off\"\n",
    "    dendritic_branches,output_faces_list,classifier = extract_branches_whole_neuron(mesh_file_location,file_name,\n",
    "                                return_cilia = True,\n",
    "                                return_soma = True,\n",
    "                                return_axon = True,\n",
    "                                return_error = True)\n",
    "    segment_results[segment_id] = [dendritic_branches,output_faces_list,classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([648518346342797556, 648518346342806978, 648518346349468264, 648518346349471156])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648518346342797556\n",
      "Done writing OFF file\n",
      "shaft 0\n",
      "Done writing OFF file\n",
      "shaft 1\n",
      "Done writing OFF file\n",
      "shaft 2\n",
      "Done writing OFF file\n",
      "shaft 3\n",
      "Done writing OFF file\n",
      "shaft 4\n",
      "648518346342806978\n",
      "Done writing OFF file\n",
      "shaft 0\n",
      "Done writing OFF file\n",
      "shaft 1\n",
      "Done writing OFF file\n",
      "shaft 2\n",
      "Done writing OFF file\n",
      "shaft 3\n",
      "648518346349468264\n",
      "Done writing OFF file\n",
      "shaft 0\n",
      "Done writing OFF file\n",
      "shaft 1\n",
      "Done writing OFF file\n",
      "shaft 2\n",
      "Done writing OFF file\n",
      "shaft 3\n",
      "Done writing OFF file\n",
      "shaft 4\n",
      "Done writing OFF file\n",
      "shaft 5\n",
      "648518346349471156\n"
     ]
    }
   ],
   "source": [
    "for neuron_id in segment_results.keys():\n",
    "    print(neuron_id)\n",
    "    if segment_results[neuron_id][0][\"dendrites\"] != None:\n",
    "        for i,seg in enumerate(segment_results[neuron_id][0][\"dendrites\"]):\n",
    "            write_Whole_Neuron_Off_file(str(neuron_id) + \"shaft_\" + str(i), seg.vertices,seg.faces)\n",
    "            print(\"shaft \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., ..., 3., 3., 3.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_faces_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the classification\n",
    "#output all of the final labels for the fixed meshes\n",
    "segment_results = dict(segment_id=output_faces_list)\n",
    "for seg_id,vals in segment_results.items():\n",
    "    final_lables = vals\n",
    "\n",
    "    with open(str(seg_id) + \"_classification.csv\",\"w\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile,delimiter=\",\")\n",
    "        for i in final_lables:\n",
    "            csv_writer.writerow([int(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

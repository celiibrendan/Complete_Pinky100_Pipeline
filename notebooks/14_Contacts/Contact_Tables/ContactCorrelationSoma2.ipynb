{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import time\n",
    "import pymeshfix\n",
    "import os\n",
    "import datetime\n",
    "import calcification_Module as cm\n",
    "from meshparty import trimesh_io\n",
    "\n",
    "#for supressing the output\n",
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "#for error counting\n",
    "from collections import Counter\n",
    "\n",
    "#for reading in the new raw_skeleton files\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "#setting the address and the username\n",
    "dj.config['database.host'] = '10.28.0.34'\n",
    "dj.config['database.user'] = 'celiib'\n",
    "dj.config['database.password'] = 'newceliipass'\n",
    "dj.config['safemode']=True\n",
    "dj.config[\"display.limit\"] = 20\n",
    "\n",
    "schema = dj.schema('microns_ta3p100')\n",
    "ta3p100 = dj.create_virtual_module('ta3p100', 'microns_ta3p100')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the pearson correlation and cosine similarity while accounting for the corner cases\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "\n",
    "def find_pearson(v1,v2):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if np.array_equal(v1,v2):\n",
    "            return 1\n",
    "        elif abs(sum(v1 - v2)) == v1.size:\n",
    "            return -1\n",
    "        else:\n",
    "            #perform the pearson correlation\n",
    "            corr_conversion, p_value_conversion = pearsonr(v1, v2)\n",
    "            return corr_conversion\n",
    "\n",
    "def find_cosine(v1,v2):\n",
    "    if np.array_equal(v1,v2):\n",
    "        return 1\n",
    "    elif abs(sum(v1 - v2)) == v1.size:\n",
    "        return 0\n",
    "    else:\n",
    "        v1 = v1.reshape(1,len(v1))\n",
    "        v2 = v2.reshape(1,len(v2))\n",
    "        return cosine_similarity(v1, v2)[0][0]\n",
    "\n",
    "def find_binary_sim(v1,v2):\n",
    "    a = np.dot(v1,v2)\n",
    "    b = np.dot(1-v1,v2)\n",
    "    c = np.dot(v1,1-v2)\n",
    "    d = np.dot(1-v1,1-v2)\n",
    "    #print(a,b,c,d)\n",
    "\n",
    "    return (a)/(a + b + c + d),(a + d)/(a + b + c + d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class ContactCorrelationSoma2(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> ta3p100.Segment\n",
    "    segment_b :bigint unsigned #id of the postsynaptic neuron\n",
    "    ---\n",
    "    n_seg_a              :bigint unsigned #n_presyns contacting onto segment_id\n",
    "    n_seg_b              :bigint unsigned #n_presyns contacting onto segment_b\n",
    "    n_seg_shared           :bigint unsigned #n_presyns contacting onto both segment_id and segment_b\n",
    "    n_seg_union            :bigint unsigned #n_presyns contacting either segment_id or segment_b\n",
    "    n_seg_shared_converted :bigint unsigned #n_presyns contacting onto both and converting on at least 1 postsyn\n",
    "    n_seg_a_converted      :bigint unsigned #n_presyns contacting onto both and converting on postsyna a\n",
    "    n_seg_a_converted_prop=null :float           #proportion of n_presyns contacting onto both which convert at least onto postsyna a\n",
    "    n_seg_b_converted      :bigint unsigned #n_presyns contacting onto both and converting on postsyna b\n",
    "    n_seg_b_converted_prop=null :float           #proportion of n_presyns contacting onto both which convert at least onto postsyna b\n",
    "    binary_conversion_pearson=null :float   #pearson correlation for binary n_synapse/n_contact rate\n",
    "    binary_conversion_cosine=null :float    #cosine similarity correlation for binary n_synapse/n_contact rate\n",
    "    binary_conv_jaccard_ones_ratio=null :float   #a / (a + b + c  + d) for jaccard similarity of binary conversion rate\n",
    "    binary_conv_jaccard_matching_ratio=null :float  # ( a + d )/ (a + b + c  + d) for jaccard similarity of binary conversion rate\n",
    "    conversion_pearson=null :float          #Pearson correlation for n_synapse/n_contact rate\n",
    "    conversion_cosine=null :float           #cosine similarity for n_synapse/n_contact rate\n",
    "    density_pearson=null :float             #Pearson correlation for n_synapse/postsyn_length rate\n",
    "    density_cosine=null :float              #cosine similarity for n_synapse/postsyn_length rate\n",
    "    synapse_volume_mean_pearson=null :float     #Pearson correlation for mean of synaptic volume\n",
    "    synapse_volume_mean_cosine=null :float      #cosine similarity for mean of synaptic volume\n",
    "    synapse_vol_density_pearson=null :float         #Pearson correlation for n_synapses*synapse_sizes_mean/postsyn_length rate\n",
    "    synapse_vol_density_cosine=null :float          #cosine similarity for n_synapses*synapse_sizes_mean/postsyn_length rate\n",
    "    binary_conversion_pearson_converted=null :float   #pearson correlation for binary n_synapse/n_contact rate for axon group with at least 1 conversion\n",
    "    binary_conversion_cosine_converted=null :float    #cosine similarity correlation for binary n_synapse/n_contact rate for axon group with at least 1 conversion\n",
    "    binary_conv_jaccard_ones_ratio_converted=null :float   #a / (a + b + c  + d) for jaccard similarity of binary conversion rate with at least 1 conversion\n",
    "    binary_conv_jaccard_matching_ratio_converted=null :float  # ( a + d )/ (a + b + c  + d) for jaccard similarity of binary conversion rate with at least 1 conversion\n",
    "    conversion_pearson_converted=null :float          #Pearson correlation for n_synapse/n_contact rate for axon group with at least 1 conversion\n",
    "    conversion_cosine_converted=null :float           #cosine similarity for n_synapse/n_contact rate for axon group with at least 1 conversion\n",
    "    density_pearson_converted=null :float             #Pearson correlation for n_synapse/postsyn_length rate for axon group with at least 1 conversion\n",
    "    density_cosine_converted=null :float              #cosine similarity for n_synapse/postsyn_length rate for axon group with at least 1 conversion\n",
    "    synapse_volume_mean_pearson_converted=null :float     #Pearson correlation for mean of synaptic volume for axon group with at least 1 conversion\n",
    "    synapse_volume_mean_cosine_converted=null :float      #cosine similarity for mean of synaptic volume for axon group with at least 1 conversion\n",
    "    synapse_vol_density_pearson_converted=null :float         #Pearson correlation for n_synapses*synapse_sizes_mean/postsyn_length rate for axon group with at least 1 conversion\n",
    "    synapse_vol_density_cosine_converted=null :float          #cosine similarity for n_synapses*synapse_sizes_mean/postsyn_length rate for axon group with at least 1 conversion\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    key_source = ta3p100.Segmentation & ta3p100.CurrentSegmentation\n",
    "    \n",
    "    def make(self,key):\n",
    "        #Retrieves the PrePost table that will be using in an all in one insertion (MAY HAVE TO ADJUST FOR BIGGER DATA SETS IN FUTURE)\n",
    "        prepost_data = (ta3p100.ContactPrePost2 & (ta3p100.SpineClusters & \"cluster_id=0\")).proj(\"postsyn\",\"total_contact_conversion\",\n",
    "                \"total_contact_density\",\"total_synapse_sizes_mean\",\n",
    "                syn_density=\"(total_n_synapses*total_synapse_sizes_mean)/total_postsyn_length\",\n",
    "                presyn=\"segment_id\").fetch()\n",
    "        df = pd.DataFrame(prepost_data)\n",
    "\n",
    "        #gets all the combinations of postsyn-postsyn without any repeats\n",
    "        targets = (dj.U(\"postsyn\") & ta3p100.Contact2).proj(segment_id=\"postsyn\") - ta3p100.SegmentExclude\n",
    "        info = targets * targets.proj(segment_b='segment_id') & 'segment_id < segment_b'\n",
    "        segment_pairs = info.fetch()\n",
    "        \n",
    "        total_correlations = []\n",
    "\n",
    "        for postsyn1,postsyn2 in tqdm(segment_pairs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            #print(\"postsyn1 = \" + str(postsyn1))\n",
    "            #print(\"postsyn2 = \" + str(postsyn2))\n",
    "\n",
    "            #get all of the rows with postsyn 1 and 2 AKA find the number of presyns for each\n",
    "            df_1 = df[df[\"postsyn\"].to_numpy()==postsyn1]\n",
    "            df_2 = df[df[\"postsyn\"].to_numpy()==postsyn2]\n",
    "\n",
    "            #reduce both tables down to common presyns\n",
    "            df_1_common = df_1[df_1[\"presyn\"].isin(df_2[\"presyn\"].to_numpy())].sort_values(by=['presyn'])\n",
    "            df_2_common = df_2[df_2[\"presyn\"].isin(df_1[\"presyn\"].to_numpy())].sort_values(by=['presyn'])\n",
    "\n",
    "\n",
    "            ###########------------------------------------------------------###########\n",
    "            #need to get the common axons that have at least one converted contact on one of the postsyns\n",
    "            \"\"\"pseudocode\n",
    "            #get the conversion rates for both tables\n",
    "            #add them up\n",
    "            #get the indices that are greater than 0\n",
    "            #get the presyn ids that match those rows\n",
    "            #further restrict both groups by those ids\n",
    "            \"\"\"\n",
    "\n",
    "            #get both of their conversion rates\n",
    "            test_1_conv = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "            test_2_conv = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "\n",
    "            test_1_presyn = df_1_common[\"presyn\"].to_numpy()\n",
    "            new_presyns = test_1_presyn[(test_1_conv + test_2_conv) > 0]\n",
    "\n",
    "            df_1_common_converted = df_1_common[df_1_common[\"presyn\"].isin(new_presyns)]\n",
    "            df_2_common_converted = df_2_common[df_2_common[\"presyn\"].isin(new_presyns)]\n",
    "            ###########------------------------------------------------------###########\n",
    "\n",
    "\n",
    "            #finds the number of segments, shared_segments and union segments\n",
    "            n_seg_a = df_1.shape[0]\n",
    "            n_seg_b = df_2.shape[0]\n",
    "            n_seg_shared = df_1_common.shape[0]\n",
    "            n_seg_shared_converted = df_1_common_converted.shape[0]\n",
    "            n_seg_union = n_seg_a + n_seg_b - n_seg_shared\n",
    "            \n",
    "            \n",
    "            #get the number and proportion on presyns that convert onto each segment inside the converted axon group\n",
    "            if n_seg_shared_converted > 0:\n",
    "                test_1_conv[test_1_conv>1] = 1\n",
    "                n_seg_a_converted = sum(np.ceil(test_1_conv))\n",
    "                test_2_conv[test_2_conv>1] = 1\n",
    "                n_seg_b_converted = sum(np.ceil(test_2_conv))\n",
    "\n",
    "                n_seg_a_converted_prop = n_seg_a_converted/n_seg_shared_converted\n",
    "                n_seg_b_converted_prop = n_seg_b_converted/n_seg_shared_converted\n",
    "            else:\n",
    "                n_seg_a_converted = 0\n",
    "                n_seg_b_converted = 0\n",
    "                n_seg_a_converted_prop = np.NaN\n",
    "                n_seg_b_converted_prop = np.NaN\n",
    "     \n",
    "            dict_segmenation=2\n",
    "            dict_segment_id=postsyn1\n",
    "            dict_segment_b=postsyn2\n",
    "            #initialize the dictionary that will be saved:\n",
    "            corr_dict = dict(segmentation=2,segment_id=postsyn1,\n",
    "                                          segment_b=postsyn2,\n",
    "                                          n_seg_a=n_seg_a,\n",
    "                                            n_seg_b=n_seg_b,\n",
    "                                            n_seg_shared=n_seg_shared,\n",
    "                                            n_seg_shared_converted=n_seg_shared_converted,\n",
    "                                            n_seg_union=n_seg_union,\n",
    "                                            n_seg_a_converted=n_seg_a_converted,\n",
    "                                            n_seg_b_converted=n_seg_b_converted,\n",
    "                                            n_seg_a_converted_prop=n_seg_a_converted_prop,\n",
    "                                            n_seg_b_converted_prop=n_seg_b_converted_prop)\n",
    "\n",
    "\n",
    "            #initialize the variables that need to be set in the dictionary\n",
    "\n",
    "\n",
    "            #ones that are set by 1st group\n",
    "            binary_conversion_pearson = np.NaN\n",
    "            binary_conversion_cosine = np.NaN\n",
    "            binary_conv_jaccard_ones_ratio = np.NaN\n",
    "            binary_conv_jaccard_matching_ratio = np.NaN\n",
    "            conversion_pearson = np.NaN\n",
    "            conversion_cosine = np.NaN\n",
    "            density_pearson = np.NaN\n",
    "            density_cosine = np.NaN\n",
    "            synapse_volume_mean_pearson = np.NaN\n",
    "            synapse_volume_mean_cosine = np.NaN\n",
    "            synapse_vol_density_pearson = np.NaN\n",
    "            synapse_vol_density_cosine = np.NaN\n",
    "\n",
    "            #ones that are set by 2nd group\n",
    "            binary_conversion_pearson_converted = np.NaN\n",
    "            binary_conversion_cosine_converted = np.NaN\n",
    "            binary_conv_jaccard_ones_ratio_converted = np.NaN\n",
    "            binary_conv_jaccard_matching_ratio_converted = np.NaN\n",
    "            conversion_pearson_converted = np.NaN\n",
    "            conversion_cosine_converted = np.NaN\n",
    "            density_pearson_converted = np.NaN\n",
    "            density_cosine_converted = np.NaN\n",
    "            synapse_volume_mean_pearson_converted = np.NaN\n",
    "            synapse_volume_mean_cosine_converted = np.NaN\n",
    "            synapse_vol_density_pearson_converted = np.NaN\n",
    "            synapse_vol_density_cosine_converted = np.NaN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (not df_1_common.to_numpy().any()) or (not df_2_common.to_numpy().any()):\n",
    "                #total_correlations.append(corr_dict)\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                #retrieve the conversion rates\n",
    "                df_1_common_conversion = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "                df_2_common_conversion = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "                \n",
    "                #calculate the binary conversion rates\n",
    "                df_1_common_binary_conversion = np.copy(df_1_common_conversion)\n",
    "                df_2_common_binary_conversion = np.copy(df_2_common_conversion)\n",
    "\n",
    "\n",
    "                df_1_common_binary_conversion[df_1_common_binary_conversion>0] = 1.0\n",
    "                df_2_common_binary_conversion[df_2_common_binary_conversion>0] = 1.0\n",
    "                \n",
    "                #retrieve the synapse/postsyn_len\n",
    "                df_1_common_density = df_1_common[\"total_contact_density\"].to_numpy()\n",
    "                df_2_common_density = df_2_common[\"total_contact_density\"].to_numpy()\n",
    "\n",
    "                #retrieve mean of synapse_size\n",
    "                df_1_common_synaptic_size = df_1_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "                df_2_common_synaptic_size = df_2_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "\n",
    "                #retrieve (total_n_synapses*total_synapse_sizes_mean)/total_postsyn_length\n",
    "                df_1_common_syn_density = df_1_common[\"syn_density\"].to_numpy()\n",
    "                df_2_common_syn_density = df_2_common[\"syn_density\"].to_numpy()\n",
    "\n",
    "\n",
    "                binary_conversion_pearson = find_pearson(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                binary_conversion_cosine = find_cosine(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                \n",
    "                #new added metric for the binary calculations based on jacard_similarity\n",
    "                binary_conv_jaccard_ones_ratio,binary_conv_jaccard_matching_ratio = find_binary_sim(df_1_common_binary_conversion,df_2_common_binary_conversion)\n",
    "                \n",
    "                conversion_pearson = find_pearson(df_1_common_conversion, df_2_common_conversion)\n",
    "                conversion_cosine = find_cosine(df_1_common_conversion, df_2_common_conversion)\n",
    "                density_pearson = find_pearson(df_1_common_density, df_2_common_density)\n",
    "                density_cosine = find_cosine(df_1_common_density, df_2_common_density)\n",
    "                synapse_volume_mean_pearson = find_pearson(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                synapse_volume_mean_cosine = find_cosine(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                synapse_vol_density_pearson = find_pearson(df_1_common_syn_density, df_2_common_syn_density)\n",
    "                synapse_vol_density_cosine = find_cosine(df_1_common_syn_density, df_2_common_syn_density)\n",
    "\n",
    "                \n",
    "                ####reset the df_1_common and df_1_common to reuse code\n",
    "                df_1_common = df_1_common_converted\n",
    "                df_2_common = df_2_common_converted\n",
    "\n",
    "                if (not df_1_common.to_numpy().any()) or (not df_2_common.to_numpy().any()):\n",
    "                    #print(\"none_in_converted\")\n",
    "                    pass\n",
    "                else:\n",
    "                    df_1_common_conversion = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "                    df_2_common_conversion = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "\n",
    "                    df_1_common_binary_conversion = np.copy(df_1_common_conversion)\n",
    "                    df_2_common_binary_conversion = np.copy(df_2_common_conversion)\n",
    "\n",
    "\n",
    "                    df_1_common_binary_conversion[df_1_common_binary_conversion>0] = 1.0\n",
    "                    df_2_common_binary_conversion[df_2_common_binary_conversion>0] = 1.0\n",
    "\n",
    "                    df_1_common_density = df_1_common[\"total_contact_density\"].to_numpy()\n",
    "                    df_2_common_density = df_2_common[\"total_contact_density\"].to_numpy()\n",
    "\n",
    "\n",
    "                    df_1_common_synaptic_size = df_1_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "                    df_2_common_synaptic_size = df_2_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "\n",
    "                    df_1_common_syn_density = df_1_common[\"syn_density\"].to_numpy()\n",
    "                    df_2_common_syn_density = df_2_common[\"syn_density\"].to_numpy()\n",
    "\n",
    "                    \n",
    "\n",
    "                    binary_conversion_pearson_converted = find_pearson(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                    binary_conversion_cosine_converted = find_cosine(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                    \n",
    "                    #new added metric for the binary calculations based on jacard_similarity\n",
    "                    binary_conv_jaccard_ones_ratio_converted,binary_conv_jaccard_matching_ratio_converted = find_binary_sim(df_1_common_binary_conversion,df_2_common_binary_conversion)\n",
    "                \n",
    "                    conversion_pearson_converted = find_pearson(df_1_common_conversion, df_2_common_conversion)\n",
    "                    conversion_cosine_converted = find_cosine(df_1_common_conversion, df_2_common_conversion)\n",
    "                    density_pearson_converted = find_pearson(df_1_common_density, df_2_common_density)\n",
    "                    density_cosine_converted = find_cosine(df_1_common_density, df_2_common_density)\n",
    "                    synapse_volume_mean_pearson_converted = find_pearson(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                    synapse_volume_mean_cosine_converted = find_cosine(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                    synapse_vol_density_pearson_converted = find_pearson(df_1_common_syn_density, df_2_common_syn_density)\n",
    "                    synapse_vol_density_cosine_converted = find_cosine(df_1_common_syn_density, df_2_common_syn_density)\n",
    "\n",
    "\n",
    "\n",
    "            corr_dict[\"binary_conversion_pearson\"] = binary_conversion_pearson\n",
    "            corr_dict[\"binary_conversion_cosine\"] = binary_conversion_cosine\n",
    "            corr_dict[\"binary_conv_jaccard_ones_ratio\"] = binary_conv_jaccard_ones_ratio\n",
    "            corr_dict[\"binary_conv_jaccard_matching_ratio\"] = binary_conv_jaccard_matching_ratio\n",
    "            corr_dict[\"conversion_pearson\"] = conversion_pearson\n",
    "            corr_dict[\"conversion_cosine\"] = conversion_cosine\n",
    "            corr_dict[\"density_pearson\"] = density_pearson\n",
    "            corr_dict[\"density_cosine\"] = density_cosine\n",
    "            corr_dict[\"synapse_volume_mean_pearson\"] = synapse_volume_mean_pearson\n",
    "            corr_dict[\"synapse_volume_mean_cosine\"] = synapse_volume_mean_cosine\n",
    "            corr_dict[\"synapse_vol_density_pearson\"] = synapse_vol_density_pearson\n",
    "            corr_dict[\"synapse_vol_density_cosine\"] = synapse_vol_density_cosine\n",
    "\n",
    "            corr_dict[\"binary_conversion_pearson_converted\"] = binary_conversion_pearson_converted\n",
    "            corr_dict[\"binary_conversion_cosine_converted\"] = binary_conversion_cosine_converted\n",
    "            corr_dict[\"binary_conv_jaccard_ones_ratio_converted\"] = binary_conv_jaccard_ones_ratio_converted\n",
    "            corr_dict[\"binary_conv_jaccard_matching_ratio_converted\"] = binary_conv_jaccard_matching_ratio_converted\n",
    "            corr_dict[\"conversion_pearson_converted\"] = conversion_pearson_converted\n",
    "            corr_dict[\"conversion_cosine_converted\"] = conversion_cosine_converted\n",
    "            corr_dict[\"density_pearson_converted\"] = density_pearson_converted\n",
    "            corr_dict[\"density_cosine_converted\"] = density_cosine_converted\n",
    "            corr_dict[\"synapse_volume_mean_pearson_converted\"] = synapse_volume_mean_pearson_converted\n",
    "            corr_dict[\"synapse_volume_mean_cosine_converted\"] = synapse_volume_mean_cosine_converted\n",
    "            corr_dict[\"synapse_vol_density_pearson_converted\"] = synapse_vol_density_pearson_converted\n",
    "            corr_dict[\"synapse_vol_density_cosine_converted\"] = synapse_vol_density_cosine_converted\n",
    "\n",
    "\n",
    "\n",
    "            total_correlations.append(corr_dict)\n",
    "\n",
    "\n",
    "        #write all of the dictionaries to the database\n",
    "        self.insert(total_correlations,skip_duplicates=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337431/337431 [55:03<00:00, 102.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3612.0999302864075\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ContactCorrelationSoma2.populate()\n",
    "print(f\"Total time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

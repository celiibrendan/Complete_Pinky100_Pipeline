{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWill generate the skeletons for all of the\\nNeurites\\n\\nProcess: \\n1) Pull from ta3p100.Mesh\\n-- DO NOT NEED TO FILTER B/C DON'T HAVE LABELS\\n2) Voxelize the mesh \\n3) Export the mesh to an off file\\n4) Clean up the mesh using:\\na. smoothing normals\\nb. Poisson surface reconstruction\\nc. Extra remove duplicate filters\\n5) Send through skeletonization\\n6) Reading in skeleton segments\\n7) Write key to database\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Will generate the skeletons for all of the\n",
    "Neurites\n",
    "\n",
    "Process: \n",
    "1) Pull from ta3p100.Mesh\n",
    "-- DO NOT NEED TO FILTER B/C DON'T HAVE LABELS\n",
    "2) Voxelize the mesh \n",
    "3) Export the mesh to an off file\n",
    "4) Clean up the mesh using:\n",
    "a. smoothing normals\n",
    "b. Poisson surface reconstruction\n",
    "c. Extra remove duplicate filters\n",
    "5) Send through skeletonization\n",
    "6) Reading in skeleton segments\n",
    "7) Write key to database\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import time\n",
    "import pymeshfix\n",
    "import os\n",
    "import datetime\n",
    "import calcification_Module as cm\n",
    "from meshparty import trimesh_io\n",
    "\n",
    "#for supressing the output\n",
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "#setting the address and the username\n",
    "dj.config['database.host'] = '10.28.0.34'\n",
    "dj.config['database.user'] = 'celiib'\n",
    "dj.config['database.password'] = 'newceliipass'\n",
    "dj.config['safemode']=True\n",
    "dj.config[\"display.limit\"] = 20\n",
    "\n",
    "schema = dj.schema('microns_ta3p100')\n",
    "ta3p100 = dj.create_virtual_module('ta3p100', 'microns_ta3p100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the output file\n",
    "##write the OFF file for the neuron\n",
    "import pathlib\n",
    "def write_Whole_Neuron_Off_file(neuron_ID,vertices=[], triangles=[]):\n",
    "    #primary_key = dict(segmentation=1, segment_id=segment_id, decimation_ratio=0.35)\n",
    "    #vertices, triangles = (mesh_Table_35 & primary_key).fetch1('vertices', 'triangles')\n",
    "    \n",
    "    num_vertices = (len(vertices))\n",
    "    num_faces = len(triangles)\n",
    "    \n",
    "    #get the current file location\n",
    "    file_loc = pathlib.Path.cwd() / \"temp_meshlab\"\n",
    "    filename = \"neuron_\" + str(neuron_ID)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #print(file_loc)\n",
    "    #print(path_and_filename)\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(num_vertices) + \" \" + str(num_faces) + \" 0\\n\" )\n",
    "    \n",
    "    \n",
    "    #iterate through and write all of the vertices in the file\n",
    "    for verts in vertices:\n",
    "        f.write(str(verts[0]) + \" \" + str(verts[1]) + \" \" + str(verts[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing verts\")\n",
    "        \n",
    "    for faces in triangles:\n",
    "        f.write(\"3 \" + str(faces[0]) + \" \" + str(faces[1]) + \" \" + str(faces[2])+\"\\n\")\n",
    "    \n",
    "    print(\"Done writing OFF file\")\n",
    "    #f.write(\"end\")\n",
    "    \n",
    "    return str(path_and_filename),str(filename),str(file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output for the skeleton edges to be stored by datajoint\n",
    "\"\"\" OLD WAY THAT DATAJOINT WAS GETTING MAD AT \n",
    "def read_skeleton(file_path):\n",
    "    with open(file_path) as f:\n",
    "        bones = list()\n",
    "        for line in f.readlines():\n",
    "            bones.append(np.array(line.split()[1:], float).reshape(-1, 3))\n",
    "    return np.array(bones)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" NEW FLAT LIST WAY\"\"\"\n",
    "#practice reading in dummy skeleton file\n",
    "def read_skeleton_flat(file_path):\n",
    "    with open(file_path) as f:\n",
    "        bones = list()\n",
    "        for line in f.readlines():\n",
    "            for r in (np.array(line.split()[1:], float).reshape(-1, 3)):\n",
    "                bones.append(r)\n",
    "            bones.append([np.nan,np.nan,np.nan])\n",
    "    return np.array(bones).astype(float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure there is a temp file in the directory, if not then make one\n",
    "#if temp folder doesn't exist then create it\n",
    "if (os.path.isdir(os.getcwd() + \"/temp_meshlab\")) == False:\n",
    "    os.mkdir(\"temp_meshlab\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_meshlab_script(mlx_script,input_mesh_file,output_mesh_file):\n",
    "    script_command = (\" -i \" + str(input_mesh_file) + \" -o \" + \n",
    "                                    str(output_mesh_file) + \" -s \" + str(mlx_script))\n",
    "    #return script_command\n",
    "    subprocess_result = subprocess.run('xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@ ' + \n",
    "                   script_command,shell=True)\n",
    "    \n",
    "    return subprocess_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get keysource:\n",
    "ta3p100.Mesh() & ta3p100.Neurite() #32207 tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meshlab_shrinkwrap(key):\n",
    "    \n",
    "    file_loc = pathlib.Path.cwd() / \"temp_meshlab\"\n",
    "    filename = \"neuron_\" + str(key[\"segment_id\"])\n",
    "    path_and_filename = str(file_loc / filename)\n",
    "    \n",
    "    \n",
    "    input_mesh = path_and_filename + \".off\"\n",
    "    midoutput_mesh = path_and_filename + \"_mid.off\"\n",
    "    output_mesh = path_and_filename+\"_mls.off\"\n",
    "    \n",
    "    \n",
    "    meshlab_script = str(pathlib.Path.cwd()) + \"/\" + \"remeshing_script.mlx\"\n",
    "    meshlab_script_rem_dupl = str(pathlib.Path.cwd()) + \"/\" + \"remeshing_script-Remove.mlx\"\n",
    "    #send to meshlabserver\n",
    "    print(\"starting meshlabserver Poisson surface reconstruction\")\n",
    "    subprocess_result_1 = run_meshlab_script(meshlab_script,\n",
    "                      input_mesh,\n",
    "                      midoutput_mesh)\n",
    "    #print(\"Poisson subprocess_result= \"+ str(subprocess_result_1))\n",
    "    \n",
    "    if str(subprocess_result_1)[-13:] != \"returncode=0)\":\n",
    "        raise Exception('neuron' + str(key[\"segment_id\"]) + \n",
    "                         ' did not get pass Poisson')\n",
    "    #print(type)\n",
    "     \n",
    "    \n",
    "    #do another call to remove the final duplicate vertices\n",
    "    print(\"starting meshlabserver cleaning\")\n",
    "    subprocess_result_2 = run_meshlab_script(meshlab_script_rem_dupl,\n",
    "                      midoutput_mesh,\n",
    "                      output_mesh)\n",
    "    #print(\"Cleaning subprocess_result= \"+ str(subprocess_result_2))\n",
    "    \n",
    "    if str(subprocess_result_2)[-13:] != \"returncode=0)\":\n",
    "        raise Exception('neuron' + str(key[\"segment_id\"]) + \n",
    "                         ' did not get pass cleaning')\n",
    "    \n",
    "    \n",
    "    return output_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't need the vertices for this one because not doing any filtering\n",
    "\n",
    "@schema\n",
    "class NeuriteSkeletons(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> ta3p100.Mesh\n",
    "    time_updated      :timestamp    # the time at which the skeleton was generated\n",
    "    ---\n",
    "    n_branches   :int unsigned #number of edges stored\n",
    "    branches     :longblob #array storing vertices of edges and each seperated by Nan\n",
    "    \"\"\"\n",
    "    \n",
    "    key_source = ta3p100.Mesh() & ta3p100.Neurite() \n",
    "    \n",
    "    #how you get the date and time  datetime.datetime.now()\n",
    "    \n",
    "    def make(self, key):\n",
    "        print(\"Starting on \"+str(key[\"segment_id\"]))\n",
    "        global_time = time.time()\n",
    "        #get the mesh with the error segments filtered away\n",
    "        start_time = time.time()\n",
    "        \n",
    "        file_loc = pathlib.Path.cwd() / \"temp_meshlab\"\n",
    "        filename = \"neuron_\" + str(key[\"segment_id\"])\n",
    "        path_and_filename = str(file_loc / filename)\n",
    "        \n",
    "        #get the mesh of the neurite\n",
    "        new_key = dict(segmentation=key[\"segmentation\"],\n",
    "                       segment_id=key[\"segment_id\"])\n",
    "        mesh = (ta3p100.Mesh & new_key).fetch1()\n",
    "        print(f\"Step 1: Retrieving Mesh: {time.time() - start_time}\")\n",
    "        \n",
    "        #Do voxelization\n",
    "        start_time = time.time()\n",
    "        mesh = trimesh_io.Mesh(vertices=mesh[\"vertices\"], faces=mesh[\"triangles\"])\n",
    "        voxels = mesh.voxelized(500)\n",
    "        voxel_mesh = voxels.as_boxes()\n",
    "        print(f\"Step 2: Voxelization: {time.time() - start_time}\")\n",
    "        \n",
    "        #Exporting the Voxelization as an off file for meshlabserver\n",
    "        start_time = time.time()\n",
    "        #try the inline printing method:\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            with contextlib.redirect_stdout(devnull):\n",
    "                voxel_mesh.export(str(path_and_filename) + \".off\")\n",
    "        print(f\"Step 3: Exporting Voxel Off function: {time.time() - start_time}\")\n",
    "        \n",
    "        #Run the meshlabserver scripts\n",
    "        start_time = time.time()\n",
    "        output_mesh = meshlab_shrinkwrap(key)\n",
    "        print(f\"Step 4: Meshlab shrinkwrapping: {time.time() - start_time}\")\n",
    "        \n",
    "        #Create the skeleton and retrieve it from the generated file\n",
    "        #skeletonize the mesh\n",
    "        start_time = time.time()\n",
    "        #print(\"starting creating skeleton\")\n",
    "        return_value = cm.calcification(output_mesh[:-4])\n",
    "        #print(\"calcif_return_value = \" + str(return_value))\n",
    "        \n",
    "        if return_value > 0:\n",
    "            raise Exception('skeletonization for neuron ' + str(new_key[\"segment_id\"]) + \n",
    "                            ' did not finish... exited with error code: ' + str(return_value))\n",
    "        print(f\"Step 4: Generating Skeleton: {time.time() - start_time}\")\n",
    "        \n",
    "        #read in the skeleton files into an array\n",
    "        start_time = time.time()\n",
    "        bone_array = read_skeleton_flat(output_mesh[:-4]+\"_skeleton.cgal\")\n",
    "        #print(bone_array)\n",
    "        if len(bone_array) <= 0:\n",
    "            raise Exception('No skeleton generated for ' + str(new_key[\"segment_id\"]))\n",
    "        print(f\"Step 5: Reading in Skeleton: {time.time() - start_time}\")\n",
    "        \n",
    "              \n",
    "        start_time = time.time()\n",
    "        new_key[\"n_branches\"] = len(bone_array)\n",
    "        new_key[\"branches\"] = bone_array\n",
    "        #new_key[\"branches\"] = []\n",
    "        \n",
    "        \n",
    "        new_key[\"time_updated\"]=str(datetime.datetime.now())\n",
    "        #print(new_key)\n",
    "        #if all goes well then write to database\n",
    "        self.insert1(new_key)\n",
    "        os.system(\"rm \"+str(path_and_filename)+\"*\")\n",
    "        print(f\"Step 6: Inserting dictionary and erased files: {time.time() - start_time}\")\n",
    "        print(f\"Total time: {time.time() - global_time}\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "NeuriteSkeletons.populate(reserve_jobs=True)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that will filter out error triangles\n",
    "def generate_neighborhood(triangles, num_vertices):\n",
    "    neighborhood = dict()\n",
    "    for i in range(num_vertices):\n",
    "        neighborhood[i] = set()\n",
    "    for node1, node2, node3 in triangles:\n",
    "        neighborhood[node1].update([node2, node3])\n",
    "        neighborhood[node2].update([node1, node3])\n",
    "        neighborhood[node3].update([node1, node2])\n",
    "    return neighborhood\n",
    "\n",
    "def set_search_first(starting_node, neighborhood):\n",
    "    \"\"\"\n",
    "    Modified Depth-First-Search utilizing sets to reduce duplicate checks:\n",
    "\n",
    "    Neighborhood must be a dict with the keys being the vertex indices!\n",
    "    \"\"\"    \n",
    "    visited_nodes = set()\n",
    "    temp_stack = set()\n",
    "    temp_stack.add(starting_node)\n",
    "    while len(temp_stack) > 0:\n",
    "        starting_node = temp_stack.pop()\n",
    "        if starting_node not in visited_nodes:\n",
    "            visited_nodes.add(starting_node)\n",
    "            temp_stack.update(neighborhood[starting_node])\n",
    "    return list(visited_nodes)\n",
    "def get_connected_portions(neighborhood):\n",
    "    neighborhood_copy = neighborhood.copy()\n",
    "    portions = []\n",
    "    while len(neighborhood_copy) > 0:\n",
    "        starting_node = next(iter(neighborhood_copy))\n",
    "        portion = set_search_first(starting_node, neighborhood_copy)\n",
    "        for node in portion:\n",
    "            neighborhood_copy.pop(node)\n",
    "        portions.append(portion)\n",
    "    return portions\n",
    "\n",
    "def get_largest_portion_index(portions):\n",
    "    portion_lengths = [len(portion) for portion in portions]\n",
    "    return portion_lengths.index(max(portion_lengths))\n",
    "\n",
    "def get_largest_portion(portions):\n",
    "    return portions[get_largest_portion_index(portions)]\n",
    "\n",
    "def remove_floating_artifacts(mesh,key):    \n",
    "    mesh_copy = mesh.copy()\n",
    "    \n",
    "    #get the labels for the mesh\n",
    "    #find out if in Orphan Table or Regular Neuron Table\n",
    "    if len(ta3p100.CoarseLabelFinal() & key) > 0:\n",
    "        mesh_labels = (ta3p100.CoarseLabelFinal & key).fetch1()\n",
    "    elif len(ta3p100.CoarseLabelOrphan() & key) > 0:\n",
    "        mesh_labels = (ta3p100.CoarseLabelOrphan & key).fetch1()\n",
    "    else:\n",
    "        raise Exception('neuron' + str(key[\"segment_id\"]) + \n",
    "                        'not present in any labels!')\n",
    "\n",
    "    \n",
    "    #look for errors\n",
    "    not_errors = [i for i,k in enumerate(mesh_labels[\"triangles\"]) if k != 10]\n",
    "    original_triangles = mesh[\"triangles\"]\n",
    "    \"\"\"\n",
    "    print(type(not_errors))\n",
    "    print(len(not_errors))\n",
    "    print(\"not_errors = \"+ str(not_errors[:100]))\n",
    "    print(type(original_triangles))\n",
    "    print(len(original_triangles))\n",
    "    #print(original_triangles)\n",
    "    print(\"not_errors = \" + str(original_triangles[not_errors]))\n",
    "    \"\"\"\n",
    "    \n",
    "    mesh_copy['triangles'] = np.array(original_triangles[not_errors])\n",
    "    \n",
    "    return mesh_copy\n",
    "\n",
    "\n",
    "def remove_isolated_vertices(mesh):\n",
    "    mesh_copy = mesh.copy()\n",
    "\n",
    "    neighborhood = generate_neighborhood(mesh_copy['triangles'], len(mesh_copy['vertices']))\n",
    "    isolated_nodes = [portion.pop() for portion in get_connected_portions(neighborhood) if len(portion) == 1]\n",
    "\n",
    "    vertices = mesh_copy['vertices']\n",
    "    triangles = mesh_copy['triangles']\n",
    "    vertex_list = list(vertices)\n",
    "\n",
    "    if len(isolated_nodes) > 0:\n",
    "        num_isolated_nodes_passed = 0\n",
    "        isolated_nodes_set = set(isolated_nodes)\n",
    "        count_to_decrement = np.zeros(len(vertices))\n",
    "        for i in range(len(vertices)):\n",
    "            if i in isolated_nodes_set:\n",
    "                num_isolated_nodes_passed += 1\n",
    "            else:\n",
    "                count_to_decrement[i] = num_isolated_nodes_passed\n",
    "\n",
    "        for i, triangle in enumerate(triangles):\n",
    "            start = time.time()\n",
    "            node1, node2, node3 = triangle\n",
    "            triangles[i][0] -= count_to_decrement[node1]\n",
    "            triangles[i][1] -= count_to_decrement[node2]\n",
    "            triangles[i][2] -= count_to_decrement[node3]\n",
    "        for i, isolated_node in enumerate(isolated_nodes):\n",
    "            vertex_list.pop(isolated_node - i)\n",
    "\n",
    "    mesh_copy['vertices'] = np.array(vertex_list)\n",
    "\n",
    "    return mesh_copy\n",
    "\n",
    "\n",
    "def remove_error_segments(key):\n",
    "    \n",
    "    \n",
    "    full_start = time.time()\n",
    "\n",
    "    print(str(key['segment_id']) +  \":\")\n",
    "    start = time.time()\n",
    "\n",
    "    #find out if in Orphan Table or Regular Neuron Table\n",
    "    if len(ta3p100.CoarseLabelFinal() & key) > 0:\n",
    "        mesh = (ta3p100.CleansedMesh & key).fetch1()\n",
    "    elif len(ta3p100.CoarseLabelOrphan() & key) > 0:\n",
    "        mesh = (ta3p100.CleansedMeshOrphan & key).fetch1()\n",
    "    else:\n",
    "        raise Exception('neuron' + str(key[\"segment_id\"]) + \n",
    "                        'not present in any labels!')\n",
    "    \n",
    "    print(key['segment_id'], \"mesh fetched.\", time.time() - start)\n",
    "    start = time.time()\n",
    "    \n",
    "    neighborhood = generate_neighborhood(mesh['triangles'], len(mesh['vertices']))\n",
    "    print(key['segment_id'] , \"neighborhood generated.\", time.time() - start)\n",
    "    start = time.time()\n",
    "    \n",
    "    mesh = remove_floating_artifacts(mesh,key)\n",
    "    print(key['segment_id'], \"floating artifacts removed.\", time.time() - start)\n",
    "    start = time.time()\n",
    "\n",
    "    mesh = remove_isolated_vertices(mesh)\n",
    "    print(key['segment_id'], \"isolated nodes removed.\", time.time() - start)\n",
    "    start = time.time()\n",
    "\n",
    "    key['n_vertices'] = len(mesh['vertices'])\n",
    "    key['n_triangles'] = len(mesh['triangles'])\n",
    "    key['vertices'] = mesh['vertices']\n",
    "    key['triangles'] = mesh['triangles']\n",
    "\n",
    "    #self.insert1(key, skip_duplicates=True)\n",
    "    print(key['segment_id'], \"key successfully filtered.\", time.time() - start)\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"This took \", time.time() - full_start, \"seconds.\")\n",
    "    print()\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_skeleton(key):\n",
    "    print(\"Starting on \"+str(key[\"segment_id\"]))\n",
    "    \n",
    "\n",
    "    file_loc = pathlib.Path.cwd() / \"temp_meshlab\"\n",
    "    filename = \"neuron_\" + str(key[\"segment_id\"])\n",
    "    path_and_filename = str(file_loc / filename)\n",
    "\n",
    "    \"\"\"\n",
    "    #get the mesh of the neurite\n",
    "    new_key = dict(segmentation=key[\"segmentation\"],\n",
    "                   segment_id=key[\"segment_id\"])\n",
    "    mesh = (ta3p100.Mesh & new_key).fetch1()\n",
    "    print(f\"Step 1: Retrieving Mesh: {time.time() - start_time}\")\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    global_time = time.time()\n",
    "    #get the mesh with the error segments filtered away\n",
    "    start_time = time.time()\n",
    "    new_key = remove_error_segments(key)\n",
    "    print(f\"Step 1: Removing error segments: {time.time() - start_time}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    #Do voxelization\n",
    "    start_time = time.time()\n",
    "    mesh = trimesh_io.Mesh(vertices=new_key[\"vertices\"], faces=new_key[\"triangles\"])\n",
    "    \n",
    "    #Exporting the Voxelization as an off file for meshlabserver\n",
    "    start_time = time.time()\n",
    "    #try the inline printing method:\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with contextlib.redirect_stdout(devnull):\n",
    "            mesh.export(str(path_and_filename) + \"_original.off\")\n",
    "    print(f\"Step 2a: Exporting Mesh before Voxel function: {time.time() - start_time}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    voxels = mesh.voxelized(100)\n",
    "    voxel_mesh = voxels.as_boxes()\n",
    "    print(f\"Step 2: Voxelization: {time.time() - start_time}\")\n",
    "\n",
    "    #Exporting the Voxelization as an off file for meshlabserver\n",
    "    start_time = time.time()\n",
    "    #try the inline printing method:\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with contextlib.redirect_stdout(devnull):\n",
    "            voxel_mesh.export(str(path_and_filename) + \"voxel_.off\")\n",
    "    print(f\"Step 3: Exporting Voxel Off function: {time.time() - start_time}\")\n",
    "\n",
    "    \n",
    "    #Run the meshlabserver scripts\n",
    "    start_time = time.time()\n",
    "    output_mesh = meshlab_shrinkwrap(key)\n",
    "    print(f\"Step 4: Meshlab shrinkwrapping: {time.time() - start_time}\")\n",
    "\n",
    "    #Create the skeleton and retrieve it from the generated file\n",
    "    #skeletonize the mesh\n",
    "    start_time = time.time()\n",
    "    #print(\"starting creating skeleton\")\n",
    "    return_value = cm.calcification(output_mesh[:-4])\n",
    "    #print(\"calcif_return_value = \" + str(return_value))\n",
    "\n",
    "    if return_value > 0:\n",
    "        raise Exception('skeletonization for neuron ' + str(new_key[\"segment_id\"]) + \n",
    "                        ' did not finish... exited with error code: ' + str(return_value))\n",
    "    print(f\"Step 4: Generating Skeleton: {time.time() - start_time}\")\n",
    "\n",
    "    #read in the skeleton files into an array\n",
    "    start_time = time.time()\n",
    "    bone_array = read_skeleton_flat(output_mesh[:-4]+\"_skeleton.cgal\")\n",
    "    #print(bone_array)\n",
    "    if len(bone_array) <= 0:\n",
    "        raise Exception('No skeleton generated for ' + str(new_key[\"segment_id\"]))\n",
    "    print(f\"Step 5: Reading in Skeleton: {time.time() - start_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_key = dict(segmentation=2,segment_id=648518346349483124)\n",
    "make_skeleton(new_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing Voxel --> meshlabserver --> skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on 648518346349483124\n",
      "648518346349483124:\n",
      "648518346349483124 mesh fetched. 0.2661917209625244\n",
      "648518346349483124 neighborhood generated. 6.447141647338867\n",
      "648518346349483124 floating artifacts removed. 1.5665276050567627\n",
      "648518346349483124 isolated nodes removed. 9.217100858688354\n",
      "648518346349483124 key successfully filtered. 5.7220458984375e-06\n",
      "This took  17.498077154159546 seconds.\n",
      "\n",
      "Step 1: Removing error segments: 17.783402919769287\n",
      "Step 2a: Exporting Mesh before Voxel function: 2.4862053394317627\n",
      "Step 2: Voxelization: 3.32119083404541\n"
     ]
    }
   ],
   "source": [
    "key = dict(segmentation=2,segment_id=648518346349483124)\n",
    "print(\"Starting on \"+str(key[\"segment_id\"]))\n",
    "\n",
    "\n",
    "file_loc = pathlib.Path.cwd() / \"temp_meshlab\"\n",
    "filename = \"neuron_\" + str(key[\"segment_id\"])\n",
    "path_and_filename = str(file_loc / filename)\n",
    "\n",
    "\"\"\"\n",
    "#get the mesh of the neurite\n",
    "new_key = dict(segmentation=key[\"segmentation\"],\n",
    "               segment_id=key[\"segment_id\"])\n",
    "mesh = (ta3p100.Mesh & new_key).fetch1()\n",
    "print(f\"Step 1: Retrieving Mesh: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "global_time = time.time()\n",
    "#get the mesh with the error segments filtered away\n",
    "start_time = time.time()\n",
    "new_key = remove_error_segments(key)\n",
    "print(f\"Step 1: Removing error segments: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "\n",
    "#Do voxelization\n",
    "start_time = time.time()\n",
    "mesh = trimesh_io.Mesh(vertices=new_key[\"vertices\"], faces=new_key[\"triangles\"])\n",
    "\n",
    "#Exporting the Voxelization as an off file for meshlabserver\n",
    "start_time = time.time()\n",
    "#try the inline printing method:\n",
    "with open(os.devnull, 'w') as devnull:\n",
    "    with contextlib.redirect_stdout(devnull):\n",
    "        mesh.export(str(path_and_filename) + \"_original.off\")\n",
    "print(f\"Step 2a: Exporting Mesh before Voxel function: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "\n",
    "voxels = mesh.voxelized(1000)\n",
    "voxel_mesh = voxels.as_boxes()\n",
    "print(f\"Step 2: Voxelization: {time.time() - start_time}\")\n",
    "    \n",
    "#     #try exporting with own function\n",
    "#     start_time = time.time()\n",
    "#     #try the inline printing method:\n",
    "#     with open(os.devnull, 'w') as devnull:\n",
    "#         with contextlib.redirect_stdout(devnull):\n",
    "#             voxel_mesh.export(str(path_and_filename) + \"voxel_.off\")\n",
    "#     print(f\"Step 3: Exporting Voxel Off function: {time.time() - start_time}\")\n",
    "\n",
    "    \n",
    "\n",
    "#     #Exporting the Voxelization as an off file for meshlabserver\n",
    "#     start_time = time.time()\n",
    "#     #try the inline printing method:\n",
    "#     with open(os.devnull, 'w') as devnull:\n",
    "#         with contextlib.redirect_stdout(devnull):\n",
    "#             voxel_mesh.export(str(path_and_filename) + \"voxel_.off\")\n",
    "#     print(f\"Step 3: Exporting Voxel Off function: {time.time() - start_time}\")\n",
    "\n",
    "    \n",
    "#     #Run the meshlabserver scripts\n",
    "#     start_time = time.time()\n",
    "#     output_mesh = meshlab_shrinkwrap(key)\n",
    "#     print(f\"Step 4: Meshlab shrinkwrapping: {time.time() - start_time}\")\n",
    "\n",
    "#     #Create the skeleton and retrieve it from the generated file\n",
    "#     #skeletonize the mesh\n",
    "#     start_time = time.time()\n",
    "#     #print(\"starting creating skeleton\")\n",
    "#     return_value = cm.calcification(output_mesh[:-4])\n",
    "#     #print(\"calcif_return_value = \" + str(return_value))\n",
    "\n",
    "#     if return_value > 0:\n",
    "#         raise Exception('skeletonization for neuron ' + str(new_key[\"segment_id\"]) + \n",
    "#                         ' did not finish... exited with error code: ' + str(return_value))\n",
    "#     print(f\"Step 4: Generating Skeleton: {time.time() - start_time}\")\n",
    "\n",
    "#     #read in the skeleton files into an array\n",
    "#     start_time = time.time()\n",
    "#     bone_array = read_skeleton_flat(output_mesh[:-4]+\"_skeleton.cgal\")\n",
    "#     #print(bone_array)\n",
    "#     if len(bone_array) <= 0:\n",
    "#         raise Exception('No skeleton generated for ' + str(new_key[\"segment_id\"]))\n",
    "#     print(f\"Step 5: Reading in Skeleton: {time.time() - start_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_cache',\n",
       " '_center_mass',\n",
       " '_data',\n",
       " '_density',\n",
       " '_kwargs',\n",
       " '_validate',\n",
       " '_visual',\n",
       " 'apply_obb',\n",
       " 'apply_scale',\n",
       " 'apply_transform',\n",
       " 'apply_translation',\n",
       " 'area',\n",
       " 'area_faces',\n",
       " 'body_count',\n",
       " 'bounding_box',\n",
       " 'bounding_box_oriented',\n",
       " 'bounding_cylinder',\n",
       " 'bounding_primitive',\n",
       " 'bounding_sphere',\n",
       " 'bounds',\n",
       " 'center_mass',\n",
       " 'centroid',\n",
       " 'compute_stable_poses',\n",
       " 'contains',\n",
       " 'convert_units',\n",
       " 'convex_decomposition',\n",
       " 'convex_hull',\n",
       " 'copy',\n",
       " 'crc',\n",
       " 'density',\n",
       " 'difference',\n",
       " 'edges',\n",
       " 'edges_face',\n",
       " 'edges_sorted',\n",
       " 'edges_sparse',\n",
       " 'edges_unique',\n",
       " 'edges_unique_inverse',\n",
       " 'edges_unique_length',\n",
       " 'euler_number',\n",
       " 'eval_cached',\n",
       " 'export',\n",
       " 'extents',\n",
       " 'face_adjacency',\n",
       " 'face_adjacency_angles',\n",
       " 'face_adjacency_convex',\n",
       " 'face_adjacency_edges',\n",
       " 'face_adjacency_projections',\n",
       " 'face_adjacency_radius',\n",
       " 'face_adjacency_span',\n",
       " 'face_adjacency_tree',\n",
       " 'face_adjacency_unshared',\n",
       " 'face_angles',\n",
       " 'face_angles_sparse',\n",
       " 'face_normals',\n",
       " 'faces',\n",
       " 'faces_sparse',\n",
       " 'faces_unique_edges',\n",
       " 'facets',\n",
       " 'facets_area',\n",
       " 'facets_boundary',\n",
       " 'facets_normal',\n",
       " 'facets_on_hull',\n",
       " 'facets_origin',\n",
       " 'fill_holes',\n",
       " 'fix_normals',\n",
       " 'identifier',\n",
       " 'identifier_md5',\n",
       " 'intersection',\n",
       " 'invert',\n",
       " 'is_convex',\n",
       " 'is_empty',\n",
       " 'is_volume',\n",
       " 'is_watertight',\n",
       " 'is_winding_consistent',\n",
       " 'kdtree',\n",
       " 'mass',\n",
       " 'mass_properties',\n",
       " 'md5',\n",
       " 'merge_vertices',\n",
       " 'metadata',\n",
       " 'moment_inertia',\n",
       " 'nearest',\n",
       " 'outline',\n",
       " 'permutate',\n",
       " 'principal_inertia_components',\n",
       " 'principal_inertia_transform',\n",
       " 'principal_inertia_vectors',\n",
       " 'process',\n",
       " 'ray',\n",
       " 'register',\n",
       " 'remove_degenerate_faces',\n",
       " 'remove_duplicate_faces',\n",
       " 'remove_infinite_values',\n",
       " 'remove_unreferenced_vertices',\n",
       " 'rezero',\n",
       " 'sample',\n",
       " 'scale',\n",
       " 'scene',\n",
       " 'section',\n",
       " 'section_multiplane',\n",
       " 'show',\n",
       " 'slice_plane',\n",
       " 'smoothed',\n",
       " 'split',\n",
       " 'subdivide',\n",
       " 'submesh',\n",
       " 'symmetry',\n",
       " 'symmetry_axis',\n",
       " 'symmetry_section',\n",
       " 'to_dict',\n",
       " 'triangles',\n",
       " 'triangles_center',\n",
       " 'triangles_cross',\n",
       " 'triangles_tree',\n",
       " 'union',\n",
       " 'units',\n",
       " 'unmerge_vertices',\n",
       " 'update_faces',\n",
       " 'update_vertices',\n",
       " 'vertex_adjacency_graph',\n",
       " 'vertex_defects',\n",
       " 'vertex_neighbors',\n",
       " 'vertex_normals',\n",
       " 'vertices',\n",
       " 'visual',\n",
       " 'volume',\n",
       " 'voxelized']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(voxel_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing OFF file\n",
      "Step 3: Exporting Voxel Off function: 1.0204806327819824\n"
     ]
    }
   ],
   "source": [
    "#try exporting with own function\n",
    "start_time = time.time()\n",
    "#try the inline printing method:\n",
    "write_Whole_Neuron_Off_file(str(key[\"segment_id\"]) + \"_voxel_1000\"\n",
    "                            ,voxel_mesh.vertices\n",
    "                            , voxel_mesh.faces)\n",
    "print(f\"Step 3: Exporting Voxel Off function: {time.time() - start_time}\")\n",
    "\n",
    "#     #Exporting the Voxelization as an off file for meshlabserver\n",
    "#     start_time = time.time()\n",
    "#     #try the inline printing method:\n",
    "#     with open(os.devnull, 'w') as devnull:\n",
    "#         with contextlib.redirect_stdout(devnull):\n",
    "#             voxel_mesh.export(str(path_and_filename) + \"voxel_.off\")\n",
    "#     print(f\"Step 3: Exporting Voxel Off function: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_key = dict(segmentation=2,segment_id=648518346349483124)\n",
    "make_skeleton_voxels(new_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

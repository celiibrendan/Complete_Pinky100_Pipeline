{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing out the full soma extraction\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "from whole_neuron_classifier_datajoint_adapted import extract_branches_whole_neuron\n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m65 = dj.create_virtual_module('m65', 'microns_minnie65_01')\n",
    "schema = dj.schema(\"microns_minnie65_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_id = 107816118160698192\n",
    "version = 0\n",
    "key = dict(segment_id=segment_id, version = version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Entire Mesh Processed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all the meshes into one mesh\n",
    "def add_mesh_piece(main_mesh_vertices,main_mesh_faces,sub_mesh_vertices,sub_mesh_faces):\n",
    "    \"\"\"\n",
    "    Purpose: Takes in a large mesh piece and an array of other meshes and \n",
    "    returns a large mesh with all meshes appended\n",
    "    \n",
    "    Parameters:\n",
    "    main_mesh_vertices (np.array) : np array store the vertices as rows and the elements as the coordinates\n",
    "    main_mesh_faces (np.array) : np array store the faces as rows and the elements as the referenced vertices\n",
    "    sub_mesh_vertices(list of np.arrays) : list of np arrays with the vertices arrays for all subsegments to be added\n",
    "    sub_mesh_faces(list of np.arrays) : list of np arrays with the faces arrays for all subsegments to be added\n",
    "    \n",
    "    Returns:\n",
    "    mesh_vertices (np.array) : np array store the vertices as rows and the elements as the coordinates for NEW CONCATENATED MESH\n",
    "    mesh_faces (np.array) : np array store the faces as rows and the elements as the referenced vertices for NEW CONCATENATED MESH\n",
    "    \n",
    "    \n",
    "    Pseudocode: \n",
    "    - Checks: \n",
    "    a. Make sure there sub_mesh arrays are greater than 0 and of the same length\n",
    "\n",
    "    1) Count the number of vertices and faces in the main mesh\n",
    "    2) Iterate through the submesh vertices and faces. In loop:\n",
    "    a. Count the number of vertices in the submesh and concate the vertices arrays to the main mesh array\n",
    "    b. Add the vertices_count and add that to every number in the faces array\n",
    "    c. Concatenate the submesh faces onto the larger mesh face\n",
    "    d. Save this new vertices and faces as the main_mesh verts and faces\n",
    "    e. Print out how many new vertices and faces added\n",
    "    3) Print out number of segments added, total faces/vertices for new mesh\n",
    "    4) Return the main mesh vertices and faces\n",
    "    \n",
    "    \"\"\"\n",
    "    #a. Make sure there sub_mesh arrays are greater than 0 and of the same length\n",
    "    if len(sub_mesh_vertices) <= 0:\n",
    "        print(\"There were no vertices in submesh to add, returning main mesh\")\n",
    "        return main_mesh_vertices, main_mesh_faces\n",
    "    if len(sub_mesh_faces) <= 0:\n",
    "        print(\"There were no face in submesh to add, returning main mesh\")\n",
    "        return main_mesh_vertices, main_mesh_faces\n",
    "    if len(sub_mesh_faces) != len(sub_mesh_vertices):\n",
    "        raise Exception(\"The sub_mesh_faces and sub_mesh_vertices length did not match\")\n",
    "        \n",
    "    \n",
    "    #1) Count the number of vertices and faces in the main mesh\n",
    "    n_main_vertices = len(main_mesh_vertices)\n",
    "    n_main_faces = len(main_mesh_faces)\n",
    "    \n",
    "    \n",
    "    #2) Iterate through the submesh vertices and faces. In loop:\n",
    "    for i,(sub_verts, sub_faces) in enumerate(zip(sub_mesh_vertices,sub_mesh_faces)):\n",
    "        #a. Count the number of vertices in the submesh and concate the vertices arrays to the main mesh array\n",
    "        n_sub_verts = len(sub_verts)\n",
    "        n_sub_faces = len(sub_faces)\n",
    "        \n",
    "        main_mesh_vertices = np.vstack([main_mesh_vertices,sub_verts])\n",
    "\n",
    "        \n",
    "        #b. Add the vertices_count of main to every number in the faces array\n",
    "        sub_faces = sub_faces + n_main_vertices\n",
    "        \n",
    "        #c. Concatenate the submesh faces onto the larger mesh face\n",
    "        main_mesh_faces = np.vstack([main_mesh_faces,sub_faces])\n",
    "        \n",
    "        #d. Save this new vertices and faces as the main_mesh verts and faces (DONE)\n",
    "        \n",
    "        #e. Print out how many new vertices and faces added\n",
    "        #print(f\"Added subsegment {i} with {n_sub_verts} vertices and {n_sub_faces} faces\")\n",
    "        \n",
    "        n_main_vertices = len(main_mesh_vertices)\n",
    "        n_main_faces = len(main_mesh_faces)\n",
    "    \n",
    "    #3) Print out number of segments added, total faces/vertices for new mesh  \n",
    "    print(f\"Added {len(sub_mesh_vertices)} subsegements \\n  --> final mesh: {len(main_mesh_vertices)} vertices and {len(main_mesh_faces)} faces\")\n",
    "        \n",
    "    return main_mesh_vertices,main_mesh_faces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = (m65.FromNeuromancer & key).fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---Main Neuron ---\")\n",
    "print(n[\"segment_id\"])\n",
    "print(n[\"n_vertices\"])\n",
    "print(n[\"vertices\"])\n",
    "print(n[\"faces\"])\n",
    "\n",
    "#get all of the segments and their data\n",
    "lookup_key = dict(segment_id=n[\"segment_id\"],version = n[\"version\"])\n",
    "subsegments = (m65.FromNeuromancer.Subsegment & lookup_key).fetch(as_dict=True)\n",
    "\n",
    "subsegment_dicts = dict([(k[\"subsegment_id\"],dict(vertices=k[\"vertices\"],faces=k[\"faces\"])) for k in subsegments])\n",
    "\n",
    "subsegment_ordered_list = np.sort(np.array(list(subsegment_dicts.keys())))\n",
    "subsegments_vertices = [subsegment_dicts[k][\"vertices\"] for k in subsegment_ordered_list]\n",
    "subsegments_faces = [subsegment_dicts[k][\"faces\"] for k in subsegment_ordered_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the entire mesh from the main mesh and all of its sub meshes: \n",
    "new_mesh_vertices, new_mesh_faces = add_mesh_piece(main_mesh_vertices=n[\"vertices\"],\n",
    "                   main_mesh_faces=n[\"faces\"],\n",
    "                   sub_mesh_vertices = subsegments_vertices,\n",
    "                   sub_mesh_faces=subsegments_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Meshlab and CGAL algorithms to extract the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"_orig_inal.off\"\n",
    "indices = [i for i, a in enumerate(s) if a == \"_\"]\n",
    "print(indices)\n",
    "s[:-(len(s)-indices[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "original_main = trimesh.Trimesh(vertices=n[\"vertices\"],faces=n[\"faces\"])\n",
    "output_mesh_name = \"temp/\" + str(n[\"segment_id\"]) + \"_original.off\"\n",
    "original_main.export(\"./\" + output_mesh_name)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# having exporting the mesh then starts to do the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 81498689075439039_multiple_somas.off\n"
     ]
    }
   ],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "from whole_neuron_classifier_datajoint_adapted import extract_branches_whole_neuron\n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datajoint as dj\n",
    "\n",
    "import os\n",
    "# total_test_meshes = [\n",
    "# '103178515351946567_stitched.off',\n",
    "# '104726695973782750_stitched.off',\n",
    "# '106626583548494129_stitched.off',\n",
    "# '107816118160698192_stitched.off',\n",
    "# '110778132960975016_stitched.off',\n",
    "# '96631955273149705_stitched.off',\n",
    "# '81498689075439039_multiple_somas.off']\n",
    "\n",
    "# output_file = total_test_meshes[0]\n",
    "# folder_name = \"test_neurons/\" \n",
    "\n",
    "total_test_meshes = [\n",
    "'81498689075439039_multiple_somas.off']\n",
    "\n",
    "output_file = total_test_meshes[0]\n",
    "folder_name = \"neurons_potential_soma/\" \n",
    "\n",
    "output_mesh_name = folder_name + output_file\n",
    "print(f\"Working on {output_file}\")\n",
    "\n",
    "indices = [i for i, a in enumerate(output_file) if a == \"_\"]\n",
    "indices\n",
    "seg_id_stripped = output_file[:indices[0]]\n",
    "n = dict(segment_id=int(seg_id_stripped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_meshlab_script(mlx_script,input_mesh_file,output_mesh_file):\n",
    "    script_command = (\" -i \" + str(input_mesh_file) + \" -o \" + \n",
    "                                    str(output_mesh_file) + \" -s \" + str(mlx_script))\n",
    "    #return script_command\n",
    "    command_to_run = 'xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@ ' + script_command\n",
    "    #command_to_run = 'meshlabserver ' + script_command\n",
    "    \n",
    "    print(command_to_run)\n",
    "    subprocess_result = subprocess.run(command_to_run,shell=True)\n",
    "    \n",
    "    return subprocess_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess\n",
    "def meshlab_fix_manifold_path_specific_mls(input_path_and_filename,\n",
    "                                           output_path_and_filename=\"\",\n",
    "                                           segment_id=-1,meshlab_script=\"\"):\n",
    "    #fix the path if it comes with the extension\n",
    "    if input_path_and_filename[-4:] == \".off\":\n",
    "        path_and_filename = input_path_and_filename[:-4]\n",
    "        input_mesh = input_path_and_filename\n",
    "    else:\n",
    "        raise Exception(\"Not passed off file\")\n",
    "    \n",
    "    \n",
    "    if output_path_and_filename == \"\":\n",
    "        output_mesh = path_and_filename+\"_mls.off\"\n",
    "    else:\n",
    "        output_mesh = output_path_and_filename\n",
    "    \n",
    "    if meshlab_script == \"\":\n",
    "        meshlab_script = str(pathlib.Path.cwd()) + \"/\" + \"remeshing_remove_non_man_edges.mls\"\n",
    "    \n",
    "    #print(\"meshlab_script = \" + str(meshlab_script))\n",
    "    #print(\"starting meshlabserver fixing non-manifolds\")\n",
    "    subprocess_result_1 = run_meshlab_script(meshlab_script,\n",
    "                      input_mesh,\n",
    "                      output_mesh)\n",
    "    #print(\"Poisson subprocess_result= \"+ str(subprocess_result_1))\n",
    "    \n",
    "    if str(subprocess_result_1)[-13:] != \"returncode=0)\":\n",
    "        raise Exception('neuron' + str(segment_id) + \n",
    "                         ' did not fix the manifold edges')\n",
    "    \n",
    "    return output_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done exporting\n"
     ]
    }
   ],
   "source": [
    "new_mesh = trimesh.load_mesh(output_mesh_name)\n",
    "mesh_splits = new_mesh.split(only_watertight=True)\n",
    "\n",
    "len(\"Total mesh splits = \" + str(mesh_splits))\n",
    "#get the largest mesh\n",
    "mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# sns.distplot(mesh_lengths)\n",
    "\n",
    "largest_index = np.where(mesh_lengths == np.max(mesh_lengths))\n",
    "largest_mesh = mesh_splits[largest_index][0]\n",
    "\n",
    "\n",
    "stripped_ending = output_mesh_name[:-4]\n",
    "pre_largest_mesh_path = stripped_ending + \"_largest_piece.off\"\n",
    "\n",
    "largest_mesh.export(pre_largest_mesh_path)\n",
    "print(\"done exporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/poisson_working_meshlab.mls\n",
      "/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece.off\n",
      "/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece_mls.off\n",
      "Running the mls function\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece.off -o /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece_mls.off -s /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/poisson_working_meshlab.mls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/neurons_potential_soma/81498689075439039_multiple_somas_largest_piece_mls.off'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "# run the meshlab server script\n",
    "script_name = \"poisson_working_meshlab.mls\"\n",
    "meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "input_path =str(pathlib.Path.cwd()) + \"/\" +  pre_largest_mesh_path\n",
    "\n",
    "indices = [i for i, a in enumerate(input_path) if a == \"_\"]\n",
    "stripped_ending = input_path[:-4]\n",
    "\n",
    "output_path = stripped_ending + \"_mls.off\"\n",
    "print(meshlab_script_path_and_name)\n",
    "print(input_path)\n",
    "print(output_path)\n",
    "print(\"Running the mls function\")\n",
    "meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_path,\n",
    "                                           output_path_and_filename=output_path,\n",
    "                                           segment_id=n[\"segment_id\"],\n",
    "                                           meshlab_script=meshlab_script_path_and_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CGAL segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the mesh\n",
    "new_mesh = trimesh.load_mesh(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done exporting\n"
     ]
    }
   ],
   "source": [
    "mesh_splits = new_mesh.split(only_watertight=True)\n",
    "\n",
    "len(\"Total mesh splits = \" + str(mesh_splits))\n",
    "#get the largest mesh\n",
    "mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# sns.distplot(mesh_lengths)\n",
    "\n",
    "largest_index = np.where(mesh_lengths == np.max(mesh_lengths))\n",
    "largest_mesh = mesh_splits[largest_index][0]\n",
    "\n",
    "\n",
    "indices = [i for i, a in enumerate(output_path) if a == \"_\"]\n",
    "stripped_ending = output_path[:-4]\n",
    "largest_mesh_path = stripped_ending + \"_largest_piece.off\"\n",
    "\n",
    "largest_mesh.export(largest_mesh_path)\n",
    "print(\"done exporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+ only.\n",
    "import whole_neuron_classifier_datajoint_adapted as wcda \n",
    "wcda = reload(wcda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0014100074768066406\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2\n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.9991397857666016\n",
      "2) Finished: Generating CGAL segmentation for neuron: 1.184265375137329\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.00316619873046875\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 0.00152587890625\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.011465311050415039\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.041910409927368164\n",
      "Returning the soma_sdf value\n"
     ]
    }
   ],
   "source": [
    "segment_id = int(n[\"segment_id\"])\n",
    "faces = np.array(largest_mesh.faces)\n",
    "verts = np.array(largest_mesh.vertices)\n",
    "#run the whole algorithm on the neuron to test\n",
    "verts_labels, faces_labels, soma_value = wcda.extract_branches_whole_neuron(import_Off_Flag=False,segment_id=segment_id,vertices=verts,\n",
    "                     triangles=faces,pymeshfix_Flag=False,\n",
    "                     import_CGAL_Flag=False,\n",
    "                     return_Only_Labels=True,\n",
    "                     clusters=3,\n",
    "                     smoothness=0.2,\n",
    "                    soma_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.717241"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soma_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5.0: 4891, 19.0: 2761})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(faces_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "soma_faces = np.where(faces_labels == 5.0)[0]\n",
    "soma_mesh = largest_mesh.submesh([soma_faces],append=True)\n",
    "soma_mesh.export(folder_name + str(n[\"segment_id\"]) + \"_soma.off\")\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "non_soma_faces = np.where(faces_labels != 5.0)[0]\n",
    "non_soma_mesh = largest_mesh.submesh([soma_faces],append=True)\n",
    "non_soma_mesh.export(folder_name + str(n[\"segment_id\"]) + \"_NON_soma.off\")\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the center of the soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_center = soma_mesh.vertices.mean(axis=0).astype(\"float\")\n",
    "print(\"Poor man's center from just averagin vertices = \" + str(soma_center))\n",
    "print(\"Trimesh center of mass = \" + str(soma_mesh.center_mass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

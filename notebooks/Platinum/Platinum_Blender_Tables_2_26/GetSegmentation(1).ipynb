{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudvolume # run pip3 install scikit-build analysisdatalink cloud-volume if pkg is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_path = 'precomputed://gs://microns-seunglab/minnie65/seg_minnie65_0'\n",
    "cv = cloudvolume.CloudVolume(segmentation_path, use_https=True, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## available resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmip is describing the pixel resolution\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mip is describing the pixel resolution\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mip</th>\n",
       "      <th>x (nm)</th>\n",
       "      <th>y (nm)</th>\n",
       "      <th>z (nm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>4096</td>\n",
       "      <td>4096</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mip  x (nm)  y (nm)  z (nm)\n",
       "0    1       8       8      40\n",
       "1    2      16      16      40\n",
       "2    3      32      32      40\n",
       "3    4      64      64      40\n",
       "4    5     128     128      40\n",
       "5    6     256     256      40\n",
       "6    7     512     512      40\n",
       "7    8    1024    1024      40\n",
       "8    9    2048    2048      40\n",
       "9   10    4096    4096      40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.hstack([np.arange(1,11)[:,None],np.array(list(cv.available_resolutions))])).rename(columns={0:\"mip\", 1:\"x (nm)\", 2:\"y (nm)\", 3:\"z (nm)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_mip = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cloudvolume.CloudVolume(segmentation_path, mip=chosen_mip, use_https=True, progress=True) # set desired mip from table above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_bounds = cv.mip_bounds(mip=cv.mip).to_dict() # get min and max boundaries of chosen mip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dir = '/notebooks3/segmentation/mip8' # enter your desired path for saving the segmentation then run next cell to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = 'file://' + path_to_dir\n",
    "volume_slice = cloudvolume.Bbox(mip_bounds['minpt'], mip_bounds['maxpt'])\n",
    "cv.transfer_to(path_to_save, volume_slice, cv.mip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dir = '/notebooks3/segmentation/mip8' # enter your desired path for saving the segmentation then run next cell to download\n",
    "#cv = cloudvolume.CloudVolume('file://' + path_to_dir, mip=cv.mip) # load segmentation after already downloaded\n",
    "cv = cloudvolume.CloudVolume('file://' + path_to_dir, mip=8) # load segmentation after already downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroglancer_coordinate = np.array([236719, 188544, 17210]) # enter your desired neuroglancer coordinate here then run next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95855698856145170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "segment_id = cv[list(cv.point_to_mip(neuroglancer_coordinate, 0,cv.mip)/np.array([2,2,1]))].squeeze() # need to divide by an extra factor of [2,2,1] because mip 0 not counted in cv.mip and Neuroglancer is in mip0 coords\n",
    "print(segment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify AllenSegmentCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "m65 = dj.create_virtual_module('microns_minnie65_01', 'microns_minnie65_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provided_link = 'https://neuromancer-seung-import.appspot.com/#!%7B%22layers%22:%5B%7B%22source%22:%22precomputed://gs://microns-seunglab/minnie_v4/alignment/fine/sergiy_multimodel_v1/vector_fixer30_faster_v01/image_stitch_multi_block_v1%22%2C%22type%22:%22image%22%2C%22blend%22:%22default%22%2C%22shaderControls%22:%7B%7D%2C%22name%22:%22image%22%7D%2C%7B%22source%22:%22precomputed://gs://microns-seunglab/minnie65/single_sections%22%2C%22type%22:%22image%22%2C%22blend%22:%22default%22%2C%22shaderControls%22:%7B%7D%2C%22name%22:%22mip1%22%2C%22visible%22:false%7D%2C%7B%22tool%22:%22annotateLine%22%2C%22selectedAnnotation%22:%7B%22id%22:%22f5faca4325acb092b58b7dc0537540731753c48b%22%7D%2C%22annotationColor%22:%22#ffffff%22%2C%22type%22:%22annotation%22%2C%22annotations%22:%5B%7B%22pointA%22:%5B282609.28125%2C339508%2C14884%5D%2C%22pointB%22:%5B229619.796875%2C339508%2C27882%5D%2C%22type%22:%22line%22%2C%22id%22:%221fc63d14acf21afc7be1bf019ad228732b8d0f26%22%2C%22tagIds%22:%5B%5D%7D%2C%7B%22pointA%22:%5B297635.59375%2C339508%2C14884%5D%2C%22pointB%22:%5B392616.71875%2C339508%2C27882%5D%2C%22type%22:%22line%22%2C%22id%22:%22918338b03490963acb90a5245f02a353a41fae84%22%2C%22tagIds%22:%5B%5D%7D%2C%7B%22pointA%22:%5B282609.28125%2C339508%2C14884%5D%2C%22pointB%22:%5B282600.15625%2C87589.9453125%2C14884%5D%2C%22type%22:%22line%22%2C%22id%22:%22fea8955e5bd0d9376a14a0a63f44ef502c68f581%22%2C%22tagIds%22:%5B%5D%7D%2C%7B%22pointA%22:%5B297635.59375%2C339508%2C14884%5D%2C%22pointB%22:%5B298099.09375%2C87556%2C14884%5D%2C%22type%22:%22line%22%2C%22id%22:%22363e552e5d3290650a7b877e1cb28c82092f1b60%22%2C%22tagIds%22:%5B%5D%7D%2C%7B%22pointA%22:%5B229619.796875%2C339508%2C27882%5D%2C%22pointB%22:%5B228688.6875%2C87556%2C27882%5D%2C%22type%22:%22line%22%2C%22id%22:%228c7a1f26dabce2b261c36128035b647d8c1fcb36%22%2C%22tagIds%22:%5B%5D%7D%2C%7B%22pointA%22:%5B392616.71875%2C339508%2C27882%5D%2C%22pointB%22:%5B392170.96875%2C87556%2C27882%5D%2C%22type%22:%22line%22%2C%22id%22:%223aae3309f646e6362959b6ea778639484e836aef%22%2C%22tagIds%22:%5B%5D%7D%2C%7B%22pointA%22:%5B228688.6875%2C87556%2C27882%5D%2C%22pointB%22:%5B282600.15625%2C87589.9453125%2C14884%5D%2C%22type%22:%22line%22%2C%22id%22:%22e287f6fe233d27cdb37662681159cc7dfb4d91c3%22%2C%22tagIds%22:%5B%5D%7D%2C%7B%22pointA%22:%5B298099.09375%2C87556%2C14884%5D%2C%22pointB%22:%5B392170.96875%2C87556%2C27882%5D%2C%22type%22:%22line%22%2C%22id%22:%22f5faca4325acb092b58b7dc0537540731753c48b%22%2C%22tagIds%22:%5B%5D%7D%5D%2C%22annotationTags%22:%5B%5D%2C%22voxelSize%22:%5B4%2C4%2C40%5D%2C%22name%22:%22area-boundaries%22%7D%2C%7B%22type%22:%22annotation%22%2C%22annotations%22:%5B%5D%2C%22annotationTags%22:%5B%5D%2C%22voxelSize%22:%5B4%2C4%2C40%5D%2C%22name%22:%22centroid%22%7D%2C%7B%22source%22:%22precomputed://gs://microns-seunglab/minnie65/seg_minnie65_0%22%2C%22type%22:%22segmentation%22%2C%22skeletonRendering%22:%7B%22mode2d%22:%22lines_and_points%22%2C%22mode3d%22:%22lines%22%7D%2C%22name%22:%22seg%22%7D%5D%2C%22navigation%22:%7B%22pose%22:%7B%22position%22:%7B%22voxelSize%22:%5B4%2C4%2C40%5D%2C%22voxelCoordinates%22:%5B288360%2C136805%2C20366%5D%7D%2C%22orientation%22:%5B0%2C-0.7071067690849304%2C0%2C0.7071067690849304%5D%7D%2C%22zoomFactor%22:442.82440099225244%7D%2C%22showAxisLines%22:false%2C%22perspectiveOrientation%22:%5B0.5%2C-0.5%2C0.5%2C0.5%5D%2C%22perspectiveZoom%22:43225.153342864134%2C%22showSlices%22:false%2C%22gpuMemoryLimit%22:2500000000%2C%22systemMemoryLimit%22:2500000000%2C%22selectedLayer%22:%7B%22layer%22:%22seg%22%2C%22visible%22:true%7D%2C%22layout%22:%7B%22type%22:%223d%22%2C%22orthographicProjection%22:true%7D%7D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import re\n",
    "from IPython.display import HTML, display, clear_output\n",
    "from ipywidgets import widgets\n",
    "\n",
    "def html_to_json(url_string, return_parsed_url=False, fragment_prefix='!'):\n",
    "    # Parse neuromancer url to logically separate the json state dict from the rest of it.\n",
    "    full_url_parsed = urllib.parse.urlparse(url_string)\n",
    "    # Decode percent-encoding in url, and skip \"!\" from beginning of string.\n",
    "    decoded_fragment = urllib.parse.unquote(full_url_parsed.fragment)\n",
    "    if decoded_fragment.startswith(fragment_prefix):\n",
    "        decoded_fragment = decoded_fragment[1:]\n",
    "    # Load the json state dict string into a python dictionary.\n",
    "    json_state_dict = json.loads(decoded_fragment)\n",
    "\n",
    "    if return_parsed_url:\n",
    "        return json_state_dict, full_url_parsed\n",
    "    else:\n",
    "        return json_state_dict\n",
    "\n",
    "\n",
    "def add_segments(provided_link, segments, seg_name='seg', overwrite=True, color=None):\n",
    "    json_data, parsed_url = html_to_json(provided_link, return_parsed_url=True)\n",
    "    seg_strings = []\n",
    "    for seg in segments:\n",
    "        seg_strings.append(seg.astype(np.str))\n",
    "    segmentation_layer = list(filter(lambda d: np.logical_and(d['type'] == 'segmentation', d['name'] == seg_name), json_data['layers']))\n",
    "    if len(segmentation_layer) == 0:\n",
    "        print('segmentation layer does not exist... creating it')\n",
    "        json_data['layers'].append({\n",
    "        \"source\": \"precomputed://gs://microns-seunglab/minnie65/seg_minnie65_0\",\n",
    "        \"type\": \"segmentation\",\n",
    "        \"colorSeed\": 2940450712,\n",
    "        \"segments\": [],\n",
    "        \"skeletonRendering\": {\n",
    "        \"mode2d\": \"lines_and_points\",\n",
    "        \"mode3d\": \"lines\"\n",
    "          },\n",
    "       \"name\": f\"{seg_name}\"\n",
    "        })\n",
    "        segmentation_layer = list(filter(lambda d: np.logical_and(d['type'] == 'segmentation', d['name'] == seg_name), json_data['layers']))\n",
    "    if re.search('segments',json.dumps(json_data)) is None:\n",
    "        segmentation_layer[0].update({'segments':[]})\n",
    "    if overwrite:\n",
    "        segmentation_layer[0]['segments'] = seg_strings\n",
    "    else:\n",
    "        segmentation_layer[0]['segments'].extend(seg_strings)\n",
    "    if color is not None:\n",
    "        if re.search('segmentColors',json.dumps(json_data)) is None:\n",
    "            segmentation_layer[0].update({'segmentColors':{}})\n",
    "        color_dict = {}\n",
    "        for seg in segments:\n",
    "            color_dict.update({str(seg):color})\n",
    "        segmentation_layer[0]['segmentColors'] = color_dict\n",
    "            \n",
    "    return urllib.parse.urlunparse([parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, parsed_url.query, '!'+ urllib.parse.quote(json.dumps(json_data))])\n",
    "\n",
    "def add_point_annotations(provided_link, ano_name, ano_list, voxelsize, descriptions=None, color='#f1ff00', overwrite=True):\n",
    "    # format annotation list\n",
    "    ano_list_dict = []\n",
    "    if ano_list.ndim<2:\n",
    "        ano_list = np.expand_dims(ano_list,0)\n",
    "    if ano_list.ndim>2:\n",
    "        return print('The annotation list must be 1D or 2D')\n",
    "    \n",
    "    if descriptions is not None:\n",
    "        for i, (centroid, desc) in enumerate(zip(ano_list.tolist(), descriptions)):\n",
    "            dict_to_add = {'point':centroid, 'type':'point', 'id':str(i+1), 'description':str(desc), 'tagIds':[]}\n",
    "            ano_list_dict.append(dict_to_add)\n",
    "            print(dict_to_add)\n",
    "            \n",
    "    for i, centroid in enumerate(ano_list.tolist()):\n",
    "            ano_list_dict.append({'point':centroid, 'type':'point', 'id':str(i+1), 'tagIds':[]})\n",
    "\n",
    "    json_data, parsed_url = html_to_json(provided_link, return_parsed_url=True)\n",
    "    # if annotation layer doesn't exist, create it\n",
    "    if re.search(ano_name,json.dumps(json_data)) is None:\n",
    "        json_data['layers'].append({'tool': 'annotatePoint',\n",
    "                               'type': 'annotation',\n",
    "                               'annotations': [],\n",
    "                               'annotationColor': color,\n",
    "                               'annotationTags': [],\n",
    "                               'voxelSize': voxelsize,\n",
    "                               'name': ano_name})\n",
    "        print('annotation layer does not exist... creating it')\n",
    "    annotation_dict = list(filter(lambda d: d['name'] == ano_name, json_data['layers']))\n",
    "    annotation_ind = np.where(np.array(json_data['layers']) == annotation_dict)[0][0].squeeze()\n",
    "    # test if voxel size of annotation matches provided voxel size\n",
    "    if json_data['layers'][annotation_ind]['voxelSize']!=voxelsize:\n",
    "        return print('The annotation layer already exists but does not match your provided voxelsize')\n",
    "    # add annotations\n",
    "    if overwrite:\n",
    "        json_data['layers'][annotation_ind]['annotations'] = ano_list_dict\n",
    "    else:\n",
    "        json_data['layers'][annotation_ind]['annotations'].extend(ano_list_dict)\n",
    "\n",
    "    return urllib.parse.urlunparse([parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, parsed_url.query, '!'+ urllib.parse.quote(json.dumps(json_data))])\n",
    "\n",
    "def verifyAllenSegmentCentroid(provided_link):\n",
    "    unchecked = np.stack((m65.AllenSegmentCentroid & 'status is NULL').fetch('segment_id', 'centroid_x', 'centroid_y', 'centroid_z'),-1)\n",
    "    seg_id, cenx, ceny, cenz = np.random.permutation(unchecked)[0]\n",
    "    print(f'segment_id: {seg_id}, centroid: {[cenx, ceny, cenz]}')\n",
    "    with_cent = add_point_annotations(provided_link, 'centroid', np.array([[cenx, ceny, cenz]]), voxelsize=[4,4,40])\n",
    "    with_seg = add_segments(with_cent, np.array([seg_id]), 'seg')\n",
    "    \n",
    "    correct_button = widgets.Button(description=\"Correct\")\n",
    "    incorrect_button = widgets.Button(description=\"Incorrect\")\n",
    "    display(HTML(f'<a href=\"{with_seg}\">Neuroglancer</a>'), correct_button, incorrect_button)\n",
    "\n",
    "    def on_correct_button_clicked(correct_button):\n",
    "        (m65.AllenSegmentCentroid() & f'segment_id={seg_id}' & f'centroid_x={cenx}')._update('status', 1)\n",
    "        print(f'Successful Update as Correct: seg_id: {seg_id}, centroid: {cenx, ceny, cenz}, status: 1')\n",
    "    \n",
    "    def on_incorrect_button_clicked(incorrect_button):\n",
    "        (m65.AllenSegmentCentroid() & f'segment_id={seg_id}' & f'centroid_x={cenx}')._update('status', 0)\n",
    "        print(f'Successful Update as Incorrect: seg_id: {seg_id}, centroid: {cenx, ceny, cenz}, status: 0')\n",
    "\n",
    "    correct_button.on_click(on_correct_button_clicked)\n",
    "    incorrect_button.on_click(on_incorrect_button_clicked)\n",
    "\n",
    "    return seg_id, cenx, ceny, cenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verifyAllenSegmentCentroid(provided_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m65.AllenSegmentCentroid() & 'status>0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

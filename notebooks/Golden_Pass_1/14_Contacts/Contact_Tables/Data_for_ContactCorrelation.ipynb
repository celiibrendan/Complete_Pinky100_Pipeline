{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieves the PrePost table that will be using in an all in one insertion (MAY HAVE TO ADJUST FOR BIGGER DATA SETS IN FUTURE)\n",
    "        prepost_data = ta3p100.ContactPrePost.proj(\"postsyn\",\"total_contact_conversion\",\n",
    "                \"total_contact_density\",\"total_synapse_sizes_mean\",\n",
    "                syn_density=\"(total_n_synapses*total_synapse_sizes_mean)/total_postsyn_length\",\n",
    "                presyn=\"segment_id\").fetch()\n",
    "        df = pd.DataFrame(prepost_data)\n",
    "\n",
    "        #gets all the combinations of postsyn-postsyn without any repeats\n",
    "        targets = (dj.U(\"postsyn\") & ta3p100.Contact).proj(segment_id=\"postsyn\") - ta3p100.SegmentExclude\n",
    "        info = targets * targets.proj(segment_b='segment_id') & 'segment_id < segment_b'\n",
    "        segment_pairs = info.fetch()\n",
    "        \n",
    "        total_correlations = []\n",
    "\n",
    "        for postsyn1,postsyn2 in tqdm(segment_pairs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            #print(\"postsyn1 = \" + str(postsyn1))\n",
    "            #print(\"postsyn2 = \" + str(postsyn2))\n",
    "\n",
    "            #get all of the rows with postsyn 1 and 2 AKA find the number of presyns for each\n",
    "            df_1 = df[df[\"postsyn\"]==postsyn1]\n",
    "            df_2 = df[df[\"postsyn\"]==postsyn2]\n",
    "\n",
    "            #reduce both tables down to common presyns\n",
    "            df_1_common = df_1[df_1[\"presyn\"].isin(df_2[\"presyn\"])].sort_values(by=['presyn'])\n",
    "            df_2_common = df_2[df_2[\"presyn\"].isin(df_1[\"presyn\"])].sort_values(by=['presyn'])\n",
    "\n",
    "\n",
    "            ###########------------------------------------------------------###########\n",
    "            #need to get the common axons that have at least one converted contact on one of the postsyns\n",
    "            \"\"\"pseudocode\n",
    "            #get the conversion rates for both tables\n",
    "            #add them up\n",
    "            #get the indices that are greater than 0\n",
    "            #get the presyn ids that match those rows\n",
    "            #further restrict both groups by those ids\n",
    "            \"\"\"\n",
    "\n",
    "            #get both of their conversion rates\n",
    "            test_1_conv = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "            test_2_conv = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "\n",
    "            test_1_presyn = df_1_common[\"presyn\"].to_numpy()\n",
    "            new_presyns = test_1_presyn[(test_1_conv + test_2_conv) > 0]\n",
    "\n",
    "            df_1_common_converted = df_1_common[df_1_common[\"presyn\"].isin(new_presyns)]\n",
    "            df_2_common_converted = df_2_common[df_2_common[\"presyn\"].isin(new_presyns)]\n",
    "            ###########------------------------------------------------------###########\n",
    "\n",
    "\n",
    "            #finds the number of segments, shared_segments and union segments\n",
    "            n_seg_a = df_1.shape[0]\n",
    "            n_seg_b = df_2.shape[0]\n",
    "            n_seg_shared = df_1_common.shape[0]\n",
    "            n_seg_shared_converted = df_1_common_converted.shape[0]\n",
    "            n_seg_union = n_seg_a + n_seg_b - n_seg_shared\n",
    "            \n",
    "            \n",
    "            #get the number and proportion on presyns that convert onto each segment inside the converted axon group\n",
    "            if n_seg_shared_converted > 0:\n",
    "                test_1_conv[test_1_conv>1] = 1\n",
    "                n_seg_a_converted = sum(np.ceil(test_1_conv))\n",
    "                test_2_conv[test_2_conv>1] = 1\n",
    "                n_seg_b_converted = sum(np.ceil(test_2_conv))\n",
    "\n",
    "                n_seg_a_converted_prop = n_seg_a_converted/n_seg_shared_converted\n",
    "                n_seg_b_converted_prop = n_seg_b_converted/n_seg_shared_converted\n",
    "            else:\n",
    "                n_seg_a_converted = 0\n",
    "                n_seg_b_converted = 0\n",
    "                n_seg_a_converted_prop = np.NaN\n",
    "                n_seg_b_converted_prop = np.NaN\n",
    "     \n",
    "            dict_segmenation=2\n",
    "            dict_segment_id=postsyn1\n",
    "            dict_segment_b=postsyn2\n",
    "            #initialize the dictionary that will be saved:\n",
    "            corr_dict = dict(segmentation=2,segment_id=postsyn1,\n",
    "                                          segment_b=postsyn2,\n",
    "                                          n_seg_a=n_seg_a,\n",
    "                                            n_seg_b=n_seg_b,\n",
    "                                            n_seg_shared=n_seg_shared,\n",
    "                                            n_seg_shared_converted=n_seg_shared_converted,\n",
    "                                            n_seg_union=n_seg_union,\n",
    "                                            n_seg_a_converted=n_seg_a_converted,\n",
    "                                            n_seg_b_converted=n_seg_b_converted,\n",
    "                                            n_seg_a_converted_prop=n_seg_a_converted_prop,\n",
    "                                            n_seg_b_converted_prop=n_seg_b_converted_prop)\n",
    "\n",
    "\n",
    "            #initialize the variables that need to be set in the dictionary\n",
    "\n",
    "\n",
    "            #ones that are set by 1st group\n",
    "            binary_conversion_pearson = np.NaN\n",
    "            binary_conversion_cosine = np.NaN\n",
    "            binary_conv_jaccard_ones_ratio = np.NaN\n",
    "            binary_conv_jaccard_matching_ratio = np.NaN\n",
    "            conversion_pearson = np.NaN\n",
    "            conversion_cosine = np.NaN\n",
    "            density_pearson = np.NaN\n",
    "            density_cosine = np.NaN\n",
    "            synapse_volume_mean_pearson = np.NaN\n",
    "            synapse_volume_mean_cosine = np.NaN\n",
    "            synapse_vol_density_pearson = np.NaN\n",
    "            synapse_vol_density_cosine = np.NaN\n",
    "\n",
    "            #ones that are set by 2nd group\n",
    "            binary_conversion_pearson_converted = np.NaN\n",
    "            binary_conversion_cosine_converted = np.NaN\n",
    "            binary_conv_jaccard_ones_ratio_converted = np.NaN\n",
    "            binary_conv_jaccard_matching_ratio_converted = np.NaN\n",
    "            conversion_pearson_converted = np.NaN\n",
    "            conversion_cosine_converted = np.NaN\n",
    "            density_pearson_converted = np.NaN\n",
    "            density_cosine_converted = np.NaN\n",
    "            synapse_volume_mean_pearson_converted = np.NaN\n",
    "            synapse_volume_mean_cosine_converted = np.NaN\n",
    "            synapse_vol_density_pearson_converted = np.NaN\n",
    "            synapse_vol_density_cosine_converted = np.NaN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (not df_1_common.to_numpy().any()) or (not df_2_common.to_numpy().any()):\n",
    "                #total_correlations.append(corr_dict)\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                #retrieve the conversion rates\n",
    "                df_1_common_conversion = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "                df_2_common_conversion = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "                \n",
    "                #calculate the binary conversion rates\n",
    "                df_1_common_binary_conversion = np.copy(df_1_common_conversion)\n",
    "                df_2_common_binary_conversion = np.copy(df_2_common_conversion)\n",
    "\n",
    "\n",
    "                df_1_common_binary_conversion[df_1_common_binary_conversion>0] = 1.0\n",
    "                df_2_common_binary_conversion[df_2_common_binary_conversion>0] = 1.0\n",
    "                \n",
    "                #retrieve the synapse/postsyn_len\n",
    "                df_1_common_density = df_1_common[\"total_contact_density\"].to_numpy()\n",
    "                df_2_common_density = df_2_common[\"total_contact_density\"].to_numpy()\n",
    "\n",
    "                #retrieve mean of synapse_size\n",
    "                df_1_common_synaptic_size = df_1_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "                df_2_common_synaptic_size = df_2_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "\n",
    "                #retrieve (total_n_synapses*total_synapse_sizes_mean)/total_postsyn_length\n",
    "                df_1_common_syn_density = df_1_common[\"syn_density\"].to_numpy()\n",
    "                df_2_common_syn_density = df_2_common[\"syn_density\"].to_numpy()\n",
    "\n",
    "\n",
    "                binary_conversion_pearson = find_pearson(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                binary_conversion_cosine = find_cosine(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                \n",
    "                #new added metric for the binary calculations based on jacard_similarity\n",
    "                binary_conv_jaccard_ones_ratio,binary_conv_jaccard_matching_ratio = find_binary_sim(df_1_common_binary_conversion,df_2_common_binary_conversion)\n",
    "                \n",
    "                conversion_pearson = find_pearson(df_1_common_conversion, df_2_common_conversion)\n",
    "                conversion_cosine = find_cosine(df_1_common_conversion, df_2_common_conversion)\n",
    "                density_pearson = find_pearson(df_1_common_density, df_2_common_density)\n",
    "                density_cosine = find_cosine(df_1_common_density, df_2_common_density)\n",
    "                synapse_volume_mean_pearson = find_pearson(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                synapse_volume_mean_cosine = find_cosine(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                synapse_vol_density_pearson = find_pearson(df_1_common_syn_density, df_2_common_syn_density)\n",
    "                synapse_vol_density_cosine = find_cosine(df_1_common_syn_density, df_2_common_syn_density)\n",
    "\n",
    "                \n",
    "                ####reset the df_1_common and df_1_common to reuse code\n",
    "                df_1_common = df_1_common_converted\n",
    "                df_2_common = df_2_common_converted\n",
    "\n",
    "                if (not df_1_common.to_numpy().any()) or (not df_2_common.to_numpy().any()):\n",
    "                    #print(\"none_in_converted\")\n",
    "                    pass\n",
    "                else:\n",
    "                    df_1_common_conversion = df_1_common[\"total_contact_conversion\"].to_numpy()\n",
    "                    df_2_common_conversion = df_2_common[\"total_contact_conversion\"].to_numpy()\n",
    "\n",
    "                    df_1_common_binary_conversion = np.copy(df_1_common_conversion)\n",
    "                    df_2_common_binary_conversion = np.copy(df_2_common_conversion)\n",
    "\n",
    "\n",
    "                    df_1_common_binary_conversion[df_1_common_binary_conversion>0] = 1.0\n",
    "                    df_2_common_binary_conversion[df_2_common_binary_conversion>0] = 1.0\n",
    "\n",
    "                    df_1_common_density = df_1_common[\"total_contact_density\"].to_numpy()\n",
    "                    df_2_common_density = df_2_common[\"total_contact_density\"].to_numpy()\n",
    "\n",
    "\n",
    "                    df_1_common_synaptic_size = df_1_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "                    df_2_common_synaptic_size = df_2_common[\"total_synapse_sizes_mean\"].to_numpy()\n",
    "\n",
    "                    df_1_common_syn_density = df_1_common[\"syn_density\"].to_numpy()\n",
    "                    df_2_common_syn_density = df_2_common[\"syn_density\"].to_numpy()\n",
    "\n",
    "                    \n",
    "\n",
    "                    binary_conversion_pearson_converted = find_pearson(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                    binary_conversion_cosine_converted = find_cosine(df_1_common_binary_conversion, df_2_common_binary_conversion)\n",
    "                    \n",
    "                    #new added metric for the binary calculations based on jacard_similarity\n",
    "                    binary_conv_jaccard_ones_ratio_converted,binary_conv_jaccard_matching_ratio_converted = find_binary_sim(df_1_common_binary_conversion,df_2_common_binary_conversion)\n",
    "                \n",
    "                    conversion_pearson_converted = find_pearson(df_1_common_conversion, df_2_common_conversion)\n",
    "                    conversion_cosine_converted = find_cosine(df_1_common_conversion, df_2_common_conversion)\n",
    "                    density_pearson_converted = find_pearson(df_1_common_density, df_2_common_density)\n",
    "                    density_cosine_converted = find_cosine(df_1_common_density, df_2_common_density)\n",
    "                    synapse_volume_mean_pearson_converted = find_pearson(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                    synapse_volume_mean_cosine_converted = find_cosine(df_1_common_synaptic_size, df_2_common_synaptic_size)\n",
    "                    synapse_vol_density_pearson_converted = find_pearson(df_1_common_syn_density, df_2_common_syn_density)\n",
    "                    synapse_vol_density_cosine_converted = find_cosine(df_1_common_syn_density, df_2_common_syn_density)\n",
    "\n",
    "\n",
    "\n",
    "            corr_dict[\"binary_conversion_pearson\"] = binary_conversion_pearson\n",
    "            corr_dict[\"binary_conversion_cosine\"] = binary_conversion_cosine\n",
    "            corr_dict[\"binary_conv_jaccard_ones_ratio\"] = binary_conv_jaccard_ones_ratio\n",
    "            corr_dict[\"binary_conv_jaccard_matching_ratio\"] = binary_conv_jaccard_matching_ratio\n",
    "            corr_dict[\"conversion_pearson\"] = conversion_pearson\n",
    "            corr_dict[\"conversion_cosine\"] = conversion_cosine\n",
    "            corr_dict[\"density_pearson\"] = density_pearson\n",
    "            corr_dict[\"density_cosine\"] = density_cosine\n",
    "            corr_dict[\"synapse_volume_mean_pearson\"] = synapse_volume_mean_pearson\n",
    "            corr_dict[\"synapse_volume_mean_cosine\"] = synapse_volume_mean_cosine\n",
    "            corr_dict[\"synapse_vol_density_pearson\"] = synapse_vol_density_pearson\n",
    "            corr_dict[\"synapse_vol_density_cosine\"] = synapse_vol_density_cosine\n",
    "\n",
    "            corr_dict[\"binary_conversion_pearson_converted\"] = binary_conversion_pearson_converted\n",
    "            corr_dict[\"binary_conversion_cosine_converted\"] = binary_conversion_cosine_converted\n",
    "            corr_dict[\"binary_conv_jaccard_ones_ratio_converted\"] = binary_conv_jaccard_ones_ratio_converted\n",
    "            corr_dict[\"binary_conv_jaccard_matching_ratio_converted\"] = binary_conv_jaccard_matching_ratio_converted\n",
    "            corr_dict[\"conversion_pearson_converted\"] = conversion_pearson_converted\n",
    "            corr_dict[\"conversion_cosine_converted\"] = conversion_cosine_converted\n",
    "            corr_dict[\"density_pearson_converted\"] = density_pearson_converted\n",
    "            corr_dict[\"density_cosine_converted\"] = density_cosine_converted\n",
    "            corr_dict[\"synapse_volume_mean_pearson_converted\"] = synapse_volume_mean_pearson_converted\n",
    "            corr_dict[\"synapse_volume_mean_cosine_converted\"] = synapse_volume_mean_cosine_converted\n",
    "            corr_dict[\"synapse_vol_density_pearson_converted\"] = synapse_vol_density_pearson_converted\n",
    "            corr_dict[\"synapse_vol_density_cosine_converted\"] = synapse_vol_density_cosine_converted\n",
    "\n",
    "\n",
    "\n",
    "            total_correlations.append(corr_dict)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Things that could go wrong (and have error checks for )\n",
    "1) Not getting an off file\n",
    "2) Cgal segmentation not going right\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import sys\n",
    "#import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import trimesh\n",
    "\n",
    "#for cgal segmentation\n",
    "import cgal_Segmentation_Module as csm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyMesh(object):\n",
    "    \n",
    "    #generates the mapping of vertices to the faces that are touching it\n",
    "    def generate_verts_to_face_dictionary(self):\n",
    "        verts_to_Face = {}\n",
    "\n",
    "        #initialize the lookup dictionary as empty lists\n",
    "        faces_raw = self.mesh.faces\n",
    "        verts_raw = self.mesh.vertices\n",
    "        \n",
    "        for i,pre_vertex in enumerate(verts_raw):\n",
    "            verts_to_Face[i] = []\n",
    "        \n",
    "\n",
    "        for i,verts in enumerate(faces_raw):\n",
    "            #add the index to the list for each of the vertices\n",
    "            for vertex in verts:\n",
    "                verts_to_Face[vertex].append(i)\n",
    "\n",
    "        return verts_to_Face\n",
    "    \n",
    "    def __init__(self,mesh_file_location,file_name):\n",
    "    #import the mesh\n",
    "\n",
    "        full_path = str(Path(mesh_file_location) / Path(file_name))\n",
    "        self.mesh = trimesh.load_mesh(full_path)\n",
    "        self.verts_to_Face = self.generate_verts_to_face_dictionary()\n",
    "        #get the vertices to faces lookup table\n",
    "\n",
    "    def find_neighbors_optomized(self,current_label):\n",
    "        \n",
    "        \n",
    "        col1_member = self.adjacency_labels_col1  == current_label\n",
    "        col2_member = self.adjacency_labels_col2  == current_label\n",
    "        \n",
    "        logical_xor = np.logical_xor(col1_member,col2_member)\n",
    "\n",
    "        total_array = np.concatenate([self.adjacency_labels_col1[logical_xor],\n",
    "              self.adjacency_labels_col2[logical_xor]])\n",
    "        \n",
    "        neighbors_shared_vert = dict(Counter(total_array))\n",
    "        del neighbors_shared_vert[current_label]\n",
    "        \n",
    "        neighbors_list = list(neighbors_shared_vert.keys())\n",
    "        number_of_faces = self.labels_list_counter[current_label]\n",
    "\n",
    "        \n",
    "        return neighbors_list,neighbors_shared_vert,number_of_faces\n",
    "    \n",
    "    def smooth_backbone_vp4_optomized(self,backbone_width_threshold = 0.35,\n",
    "                                      max_backbone_threshold = 400,\n",
    "                                      backbone_threshold=300,\n",
    "                                      shared_vert_threshold=25,\n",
    "                                      shared_vert_threshold_new = 5,\n",
    "                                      backbone_neighbor_min=10):\n",
    "        #print(\"at beginning of smooth backbone vp4\")\n",
    "        \n",
    "        faces_raw = self.mesh.faces\n",
    "        verts_raw = self.mesh.vertices\n",
    "\n",
    "        #generate the easy lookup table\n",
    "        verts_to_Face = self.verts_to_Face\n",
    "        \n",
    "        #new optomized way of getting initial backbone list\n",
    "        total_items = np.array(sorted(self.labels_list_counter.items()))\n",
    "        keys = total_items[:,0]\n",
    "        values = total_items[:,1]\n",
    "        big_threshold = values >= max_backbone_threshold\n",
    "\n",
    "        small_threshold = values > backbone_threshold \n",
    "        sdf_threshold = np.array(list(self.sdf_final_dict.values())) >= backbone_width_threshold\n",
    "        total_list = np.logical_or(big_threshold,np.logical_and(small_threshold,sdf_threshold))\n",
    "        backbone_labels = keys[total_list]\n",
    " \n",
    "        list_flag = False\n",
    "    \n",
    "        if list_flag == True:\n",
    "            to_remove = []\n",
    "        else:\n",
    "            to_remove = set()\n",
    "\n",
    "        backbone_neighbors_dict = {}\n",
    "\n",
    "        \n",
    "        \n",
    "        #finds all of the neighbors and how many shared vertices they have\n",
    "        for bkbone in backbone_labels:\n",
    "            #find_neighbors Description of Return List:\n",
    "            #1) neighbors_list = labels of all bordering neighbors\n",
    "            #2) neighbors_shared_vert = number of faces for each bordering neighbor\n",
    "            #3) number_of_faces = total number of faces for current label\n",
    "            \n",
    "            #neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors_optomized(bkbone)\n",
    "            neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors_optomized(bkbone)\n",
    "            #neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors(self.labels_list,bkbone)\n",
    "            #add the neighbor stats and count to the dictionary corresponding to that label\n",
    "            backbone_neighbors_dict[bkbone] = dict(neighbors_list=neighbors_list,neighbors_shared_vert=neighbors_shared_vert,\n",
    "                number_of_faces=number_of_faces)\n",
    "            \n",
    "        \n",
    "         #beginning smoothing round that removes ones from backbone list\n",
    "        for i in range(0,5):\n",
    "            print(\"smoothing round \" + str(i+1))\n",
    "            counter = 0\n",
    "            #iterates through all the groups that were designated as backbones\n",
    "            for bkbone in backbone_labels:\n",
    "                if bkbone not in to_remove: #if not already designated to be removed\n",
    "\n",
    "                    #just retrieve the neighbor stats and count of faces that are already stored in dict\n",
    "                    neighbors_list = backbone_neighbors_dict[bkbone][\"neighbors_list\"]\n",
    "                    neighbors_shared_vert = backbone_neighbors_dict[bkbone][\"neighbors_shared_vert\"]\n",
    "                    number_of_faces = backbone_neighbors_dict[bkbone][\"number_of_faces\"]\n",
    "\n",
    "                    #counts up the number of shared vertices with backbone neighbors\n",
    "\n",
    "                    #FUTURE OPTOMIZATION\n",
    "                    backbone_count_flag = False\n",
    "                    neighbor_counter = 0 #TOTAL NUMBER OF BACKBONE NEIGHBORS\n",
    "                    #spine_neighbor_counter = 0\n",
    "                    total_backbone_shared_verts = 0 #TOTAL NUMBER OF FACES SHARED WITH BACKBONE\n",
    "                    for n in neighbors_list:         \n",
    "                        if (n in backbone_labels) and (n not in to_remove):\n",
    "                            neighbor_counter += 1\n",
    "                            total_backbone_shared_verts = total_backbone_shared_verts + neighbors_shared_vert[n] \n",
    "                    \n",
    "\n",
    "                    #FUTURE OPTOMIZATION\n",
    "                    #if meets requirement of shared verts then activates flag     \n",
    "                    if (total_backbone_shared_verts > shared_vert_threshold):\n",
    "                        backbone_count_flag = True\n",
    "\n",
    "                    #if there are no neighbor's that are backbones or does not share enough backbone vertices --> remove from backbone list\n",
    "                    if neighbor_counter <= 0 or backbone_count_flag == False:\n",
    "                        if list_flag == True:\n",
    "                            to_remove.append(bkbone)\n",
    "                        else:\n",
    "                            to_remove.add(bkbone)\n",
    "                        counter += 1\n",
    "\n",
    "\n",
    "            #if 1 or less non-backbones were converted to remove list then go ahead to the next step\n",
    "            if counter <= 1:\n",
    "                #print(\"counter caused the break\")\n",
    "                break\n",
    "\n",
    "        #print(\"just broke out of the loop\")\n",
    "        \"\"\"\n",
    "        Status: \n",
    "        1) Started with a tentative list of backbones\n",
    "        2) Removed some potential backbone lists\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #now go through and make sure no unconnected backbone segments\n",
    "\n",
    "        \"\"\"Pseudo-code for filtering algorithm\n",
    "        1) iterate through all of the backbone labels\n",
    "        2) Go get the neighbors of the backbone\n",
    "        3) Add all of the neighbors who are too part of the backbone to the backbones to check list\n",
    "        4) While backbone neighbor counter is less than the threshold or until list to check is empty\n",
    "        5) Pop the next neighbor off the list and add it to the neighbors check list\n",
    "        6) Get the neighbors of this guy\n",
    "        7) for each of neighbors that is also on the backbone BUT HASN'T BEEN CHECKED YET append them to the list to be check and update counter\n",
    "        8) continue at beginning of loop\n",
    "        -- once loop breaks\n",
    "        9) if the counter is below the threshold:\n",
    "            Add all of values in the neighbros already checked list to the new_to_remove\n",
    "        10) Use the new_backbone_labels and new_to_remove to rewrite the labels_list\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #gets the new backbones list without the ones removed\n",
    "        #new_backbone_labels = [bkbone for bkbone in backbone_labels if bkbone not in to_remove] #OPTOMIZE\n",
    "        new_backbone_labels = list(set(backbone_labels).difference(to_remove))\n",
    "        \n",
    "        list_flag = True\n",
    "        if list_flag == True:\n",
    "            new_to_remove = []\n",
    "            skip_labels = []\n",
    "        else:\n",
    "            new_to_remove = set({})\n",
    "            skip_labels = set({})\n",
    "        \n",
    "\n",
    "        for bkbonz in new_backbone_labels:\n",
    "            if bkbonz not in skip_labels:\n",
    "                #print(\"working on backbone = \" + str(bkbonz))\n",
    "                if list_flag == True:\n",
    "                    checked_backbone_neighbors = []\n",
    "                    backbone_neighbors_to_check = []\n",
    "                else:\n",
    "                    checked_backbone_neighbors = set()\n",
    "                    backbone_neighbors_to_check = set()\n",
    "                new_backbone_neighbor_counter = 0\n",
    "\n",
    "\n",
    "#                 if bkbonz not in backbone_neighbors_dict.keys(): #should never enter this loop..... #OPTOMIZE\n",
    "#                     neighbors_list,neighbors_shared_vert,number_of_faces = self.find_neighbors(labels_list,bkbonz)\n",
    "#                     backbone_neighbors_dict[bkbonz] = dict(neighbors_list=neighbors_list,neighbors_shared_vert=neighbors_shared_vert,\n",
    "#                         number_of_faces=number_of_faces)\n",
    "                #gets the stats of the neighbors and count of current label\n",
    "                neighbors_list = backbone_neighbors_dict[bkbonz][\"neighbors_list\"]\n",
    "                neighbors_shared_vert = backbone_neighbors_dict[bkbonz][\"neighbors_shared_vert\"]\n",
    "                number_of_faces = backbone_neighbors_dict[bkbonz][\"number_of_faces\"]\n",
    "\n",
    "                for bb in neighbors_list:\n",
    "                    #counts as viable backbone neighbor if meets following conditions:\n",
    "                    #1) In the new backbone list\n",
    "                    #2) hasn't been checked yet\n",
    "                    #3) not in the new ones to remove\n",
    "                    #4) The number of neighbors shared by that label is greater than raw threshold shared_vert_threshold_new\n",
    "\n",
    "                    #OPTOMIZE: don't need checked_backbone_neighbors\n",
    "                    if (bb in new_backbone_labels) and (bb not in checked_backbone_neighbors) and (bb not in new_to_remove) and neighbors_shared_vert[bb] > shared_vert_threshold_new:\n",
    "                        if list_flag == True:\n",
    "                            backbone_neighbors_to_check.append(bb)\n",
    "                        else:\n",
    "                            backbone_neighbors_to_check.add(bb)\n",
    "                        new_backbone_neighbor_counter += 1\n",
    "\n",
    "                #at this point have :\n",
    "                #1) total number of backbone neighbors: new_backbone_neighbor_counter\n",
    "                #2) backbone neighbors in list: backbone_neighbors_to_check\n",
    "\n",
    "                if list_flag == True:\n",
    "                    checked_backbone_neighbors = [nb for nb in backbone_neighbors_to_check]\n",
    "                else:\n",
    "                    checked_backbone_neighbors = set([nb for nb in backbone_neighbors_to_check])\n",
    "\n",
    "\n",
    "                #4) While backbone neighbor counter is less than the threshold or until list to check is empty\n",
    "\n",
    "                #Iterates through all possible backbone neighbors unitl:\n",
    "                # A) new_backbone_neighbor_counter is greater than set threshold of backbone_neighbor_min OR\n",
    "                # B) no more backbone neighbors to check\n",
    "\n",
    "                #Goal: counts the backbone chain with that label, so in hopes if not high enough then not backbone piece\n",
    "                while new_backbone_neighbor_counter < backbone_neighbor_min and len(backbone_neighbors_to_check)>0:\n",
    "                    #5) Pop the next neighbor off the list and add it to the neighbors check list\n",
    "                    if list_flag == True:\n",
    "                        current_backbone = backbone_neighbors_to_check.pop(0)\n",
    "                    else:\n",
    "                        current_backbone = backbone_neighbors_to_check.pop()\n",
    "                        \n",
    "                    if current_backbone not in checked_backbone_neighbors:\n",
    "                        if list_flag == True:\n",
    "                            checked_backbone_neighbors.append(current_backbone) #mark it as checked\n",
    "                        else:\n",
    "                            checked_backbone_neighbors.add(current_backbone)\n",
    "                    \n",
    "                    #gets the current neighbors and counts of one of the possible neighbor backbones\n",
    "                    neighbors_list = backbone_neighbors_dict[current_backbone][\"neighbors_list\"]\n",
    "                    neighbors_shared_vert = backbone_neighbors_dict[current_backbone][\"neighbors_shared_vert\"]\n",
    "                    number_of_faces = backbone_neighbors_dict[current_backbone][\"number_of_faces\"]\n",
    "\n",
    "                    #7) for each of neighbors that is also on the backbone BUT HASN'T BEEN CHECKED YET append them to the list to be check and update counter\n",
    "                    for bb in neighbors_list:\n",
    "                        if (bb in new_backbone_labels) and (bb not in checked_backbone_neighbors) and (bb not in new_to_remove) and neighbors_shared_vert[bb] > shared_vert_threshold_new:\n",
    "                            if list_flag == True:\n",
    "                                backbone_neighbors_to_check.append(bb)\n",
    "                            else:\n",
    "                                backbone_neighbors_to_check.add(bb)\n",
    "                            new_backbone_neighbor_counter += 1\n",
    "\n",
    "                #9) if the counter is below the threshold --> Add all of values in the neighbros already checked list to the new_to_remove\n",
    "                if new_backbone_neighbor_counter < backbone_neighbor_min:\n",
    "                    for bz in checked_backbone_neighbors:\n",
    "                        if bz not in new_to_remove:\n",
    "                            if list_flag == True:\n",
    "                                new_to_remove.append(bz)\n",
    "                            else:\n",
    "                                new_to_remove.add(bz)\n",
    "                            #print(\"removed \" + str(checked_backbone_neighbors))\n",
    "                else:\n",
    "                    \n",
    "                    if list_flag == True:\n",
    "                        skip_labels = skip_labels + checked_backbone_neighbors\n",
    "                    else:\n",
    "                        skip_labels.update(checked_backbone_neighbors)\n",
    "                    \n",
    "     \n",
    "        #go through and switch the label of hte \n",
    "        #may not want to relabel until the end in order to preserve the labels in case label a big one wrong\n",
    "\n",
    "        for i in range(0,len(self.labels_list)):\n",
    "            if self.labels_list[i] in new_backbone_labels and self.labels_list[i] not in new_to_remove:\n",
    "                self.labels_list[i] = -1\n",
    "\n",
    "\n",
    "        #print(\"Done backbone extraction\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #used for when not pulling from datajoint\n",
    "    def get_cgal_data_and_label_local_optomized(self,ob_name,labels_file,sdf_file):\n",
    "        \n",
    "        #reads int the cgal labels for all of the faces\n",
    "        triangles_labels = np.zeros(len(self.mesh.faces)).astype(\"int64\")\n",
    "        with open(labels_file) as csvfile:\n",
    "            #print(\"inside labels file\")\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                triangles_labels[i] = int(row[0])\n",
    "\n",
    "        \"\"\" OLD WAY OF GETTING BLENDER MESH OBJECT\n",
    "        ob = bpy.context.object\n",
    "        me = ob.data\n",
    "        verts_raw = ob.data.vertices\n",
    "        faces_raw = ob.data.polygons\n",
    "        \"\"\"\n",
    "        \n",
    "        #converts the cgal labels into a list that\n",
    "        # starts at 0\n",
    "        # progresses in order for all unique labels (so no numbers are skipped and don't have corresponding face)\n",
    "        verts_raw = self.mesh.vertices\n",
    "        faces_raw = self.mesh.faces\n",
    "        #gets a list of the unique labels\n",
    "        unique_segments = list(Counter(triangles_labels).keys())\n",
    "        segmentation_length = len(unique_segments) \n",
    "        unique_index_dict = {unique_segments[x]:x for x in range(0,segmentation_length )}\n",
    "        \n",
    "        labels_list = np.zeros(len(triangles_labels)).astype(\"int64\")\n",
    "        for i,tri in enumerate(triangles_labels):\n",
    "\n",
    "            #assembles the label list that represents all of the faces\n",
    "            labels_list[i] = int(unique_index_dict[tri])\n",
    "        \n",
    "        #print(\"triangles_labels = \" + str(Counter(triangles_labels)))\n",
    "        #print(\"labels_list = \" + str(Counter(labels_list)))\n",
    "        \n",
    "\n",
    "        #print(\"done with cgal_segmentation\")\n",
    "\n",
    "        #----------------------now return a dictionary of the sdf values like in the older function get_sdf_dictionary\n",
    "        #get the sdf values and store in sdf_labels\n",
    "        sdf_labels = np.zeros(len(labels_list)).astype(\"float\")\n",
    "        with open(sdf_file) as csvfile:\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                sdf_labels[i] = float(row[0])\n",
    "\n",
    "        \n",
    "        sdf_temp_dict = {}\n",
    "        for i in range(0,segmentation_length):\n",
    "            sdf_temp_dict[i] = []\n",
    "        \n",
    "        #print(\"sdf_temp_dict = \" + str(sdf_temp_dict))\n",
    "        #print(\"sdf_labels = \" + str(sdf_labels))\n",
    "        #iterate through the labels_list\n",
    "        for i,label in enumerate(labels_list):\n",
    "            sdf_temp_dict[label].append(sdf_labels[i])\n",
    "        #print(sdf_temp_dict)\n",
    "\n",
    "        #now calculate the stats on the sdf values for each label\n",
    "        sdf_final_dict = {}\n",
    "        \n",
    "        for dict_key,value in sdf_temp_dict.items():\n",
    "\n",
    "            #just want to store the median\n",
    "            sdf_final_dict[dict_key] = np.median(value)\n",
    "\n",
    "        self.sdf_final_dict = sdf_final_dict\n",
    "        self.labels_list = labels_list\n",
    "        self.labels_list_counter = Counter(labels_list)\n",
    "    \n",
    "        adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "        \n",
    "        self.adjacency_labels_col1, self.adjacency_labels_col2 = adjacency_labels.T\n",
    "        \n",
    "        return \n",
    "\n",
    "    def filter_Stubs_optomized(self,stub_threshold):\n",
    "        \n",
    "        #update the adjacency labels graph and counter\n",
    "        adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "        self.labels_list_counter = Counter(self.labels_list)\n",
    "        \n",
    "        #feed into the networkx graph generator\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(adjacency_labels)\n",
    "        \n",
    "\n",
    "        #removes the backbone node\n",
    "        G.remove_node(-1)\n",
    "        \n",
    "        #get all of the sub graphs once backbone node is deleted\n",
    "        sub_graphs = nx.connected_component_subgraphs(G)\n",
    "\n",
    "        \n",
    "        labels_to_remove = []\n",
    "        for i, sg in enumerate(sub_graphs):\n",
    "            node_sum = sum([self.labels_list_counter[n] for n in sg.nodes() if n != -1])\n",
    "            if node_sum < stub_threshold:\n",
    "                labels_to_remove = labels_to_remove + list(sg.nodes())\n",
    "\n",
    "        print(f\"removing {len(labels_to_remove)} labels with stub threshold {stub_threshold}\")\n",
    "\n",
    "        self.labels_list[np.isin(self.labels_list,labels_to_remove)] = -1\n",
    "\n",
    "    def get_spine_classification(self,labels_file_location,file_name,clusters,smoothness,\n",
    "                                    smooth_backbone_parameters,stub_threshold=50): \n",
    "        \n",
    "        max_backbone_threshold = smooth_backbone_parameters.pop(\"max_backbone_threshold\",200) #the absolute size if it is greater than this then labeled as a possible backbone\n",
    "        backbone_threshold=smooth_backbone_parameters.pop(\"backbone_threshold\",40) #if the label meets the width requirements, these are the size requirements as well in order to be considered possible backbone\n",
    "        shared_vert_threshold=smooth_backbone_parameters.pop(\"shared_vert_threshold\",10) #raw number of backbone verts that need to be shared in order for label to possibly be a backbone\n",
    "        shared_vert_threshold_new = smooth_backbone_parameters.pop(\"shared_vert_threshAold_new\",5)\n",
    "        backbone_width_threshold = smooth_backbone_parameters.pop(\"backbone_width_threshold\",0.10)  #the median sdf/width value the segment has to have in order to be considered a possible backbone \n",
    "        backbone_neighbor_min=smooth_backbone_parameters.pop(\"smooth_backbone_parameters\",10) # number of backbones in chain in order for label to keep backbone status\n",
    "       \n",
    "        print(\"\\nbackbone Parameters\")\n",
    "        print(f\"max_backbone_threshold = {max_backbone_threshold}, \\\n",
    "                            backbone_threshold = {backbone_threshold}, \\\n",
    "                            shared_vert_threshold = {shared_vert_threshold}, \\\n",
    "                            shared_vert_threshold_new = {shared_vert_threshold_new} \\\n",
    "                             backbone_width_threshold = {backbone_width_threshold}, \\\n",
    "                             backbone_neighbor_min = {backbone_neighbor_min}\")\n",
    "        \n",
    "        print(\"\\nstub_threshold = \" + str(stub_threshold))\n",
    "        \n",
    "        original_start_time = time.time()    \n",
    "        start_time = time.time()\n",
    "\n",
    "        faces_raw = self.mesh.faces        \n",
    "        file_name = file_name[:-4]\n",
    "\n",
    "        labels_file = str(Path(labels_file_location) / Path(file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \".csv\" ))  \n",
    "        sdf_file = str(Path(labels_file_location) / Path(file_name + \"-cgal_\" + str(clusters) + \"_\" + str(smoothness) + \"_sdf.csv\" ))  \n",
    "        \n",
    "        #check to make sure thatcgal files were generated:\n",
    "        #clean up the cgal files \n",
    "        #clean up the cgal files \n",
    "        for f in [labels_file,sdf_file]:\n",
    "            if not os.path.isfile(f):\n",
    "                print(\"CGAL segmentation files weren't generated\")\n",
    "                raise ValueError(\"CGAL segmentation files weren't generated\")\n",
    "                return \"Failure\"\n",
    "        \n",
    "\n",
    "        self.get_cgal_data_and_label_local_optomized(file_name,labels_file,sdf_file)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(self.sdf_final_dict == [] and labels_list == []):\n",
    "            print(\"NO CGAL DATA FOR \" + str(neuron_ID))\n",
    "\n",
    "            return\n",
    "\n",
    "        print(\"getting cgal data--- %s seconds ---\" % (np.round(time.time() - start_time,5)))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.smooth_backbone_vp4_optomized(backbone_width_threshold,max_backbone_threshold = max_backbone_threshold,backbone_threshold=backbone_threshold,\n",
    "                shared_vert_threshold=shared_vert_threshold,\n",
    "                shared_vert_threshold_new = shared_vert_threshold_new,\n",
    "                 backbone_neighbor_min=backbone_neighbor_min)\n",
    "\n",
    "        \n",
    "        print(\"smoothing backbone--- %s seconds ---\" % (np.round(time.time() - start_time,5)))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.filter_Stubs_optomized(stub_threshold)\n",
    "        print(\"---removing stubs: %s seconds ---\" % (np.round(time.time() - start_time,5)))\n",
    "        \n",
    "        #clean up the cgal files \n",
    "        for f in [labels_file,sdf_file]:\n",
    "            os.remove(f)\n",
    "            \n",
    "        \n",
    "        #print(\"finished\")\n",
    "        print(\"Total spine extraction --- %s seconds ---\" % (np.round(time.time() - original_start_time,5)))\n",
    "        \n",
    "        status = \"Success\"\n",
    "        \n",
    "        return status\n",
    "    \n",
    "    \n",
    "    def extract_spines(self,labels_file_location,file_name,clusters,smoothness,\n",
    "                                       split_up_spines=True,shaft_mesh=False,**kwargs):\n",
    "        \n",
    "        \n",
    "        smooth_backbone_parameters = kwargs.pop('smooth_backbone_parameters', dict())\n",
    "        stub_threshold = kwargs.pop('stub_threshold', 50)\n",
    "        \n",
    "        \n",
    "        status = self.get_spine_classification(labels_file_location,file_name,clusters,\n",
    "                                      smoothness,smooth_backbone_parameters,stub_threshold)\n",
    "        \n",
    "        if status != \"Success\":\n",
    "            print(\"spine classification did not execute properly\")\n",
    "            return None\n",
    "        \n",
    "        spine_indexes = np.where(np.array(self.labels_list) != -1)\n",
    "        spine_meshes_whole = self.mesh.submesh(spine_indexes,append=True)\n",
    "        \n",
    "        #decides if passing back spines as one whole mesh or seperate meshes\n",
    "        if split_up_spines==True:\n",
    "            individual_spines = []\n",
    "            temp_spines = spine_meshes_wholes_whole.split(only_watertight=False)\n",
    "            for spine in temp_spines:\n",
    "                if len(spine.faces) >= stub_threshold:\n",
    "                    individual_spines.append(spine)\n",
    "        else:\n",
    "            individual_spines = spine_meshes_whole\n",
    "        \n",
    "        #will also pass back the shaft of the mesh with the extracted spines\n",
    "        if shaft_mesh==False:\n",
    "            return individual_spines\n",
    "        else:\n",
    "            shaft_indexes = np.where(np.array(self.labels_list) == -1) \n",
    "            shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "            return individual_spines,shaft_mesh_whole\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #divide into disconnected meshes and return this array\n",
    "        return individual_spines\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING SPINE EXTRACTION CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(\"./Dockerfile2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1555969925.372"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(time.time(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def complete_spine_extraction(mesh_file_location,\n",
    "                              file_name,\n",
    "                              **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts the spine meshes from a given dendritic mesh and returns either \n",
    "    just the spine meshes or the spine meshes and the dendritic shaft with the spines removed. \n",
    "  \n",
    "\n",
    "    Parameters: \n",
    "    mesh_file_location (str): location of the dendritic mesh on computer\n",
    "    file_name (str): file name of dendritic mesh on computer\n",
    "    \n",
    "    Optional Parameters:\n",
    "    ---configuring cgal segmentation ---\n",
    "    \n",
    "    clusters (int) : number of clusters to use for CGAL surface mesh segmentation (default = 12)\n",
    "    smoothness (int) : smoothness parameter use for CGAL surface mesh segmentation (default = 0.04)\n",
    "    \n",
    "    ---configuring output---\n",
    "    \n",
    "    split_up_spines (bool): if True will return array of trimesh objects representing each spine\n",
    "                         if False will return all spines as one mesh (default = True)\n",
    "    shaft_mesh (bool) : if True then returns the shaft mesh with the spines stripped out as well (default=False)\n",
    "    \n",
    "    --- configuring spine extraction ---\n",
    "    stub_threshold (int) : number of faces (size) that a spine mesh must include in order to be considered spine (default=50)\n",
    "                            \n",
    "    smooth_backbone_parameters (dict) : dict containing parameters for backbone extraction after cgal segmentation\n",
    "        ---- dictionary can contain the following parameters: ---\n",
    "        max_backbone_threshold (int) :the absolute size if it is greater than this then labeled as a possible backbone\n",
    "        (default = 200)\n",
    "        backbone_threshold (int) :if the label meets the width requirements, these are the size requirements as well in order to be considered possible backbone\n",
    "        (default = 40)\n",
    "        shared_vert_threshold (int): raw number of backbone verts that need to be shared in order for label to possibly be a backbone\n",
    "        (default = 10)\n",
    "        shared_vert_threshold_new (int): raw number of backbone verts that need to be shared in order for label to possibly be a backbone in phase 2\n",
    "        (default = 5)\n",
    "        backbone_width_threshold (float) :#the median sdf/width value the segment has to have in order to be considered a possible backbone \n",
    "        (default = 0.1)\n",
    "        backbone_neighbor_min (int): number of backbones in chain in order for label to keep backbone status\n",
    "        (default = 10)\n",
    "    -------------------------------------\n",
    "  \n",
    "    Returns: \n",
    "    1 or 2 trimesh.mesh objects/lists of objects depending on settings\n",
    "    \n",
    "    if split_up_spines == True (default)\n",
    "        list of trimesh.Mesh: each element in list is trimesh.mesh object representing a single spine\n",
    "    else:\n",
    "        trimesh.Mesh: trimesh.mesh object representing all spines\n",
    "    \n",
    "    if shaft_mesh == False (default):\n",
    "         No mesh object \n",
    "    else:\n",
    "        Trimesh.mesh object: representing shaft mesh with all of the spines filtered away\n",
    "        \n",
    "    \n",
    "    Examples:\n",
    "    #returns the spine meshes as one entire mesh\n",
    "    \n",
    "    list_of_spine_meshes = complete_spine_extraction(file_location,file_name)\n",
    "    list_of_spine_meshes,shaft_mesh = complete_spine_extraction(file_location,file_name,shaft_mesh=True)\n",
    "    merged_spine_meshes = complete_spine_extraction(file_location,file_name,split_up_spines=False)\n",
    "    merged_spine_meshes,shaft_mesh = complete_spine_extraction(file_location,file_name,split_up_spines=False,shaft_mesh=True)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    clusters = kwargs.pop('clusters', 12)\n",
    "    smoothness = kwargs.pop('smoothness', 0.04)\n",
    "    smooth_backbone_parameters = kwargs.pop('smooth_backbone_parameters', dict())\n",
    "    stub_threshold = kwargs.pop('stub_threshold', 50)\n",
    "    split_up_spines = kwargs.pop('split_up_spines', True)\n",
    "    shaft_mesh = kwargs.pop('shaft_mesh', False)\n",
    "    \n",
    "    \n",
    "    #making sure there is no more keyword arguments left that you weren't expecting\n",
    "    if kwargs:\n",
    "        raise TypeError('Unexpected **kwargs: %r' % kwargs)\n",
    "    \n",
    "\n",
    "    #check to see if file exists and if it is an off file\n",
    "    if file_name[-3:] != \"off\":\n",
    "        raise TypeError(\"input file must be a .off \")\n",
    "        return None\n",
    "    if not os.path.isfile(str(Path(mesh_file_location) / Path(file_name))):\n",
    "        raise TypeError(str(Path(mesh_file_location) / Path(file_name)) + \" cannot be found\")\n",
    "        return None\n",
    "    \n",
    "    total_time = time.time()\n",
    "    print(f\"Starting spine extraction for {file_name} with clusters={clusters} and smoothness={smoothness}\")\n",
    "    start_time = time.time()\n",
    "    myClassifier = ClassifyMesh(mesh_file_location,file_name)\n",
    "    print(f\"Step 1: Trimesh mesh build total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "    #make sure a cgal folder is created, and if not make one\n",
    "    \n",
    "    \n",
    "#     if (os.path.isdir(str(Path(os.getcwd()) / Path(\"cgal\")))) == False:\n",
    "#         os.chdir(str(Path(os.getcwd()) / Path(\"cgal\")))\n",
    "#         os.mkdir(\"cgal\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting CGAL segmentation\")\n",
    "    full_file_path = str(Path(mesh_file_location) / Path(file_name))[:-4]\n",
    "    csm.cgal_segmentation(full_file_path,clusters,smoothness)\n",
    "    print(f\"Step 2: CGAL segmentation total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "    \n",
    "    \n",
    "    #do the cgal processing\n",
    "    #labels_file_location = str(Path(os.getcwd()) / Path(\"cgal\"))\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting Spine Extraction\")\n",
    "    individual_spines = myClassifier.extract_spines(mesh_file_location,file_name,\n",
    "                                                    clusters,\n",
    "                                                    smoothness,\n",
    "                                                    split_up_spines,\n",
    "                                                    shaft_mesh,\n",
    "                                                   smooth_backbone_parameters=smooth_backbone_parameters,\n",
    "                                                   stub_threshold=stub_threshold\n",
    "                                                   )\n",
    "    print(f\"Step 3: Spine extraction total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "    \n",
    "    #clean of the cgal files from the computer\n",
    "    \n",
    "    \n",
    "    print(f\"Total time ---- {np.round(time.time() - total_time,5)} seconds\")\n",
    "    return individual_spines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting spine extraction for neuron-775959265587_part_4.off with clusters=12 and smoothness=0.04\n",
      "Step 1: Trimesh mesh build total time ---- 0.79953 seconds\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Step 2: CGAL segmentation total time ---- 21.09763 seconds\n",
      "\n",
      "Starting Spine Extraction\n",
      "\n",
      "backbone Parameters\n",
      "max_backbone_threshold = 100,                             backbone_threshold = 40,                             shared_vert_threshold = 12,                             shared_vert_threshold_new = 5                              backbone_width_threshold = 0.1,                              backbone_neighbor_min = 10\n",
      "\n",
      "stub_threshold = 70\n",
      "getting cgal data--- 0.32386 seconds ---\n",
      "smoothing round 1\n",
      "smoothing round 2\n",
      "smoothing round 3\n",
      "smoothing backbone--- 0.44394 seconds ---\n",
      "removing 77 labels with stub threshold 70\n",
      "---removing stubs: 0.35579 seconds ---\n",
      "Total spine extraction --- 1.12432 seconds ---\n",
      "Step 3: Spine extraction total time ---- 1.21232 seconds\n",
      "Total time ---- 23.11007 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mesh_file_location = \"/notebooks/18_Spine_Allen/neurons\"\n",
    "\n",
    "clusters = 12\n",
    "smoothness = 0.04\n",
    "\n",
    "file_name = \"neuron-775959265587_part_4.off\"\n",
    "part_2_spines,shaft = complete_spine_extraction(mesh_file_location,file_name,\n",
    "                                                clusters=clusters,\n",
    "                                                smoothness=smoothness,\n",
    "                                                split_up_spines = False,\n",
    "                                                shaft_mesh = True,\n",
    "                                                smooth_backbone_parameters=dict(max_backbone_threshold=100,shared_vert_threshold=12),\n",
    "                                                stub_threshold=70)\n",
    "# file_name = \"neuron-775959265587_part_3.off\"\n",
    "# part_3_spines = complete_spine_extraction(mesh_file_location,file_name,clusters,smoothness)\n",
    "# file_name = \"neuron-775959265587_part_4.off\"\n",
    "# part_4_spines = complete_spine_extraction(mesh_file_location,file_name,clusters,smoothness)\n",
    "# file_name = \"neuron-775959265587-part-1.off\"\n",
    "# part_1_spines = complete_spine_extraction(mesh_file_location,file_name,clusters,smoothness)\n",
    "\n",
    "\n",
    "\n",
    "# myClassifier2 = ClassifyMesh(mesh_file_location,file_name)\n",
    "# myClassifier2.get_spine_classification(labels_file_location,file_name,clusters,smoothness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that correctly got the spine meshes\n",
    "with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "    part_2_spines.export(\"./neurons/TEST_2_part_2_spines_whole.off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Trimesh' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-1a3866ae3852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmesh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart_2_spines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Trimesh' object is not iterable"
     ]
    }
   ],
   "source": [
    "for mesh in part_2_spines:\n",
    "    if len(mesh.faces) < 70:\n",
    "        print(len(mesh.faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting cgal data--- 0.7689657211303711 seconds ---\n",
      "smoothing round 1\n",
      "smoothing round 2\n",
      "smoothing round 3\n",
      "smoothing round 4\n",
      "smoothing backbone--- 2.9414358139038086 seconds ---\n",
      "removing 131 labels with stub threshold 50\n",
      "---removing stubs: 0.7735898494720459 seconds ---\n",
      "finished\n",
      "--- 4.484267234802246 seconds ---\n",
      "getting cgal data--- 1.0582191944122314 seconds ---\n",
      "smoothing round 1\n",
      "smoothing round 2\n",
      "smoothing round 3\n",
      "smoothing backbone--- 3.8706977367401123 seconds ---\n",
      "removing 110 labels with stub threshold 50\n",
      "---removing stubs: 1.0961213111877441 seconds ---\n",
      "finished\n",
      "--- 6.025381803512573 seconds ---\n",
      "getting cgal data--- 0.2942039966583252 seconds ---\n",
      "smoothing round 1\n",
      "smoothing round 2\n",
      "smoothing round 3\n",
      "smoothing backbone--- 0.438645601272583 seconds ---\n",
      "removing 67 labels with stub threshold 50\n",
      "---removing stubs: 0.30386948585510254 seconds ---\n",
      "finished\n",
      "--- 1.036958932876587 seconds ---\n",
      "getting cgal data--- 0.30161333084106445 seconds ---\n",
      "smoothing round 1\n",
      "smoothing round 2\n",
      "smoothing backbone--- 0.171370267868042 seconds ---\n",
      "removing 76 labels with stub threshold 50\n",
      "---removing stubs: 0.31806159019470215 seconds ---\n",
      "finished\n",
      "--- 0.7912273406982422 seconds ---\n"
     ]
    }
   ],
   "source": [
    "mesh_file_location = \"/notebooks/18_Spine_Allen/neurons\"\n",
    "\n",
    "clusters = 12\n",
    "smoothness = 0.04\n",
    "\n",
    "file_name = \"neuron-775959265587_part_2.off\"\n",
    "part_2_spines_whole = complete_spine_extraction(mesh_file_location,file_name,clusters,smoothness)\n",
    "file_name = \"neuron-775959265587_part_3.off\"\n",
    "part_3_spines_whole = complete_spine_extraction(mesh_file_location,file_name,clusters,smoothness)\n",
    "file_name = \"neuron-775959265587_part_4.off\"\n",
    "part_4_spines_whole = complete_spine_extraction(mesh_file_location,file_name,clusters,smoothness)\n",
    "file_name = \"neuron-775959265587-part-1.off\"\n",
    "part_1_spines_whole = complete_spine_extraction(mesh_file_location,file_name,clusters,smoothness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import contextlib\n",
    "\n",
    "with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "    part_2_spines_whole.export(\"./neurons/part_2_spines_whole.off\")\n",
    "    part_3_spines_whole.export(\"./neurons/part_3_spines_whole.off\")\n",
    "    part_4_spines_whole.export(\"./neurons/part_4_spines_whole.off\")\n",
    "    part_1_spines_whole.export(\"./neurons/part_1_spines_whole.off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Trimesh' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d792b1cadb44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmesh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart_2_spines_whole\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"greater\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Trimesh' object is not iterable"
     ]
    }
   ],
   "source": [
    "for mesh in part_2_spines_whole:\n",
    "    if len(mesh.faces) > 50:\n",
    "        print(\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "THINGS TO ADD:\n",
    "parameter access (like stub thresholding, clusters, ect)\n",
    "option where passes back the stripped out spines as well\n",
    "\n",
    "Have a second pass for stub threshold to make sure they don't get passed back\n",
    "There are cuts in some of the meshes that throws off the segmentation beware\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTesting out the full soma extraction\\n\\nPseudocode for Algorithm: \\nLoad in mesh\\nSplit mesh into largest pieces: \\n    Iterate through all mesh pieces of a certain threshold\\n    Do the Poisson surface reconstruction:\\n    Find all the mesh pieces of a certain threshold:\\n        (Optional step) Run the screened poisson surface reconstruction\\n        Run the segmentation algorithm\\n        Identify all somas\\n        Save of the soma meshes\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing out the full soma extraction\n",
    "\n",
    "Pseudocode for Algorithm: \n",
    "Load in mesh\n",
    "Split mesh into largest pieces: \n",
    "    Iterate through all mesh pieces of a certain threshold\n",
    "    Do the Poisson surface reconstruction:\n",
    "    Find all the mesh pieces of a certain threshold:\n",
    "        (Optional step) Run the screened poisson surface reconstruction\n",
    "        Run the segmentation algorithm\n",
    "        Identify all somas\n",
    "        Save of the soma meshes\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "from whole_neuron_classifier_datajoint_adapted import extract_branches_whole_neuron\n",
    "import whole_neuron_classifier_datajoint_adapted as wcda \n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_meshlab_script(mlx_script,input_mesh_file,output_mesh_file):\n",
    "    script_command = (\" -i \" + str(input_mesh_file) + \" -o \" + \n",
    "                                    str(output_mesh_file) + \" -s \" + str(mlx_script))\n",
    "    #return script_command\n",
    "    command_to_run = 'xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@ ' + script_command\n",
    "    #command_to_run = 'meshlabserver ' + script_command\n",
    "    \n",
    "    print(command_to_run)\n",
    "    subprocess_result = subprocess.run(command_to_run,shell=True)\n",
    "    \n",
    "    return subprocess_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess\n",
    "def meshlab_fix_manifold_path_specific_mls(input_path_and_filename,\n",
    "                                           output_path_and_filename=\"\",\n",
    "                                           segment_id=-1,meshlab_script=\"\"):\n",
    "    #fix the path if it comes with the extension\n",
    "    if input_path_and_filename[-4:] == \".off\":\n",
    "        path_and_filename = input_path_and_filename[:-4]\n",
    "        input_mesh = input_path_and_filename\n",
    "    else:\n",
    "        raise Exception(\"Not passed off file\")\n",
    "    \n",
    "    \n",
    "    if output_path_and_filename == \"\":\n",
    "        output_mesh = path_and_filename+\"_mls.off\"\n",
    "    else:\n",
    "        output_mesh = output_path_and_filename\n",
    "    \n",
    "    if meshlab_script == \"\":\n",
    "        meshlab_script = str(pathlib.Path.cwd()) + \"/\" + \"remeshing_remove_non_man_edges.mls\"\n",
    "    \n",
    "    #print(\"meshlab_script = \" + str(meshlab_script))\n",
    "    #print(\"starting meshlabserver fixing non-manifolds\")\n",
    "    subprocess_result_1 = run_meshlab_script(meshlab_script,\n",
    "                      input_mesh,\n",
    "                      output_mesh)\n",
    "    #print(\"Poisson subprocess_result= \"+ str(subprocess_result_1))\n",
    "    \n",
    "    if str(subprocess_result_1)[-13:] != \"returncode=0)\":\n",
    "        raise Exception('neuron' + str(segment_id) + \n",
    "                         ' did not fix the manifold edges')\n",
    "    \n",
    "    return output_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def run_poisson_surface_reconstruction(pre_largest_mesh_path,\n",
    "                                       segment_id = \"None\",\n",
    "                                      script_name = \"poisson_working_meshlab.mls\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Will run the poisson surface reconstruction\n",
    "    \n",
    "    \"\"\"\n",
    "    # run the meshlab server script\n",
    "\n",
    "    meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "    input_path =str(pathlib.Path.cwd()) + \"/\" +  pre_largest_mesh_path\n",
    "\n",
    "    indices = [i for i, a in enumerate(input_path) if a == \"_\"]\n",
    "    stripped_ending = input_path[:-4]\n",
    "\n",
    "    output_path = stripped_ending + \"_mls.off\"\n",
    "    # print(meshlab_script_path_and_name)\n",
    "    # print(input_path)\n",
    "    # print(output_path)\n",
    "    print(\"Running the mls function\")\n",
    "    meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_path,\n",
    "                                               output_path_and_filename=output_path,\n",
    "                                               segment_id=segment_id,\n",
    "                                               meshlab_script=meshlab_script_path_and_name)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decimation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate_mesh_from_verts_faces(vertices,faces,segment_id,current_folder,decimation_ratio):\n",
    "    #write the file to the temp folder\n",
    "    input_file_base = write_Whole_Neuron_Off_file(vertices,faces,segment_id,folder=current_folder)\n",
    "    output_file = input_file_base + \"_decimated\"\n",
    "    \n",
    "    \n",
    "    if decimation_ratio <= 0:\n",
    "        script_name = \"decimation_meshlab_0_25.mls\"\n",
    "    else:\n",
    "        script_name = f\"decimation_meshlab_0_{str(decimation_ratio)[2:4]}.mls\"\n",
    "    meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "\n",
    "\n",
    "    meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_file_base + \".off\",\n",
    "                                                       output_path_and_filename=output_file + \".off\",\n",
    "                                                       meshlab_script=meshlab_script_path_and_name)\n",
    "    \n",
    "    #read in the output mesh and return the vertices and faces\n",
    "    current_mesh = trimesh.load_mesh(output_file + '.off')\n",
    "    \n",
    "    #check if file exists and then delete the temporary decimated mesh filess\n",
    "    if os.path.exists(input_file_base + \".off\"):\n",
    "        os.remove(input_file_base + \".off\")\n",
    "    if os.path.exists(output_file + \".off\"):\n",
    "        os.remove(output_file + \".off\")\n",
    " \n",
    "    return current_mesh.vertices,current_mesh.faces,output_file+\".off\"\n",
    "\n",
    "def decimate_mesh_from_path(arg_path,decimation_ratio=0):\n",
    "    #write the file to the temp folder\n",
    "    input_file_base = arg_path[:-4]\n",
    "    output_file = input_file_base + \"_decimated\"\n",
    "    \n",
    "    \n",
    "    if decimation_ratio <= 0:\n",
    "        script_name = \"decimation_meshlab_0_25.mls\"\n",
    "    else:\n",
    "        script_name = f\"decimation_meshlab_0_{str(decimation_ratio)[2:4]}.mls\"\n",
    "    meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "    \n",
    "\n",
    "\n",
    "    meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_file_base + \".off\",\n",
    "                                                       output_path_and_filename=output_file + \".off\",\n",
    "                                                       meshlab_script=meshlab_script_path_and_name)\n",
    "    \n",
    "    #read in the output mesh and return the vertices and faces\n",
    "    current_mesh = trimesh.load_mesh(output_file + '.off')\n",
    "    \n",
    "#     #check if file exists and then delete the temporary decimated mesh filess\n",
    "#     if os.path.exists(input_file_base + \".off\"):\n",
    "#         os.remove(input_file_base + \".off\")\n",
    "#     if os.path.exists(output_file + \".off\"):\n",
    "#         os.remove(output_file + \".off\")\n",
    " \n",
    "    return current_mesh,output_file+\".off\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decimation that allows you to specify the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPart that needs to get added: \\n\\n<Param type=\"RichFloat\" value=\"0.25\" name=\"TargetPerc\" />\\n</filter>\\n\\n</FilterScript>\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part that needs to get added: \n",
    "\n",
    "<Param type=\"RichFloat\" value=\"0.25\" name=\"TargetPerc\" />\n",
    "</filter>\n",
    "\n",
    "</FilterScript>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1) Import mesh and find all the significant pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 450_big_neuron_dec_0_25.off\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting up the mesh file and the output files\n",
    "\"\"\"\n",
    "\n",
    "total_test_meshes = [\n",
    "'110778132960975016_stitched.off',\n",
    "\"450_big_neuron_dec_0_25.off\"]\n",
    "\n",
    "output_file = total_test_meshes[1]\n",
    "folder_name = \"soma_extraction_tests_vp1/\" \n",
    "\n",
    "output_mesh_name = folder_name + output_file\n",
    "print(f\"Working on {output_file}\")\n",
    "\n",
    "#segment_id_stripped = \"450\"\n",
    "indices = [i for i, a in enumerate(output_file) if a == \"_\"]\n",
    "indices\n",
    "seg_id_stripped = output_file[:indices[0]]\n",
    "n = dict(segment_id=int(seg_id_stripped))\n",
    "segment_id = int(seg_id_stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) Run the loop that does soma identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 150000.0 \n",
      "large_mesh_threshold_inner = 10000.0 \n",
      "soma_size_threshold = 1250.0\\outer_decimation_ratio = 0.25\\inner_decimation_ratio = 0.25\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25.off -o /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_decimated.off -s /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/decimation_meshlab_0_25.mls\n",
      "decimated_output_mesh_file = /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_decimated.off\n",
      "Total found significant pieces before Poisson = [<trimesh.Trimesh(vertices.shape=(430196, 3), faces.shape=(858554, 3))>]\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(430196, 3), faces.shape=(858554, 3))>\n",
      "done exporting\n",
      "Running the mls function\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_0_largest_piece.off -o /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_0_largest_piece_mls.off -s /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/poisson_working_meshlab.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(619123, 3), faces.shape=(1238422, 3))>]\n",
      "stripped_ending 2 = /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_0_largest_piece_mls\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(619123, 3), faces.shape=(1238422, 3))>\n",
      "done exporting /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_0_largest_piece_mls_0_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_0_largest_piece_mls_0_largest_inner.off -o /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_0_largest_piece_mls_0_largest_inner_decimated.off -s /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/soma_extraction_tests_vp1/450_big_neuron_dec_0_25_0_largest_piece_mls_0_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0003657341003417969\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Soma_Finder_2_26/temp/45000_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 85.7809693813324\n",
      "2) Finished: Generating CGAL segmentation for neuron: 91.5560655593872\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 12\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.11357641220092773\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 0.0007951259613037109\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.3150501251220703\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 2.183669090270996\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([12,  0, 23, 24, 11, 14, 25, 19,  2, 21, 18, 15, 13, 20, 17, 22, 16,\n",
      "        6,  4,  1, 26,  5, 10,  8,  9,  7,  3]), array([0.77128   , 0.2712745 , 0.1410175 , 0.119128  , 0.1155105 ,\n",
      "       0.106578  , 0.0925298 , 0.0728267 , 0.062903  , 0.0546242 ,\n",
      "       0.0519964 , 0.04851615, 0.0470944 , 0.043968  , 0.0432086 ,\n",
      "       0.04234175, 0.0414511 , 0.04107265, 0.0401327 , 0.03912145,\n",
      "       0.0380531 , 0.0379078 , 0.0349036 , 0.0348715 , 0.0347063 ,\n",
      "       0.0337527 , 0.03058855]))\n",
      "Sizes = [20577, 22574, 1538, 1781, 5346, 2981, 2075, 547, 26845, 628, 5572, 7512, 15677, 683, 2233, 622, 6001, 21044, 26199, 18702, 151, 61135, 8685, 17293, 1304, 25081, 6818]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [12]\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 264.3930277824402\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loop that will compute the soma meshes and locations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ------------parameters------------------\n",
    "\n",
    "import time\n",
    "global_start_time = time.time()\n",
    "\n",
    "\"\"\"\n",
    "Working for larger neuron: \n",
    "\n",
    "\n",
    "Try 1: With 0.25 decimation\n",
    "outer_decimation_ratio= 0.25\n",
    "large_mesh_threshold = 600000\n",
    "large_mesh_threshold_inner = 40000\n",
    "soma_width_threshold = 0.32\n",
    "soma_size_threshold = 20000\n",
    "\n",
    "large_mesh_threshold = large_mesh_threshold*outer_decimation_ratio\n",
    "large_mesh_threshold_inner = large_mesh_threshold_inner*outer_decimation_ratio\n",
    "soma_size_threshold = soma_size_threshold*outer_decimation_ratio\n",
    "\n",
    "inner_decimation_ratio = 0.25\n",
    "soma_size_threshold = soma_size_threshold*inner_decimation_ratio\n",
    "\n",
    "Faster at: 467.977885723114 seconds\n",
    "\n",
    "\n",
    "--------------------   Try 2: Using 0.05 decimation for outer layer\n",
    "\n",
    "Try 1: With 0.25 decimation\n",
    "outer_decimation_ratio= 0.05\n",
    "large_mesh_threshold = 1800000\n",
    "large_mesh_threshold_inner = 40000\n",
    "soma_width_threshold = 0.32\n",
    "soma_size_threshold = 20000\n",
    "\n",
    "large_mesh_threshold = large_mesh_threshold*outer_decimation_ratio\n",
    "large_mesh_threshold_inner = large_mesh_threshold_inner*outer_decimation_ratio\n",
    "soma_size_threshold = soma_size_threshold*outer_decimation_ratio\n",
    "\n",
    "inner_decimation_ratio = 0.25\n",
    "soma_size_threshold = soma_size_threshold*inner_decimation_ratio\n",
    "\n",
    "                what it turned out to be ------\n",
    " large_mesh_threshold= 30000.0 \n",
    "large_mesh_threshold_inner = 2000.0 \n",
    "soma_size_threshold = 250.0\\outer_decimation_ratio = 0.05\\inner_decimation_ratio = 0.25\n",
    "\n",
    "\n",
    "The 0.05 decimation took almost 597 seconds\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "outer_decimation_ratio= 0.25\n",
    "large_mesh_threshold = 600000\n",
    "large_mesh_threshold_inner = 40000\n",
    "soma_width_threshold = 0.32\n",
    "soma_size_threshold = 20000\n",
    "\n",
    "large_mesh_threshold = large_mesh_threshold*outer_decimation_ratio\n",
    "large_mesh_threshold_inner = large_mesh_threshold_inner*outer_decimation_ratio\n",
    "soma_size_threshold = soma_size_threshold*outer_decimation_ratio\n",
    "\n",
    "inner_decimation_ratio = 0.25\n",
    "soma_size_threshold = soma_size_threshold*inner_decimation_ratio\n",
    "\n",
    "print(f\"Current Arguments Using (adjusted for decimation):\\n large_mesh_threshold= {large_mesh_threshold}\"\n",
    "             f\" \\nlarge_mesh_threshold_inner = {large_mesh_threshold_inner} \\nsoma_size_threshold = {soma_size_threshold}\"\n",
    "             f\"\\outer_decimation_ratio = {outer_decimation_ratio}\"\n",
    "             f\"\\inner_decimation_ratio = {inner_decimation_ratio}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "\"\"\" New step: add decimation: \n",
    "\"\"\"\n",
    "full_output_mesh_name = str(pathlib.Path.cwd()) + \"/\" + output_mesh_name\n",
    "\n",
    "new_mesh,decimated_output_mesh_file = decimate_mesh_from_path(full_output_mesh_name,\n",
    "                                                           decimation_ratio=outer_decimation_ratio)\n",
    "\n",
    "print(\"decimated_output_mesh_file = \" + str(decimated_output_mesh_file))\n",
    "\n",
    "new_mesh = trimesh.load_mesh(output_mesh_name)\n",
    "mesh_splits = new_mesh.split(only_watertight=False)\n",
    "\n",
    "#len(\"Total mesh splits = \" + str(mesh_splits))\n",
    "#get the largest mesh\n",
    "mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# sns.distplot(mesh_lengths)\n",
    "\n",
    "# largest_index = np.where(mesh_lengths == np.max(mesh_lengths))\n",
    "# largest_mesh = mesh_splits[largest_index][0]\n",
    "\n",
    "\"\"\" -- temporarily changing to the second largest mesh\"\"\"\n",
    "total_mesh_split_lengths = [len(k.faces) for k in mesh_splits]\n",
    "ordered_mesh_splits = mesh_splits[np.flip(np.argsort(total_mesh_split_lengths))]\n",
    "list_of_largest_mesh = [k for k in ordered_mesh_splits if len(k.faces) > large_mesh_threshold]\n",
    "\n",
    "print(f\"Total found significant pieces before Poisson = {list_of_largest_mesh}\")\n",
    "\n",
    "# total_soma_mesh = trimesh.Trimesh(vertices=np.array([]),\n",
    "#                                  triangles = np.array([]))\n",
    "\n",
    "total_soma_list = []\n",
    "total_classifier_list = []\n",
    "total_poisson_list = []\n",
    "\n",
    "#start iterating through where go through all pieces before the poisson reconstruction\n",
    "no_somas_found_in_big_loop = 0\n",
    "for i,largest_mesh in enumerate(list_of_largest_mesh):\n",
    "    print(f\"----- working on large mesh #{i}: {largest_mesh}\")\n",
    "    \n",
    "    somas_found_in_big_loop = False\n",
    "\n",
    "    stripped_ending = output_mesh_name[:-4]\n",
    "    pre_largest_mesh_path = stripped_ending + \"_\" + str(i) + \"_largest_piece.off\"\n",
    "\n",
    "    largest_mesh.export(pre_largest_mesh_path)\n",
    "    print(\"done exporting\")\n",
    "    \n",
    "    output_path = run_poisson_surface_reconstruction(pre_largest_mesh_path)\n",
    "    \n",
    "    #---------------- Will carry out the cgal segmentation -------- #\n",
    "    #import the mesh\n",
    "    new_mesh_inner = trimesh.load_mesh(output_path)\n",
    "    \n",
    "    mesh_splits_inner = new_mesh_inner.split(only_watertight=False)\n",
    "    total_mesh_split_lengths_inner = [len(k.faces) for k in mesh_splits_inner]\n",
    "    ordered_mesh_splits_inner = mesh_splits_inner[np.flip(np.argsort(total_mesh_split_lengths_inner))]\n",
    "    #print(f\"Mesh lengths of inner after split: {[len(k.faces) for k in ordered_mesh_splits_inner]}\")\n",
    "\n",
    "    list_of_largest_mesh_inner = [k for k in ordered_mesh_splits_inner if len(k.faces) > large_mesh_threshold_inner]\n",
    "    print(f\"Total found significant pieces AFTER Poisson = {list_of_largest_mesh_inner}\")\n",
    "    \n",
    "    stripped_ending = output_path[:-4]\n",
    "    print(f\"stripped_ending 2 = {stripped_ending}\")\n",
    "    n_failed_inner_soma_loops = 0\n",
    "    for j, largest_mesh_inner in enumerate(list_of_largest_mesh_inner):\n",
    "\n",
    "        print(f\"----- working on mesh after poisson #{j}: {largest_mesh_inner}\")\n",
    "        \n",
    "        largest_mesh_path_inner = stripped_ending +\"_\" + str(j) + \"_largest_inner.off\"\n",
    "\n",
    "        #DON'T NEED THIS WRITE NOW BECAUSE IT ALREADY OUTPUTS THE MESH\n",
    "        largest_mesh_inner.export(largest_mesh_path_inner)\n",
    "        print(f\"done exporting {largest_mesh_path_inner}\")\n",
    "        \n",
    "        largest_mesh_path_inner_decimated,decimated_output_mesh_file_inner = decimate_mesh_from_path(largest_mesh_path_inner,\n",
    "                                                                   decimation_ratio=inner_decimation_ratio)\n",
    "        largest_mesh_path_inner_decimated.export(largest_mesh_path_inner[:-4] + \"_decimated.off\")\n",
    "        print(f\"done exporting decimated mesh: {largest_mesh_path_inner}\")\n",
    "        # Starts the actual cgal segmentation:\n",
    "        \n",
    "        faces = np.array(largest_mesh_path_inner_decimated.faces)\n",
    "        verts = np.array(largest_mesh_path_inner_decimated.vertices)\n",
    "        #run the whole algorithm on the neuron to test\n",
    "        segment_id_new = int(str(segment_id) + f\"{i}{j}\")\n",
    "        verts_labels, faces_labels, soma_value,classifier = wcda.extract_branches_whole_neuron(\n",
    "                            import_Off_Flag=False,\n",
    "                            segment_id=segment_id_new,\n",
    "                            vertices=verts,\n",
    "                             triangles=faces,\n",
    "                            pymeshfix_Flag=False,\n",
    "                             import_CGAL_Flag=False,\n",
    "                             return_Only_Labels=True,\n",
    "                             clusters=3,\n",
    "                             smoothness=0.2,\n",
    "                            soma_only=True,\n",
    "                            return_classifier = True\n",
    "                            )\n",
    "        \n",
    "        total_classifier_list.append(classifier)\n",
    "        #total_poisson_list.append(largest_mesh_path_inner_decimated)\n",
    "\n",
    "        # Save all of the portions that resemble a soma\n",
    "        median_values = np.array([v[\"median\"] for k,v in classifier.sdf_final_dict.items()])\n",
    "        segmentation = np.array([k for k,v in classifier.sdf_final_dict.items()])\n",
    "\n",
    "        #order the compartments by greatest to smallest\n",
    "        sorted_medians = np.flip(np.argsort(median_values))\n",
    "        print(f\"segmentation[sorted_medians],median_values[sorted_medians] = {(segmentation[sorted_medians],median_values[sorted_medians])}\")\n",
    "        print(f\"Sizes = {[classifier.sdf_final_dict[g]['n_faces'] for g in segmentation[sorted_medians]]}\")\n",
    "        \n",
    "        valid_soma_segments_width = [g for g,h in zip(segmentation[sorted_medians],median_values[sorted_medians]) if ((h > soma_width_threshold)\n",
    "                                                            and (classifier.sdf_final_dict[g][\"n_faces\"] > soma_size_threshold))]\n",
    "        \n",
    "        print(\"valid_soma_segments_width\")\n",
    "        to_add_list = []\n",
    "        if len(valid_soma_segments_width) > 0:\n",
    "            print(f\"      ------ Found {len(valid_soma_segments_width)} viable somas: {valid_soma_segments_width}\")\n",
    "            somas_found_in_big_loop = True\n",
    "            #get the meshes only if signfiicant length\n",
    "            labels_list = classifier.labels_list\n",
    "            \n",
    "            for v in valid_soma_segments_width:\n",
    "                submesh_face_list = np.where(classifier.labels_list == v)[0]\n",
    "                soma_mesh = largest_mesh_path_inner_decimated.submesh([submesh_face_list],append=True)\n",
    "                to_add_list.append(soma_mesh)\n",
    "\n",
    "            n_failed_inner_soma_loops = 0\n",
    "            \n",
    "        else:\n",
    "            n_failed_inner_soma_loops += 1\n",
    "        \n",
    "        total_soma_list.append(to_add_list)\n",
    "        \n",
    "        # --------------- KEEP TRACK IF FAILED TO FIND SOMA (IF TOO MANY FAILS THEN BREAK)\n",
    "        if n_failed_inner_soma_loops >= 2:\n",
    "            print(\"breaking inner loop because 2 soma fails in a row\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # --------------- KEEP TRACK IF FAILED TO FIND SOMA (IF TOO MANY FAILS THEN BREAK)\n",
    "    if somas_found_in_big_loop == False:\n",
    "        no_somas_found_in_big_loop += 1\n",
    "        if no_somas_found_in_big_loop >= 2:\n",
    "            print(\"breaking because 2 fails in a row in big loop\")\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        no_somas_found_in_big_loop = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    large_mesh_threshold= 150000.0 \n",
    "    large_mesh_threshold_inner = 10000.0 \n",
    "    soma_size_threshold = 1250.0\n",
    "    decimation_ratio = 0.05\n",
    "    \"\"\"\n",
    "    \n",
    "print(f\"\\n\\n\\n Total time for run = {time.time() - global_start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soma_extraction_tests_vp1/450_big_neuron_dec_0_25.off'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_mesh_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "new_somas = trimesh.Trimesh(vertices=np.array([]), faces=np.array([]))\n",
    "for s in total_soma_list:\n",
    "    new_somas += s\n",
    "new_somas.export(\"450_big_neuron_dec_0_25_SOMAS.off\")\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savez(\"saved_4_neuron_mesh.npz\",total_soma_list = total_soma_list,total_classifier_list = total_classifier_list, total_poisson_list = total_poisson_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

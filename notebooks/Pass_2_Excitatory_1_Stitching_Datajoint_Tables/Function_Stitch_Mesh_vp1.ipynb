{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pymeshfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Brainstorming input output functions\n",
    "\n",
    "Inputs:\n",
    "1) segment_id\n",
    "2) vertices\n",
    "3) triangles\n",
    "\n",
    "Important parameters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For generating the Main mesh and child meshes from the full mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mesh_significant_outside_pieces(unfiltered_mesh,significance_threshold=2000,n_sample_points=1000):\n",
    "    \"\"\"\n",
    "    Purpose; will take in a full, unfiltered mesh and find the biggest mesh piece, and then return a list of that mesh \n",
    "    with all of the other mesh fragments that are both above the significance_threshold AND outside of the biggest mesh piece\n",
    "\n",
    "    Pseudocode: \n",
    "    1) split the meshes to unconnected pieces\n",
    "    2) Filter the meshes for only those above the significance_threshold\n",
    "    3) find the biggest mesh piece\n",
    "    4) Iterate through all of the remaining pieces:\n",
    "        a. Determine if mesh inside or outside main mesh\n",
    "        b. If outside add to final list to return\n",
    "\n",
    "    Returns: \n",
    "    1) list of significant mesh pieces, including the main one that are not inside of main mesh\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mesh_pieces = unfiltered_mesh.split(only_watertight=False)\n",
    "    \n",
    "    print(f\"There were {len(mesh_pieces)} pieces after mesh split\")\n",
    "\n",
    "    significant_pieces = [m for m in mesh_pieces if len(m.faces) > significance_threshold]\n",
    "\n",
    "    print(f\"There were {len(significant_pieces)} pieces found after size threshold\")\n",
    "    if len(significant_pieces) <=0:\n",
    "        print(\"THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\")\n",
    "        return []\n",
    "\n",
    "    #find piece with largest size\n",
    "    max_index = 0\n",
    "    max_face_len = len(significant_pieces[max_index].faces)\n",
    "\n",
    "    for i in range(1,len(significant_pieces)):\n",
    "        if max_face_len < len(significant_pieces[i].faces):\n",
    "            max_index = i\n",
    "            max_face_len = len(significant_pieces[i].faces)\n",
    "\n",
    "    print(\"max_index = \" + str(max_index))\n",
    "    print(\"max_face_len = \" + str(max_face_len))\n",
    "\n",
    "    final_mesh_pieces = []\n",
    "\n",
    "    main_mesh = significant_pieces[max_index]\n",
    "\n",
    "    #final_mesh_pieces.append(main_mesh)\n",
    "    for i,mesh in enumerate(significant_pieces):\n",
    "        if i != max_index:\n",
    "            #get a random sample of points\n",
    "            # points = np.array(mesh.vertices[:n_sample_points,:]) # OLD WAY OF DOING THIS\n",
    "            idx = np.random.randint(len(mesh.vertices), size=n_sample_points)\n",
    "            points = mesh.vertices[idx,:]\n",
    "            \n",
    "            \n",
    "            start_time = time.time()\n",
    "            signed_distance = trimesh.proximity.signed_distance(main_mesh,points)\n",
    "            #print(f\"Total time = {time.time() - start_time}\")\n",
    "\n",
    "            outside_percentage = sum(signed_distance < 0)/n_sample_points\n",
    "            if outside_percentage > 0.9:\n",
    "                final_mesh_pieces.append(mesh)\n",
    "                #print(f\"Mesh piece {i} OUTSIDE mesh\")\n",
    "            else:\n",
    "                #print(f\"Mesh piece {i} inside mesh :( \")\n",
    "                pass\n",
    "                \n",
    "    return main_mesh,final_mesh_pieces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facet Generating Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def area(vertices):\n",
    "    \"\"\"\n",
    "    Calculates the area of a 3D triangle from it's coordinates\n",
    "    \"\"\"\n",
    "    side_a = np.linalg.norm(vertices[0]-vertices[1])\n",
    "    side_b = np.linalg.norm(vertices[1]-vertices[2])\n",
    "    side_c = np.linalg.norm(vertices[2]-vertices[0])\n",
    "    s = 0.5 * ( side_a + side_b + side_c)\n",
    "    return math.sqrt(s * (s - side_a) * (s - side_b) * (s - side_c))\n",
    "\n",
    "def find_polygon_area(mesh,list_of_faces):\n",
    "    \"Calculates the area of a 3D polygon that is created from connected traingles\"\n",
    "    return(sum([area(mesh.vertices[mesh.faces[r]]) for r in list_of_faces]))\n",
    "\n",
    "\n",
    "#filters by convexity and generates the facet centers\n",
    "\n",
    "def filter_final_facets_working(gap_mesh,final_facets,adjacency_threshold =  0.8):\n",
    "    \"\"\"\n",
    "    Gets the facets faces list and the center points of these facets from mesh\n",
    "    Filters:\n",
    "    1) Only lets facets greater than first_pass_size_threshold exist\n",
    "      **** might need to look at this because area is that of facets before expansion\n",
    "    2) Has to have a high convex border\n",
    "    \n",
    "    Expansions:\n",
    "    1) Expands the facet group to neighbors that are within the normal_closeness\n",
    "    for their normals when doing the dot product\n",
    "    \n",
    "    Order:\n",
    "    1) Size filtering\n",
    "    2) Expansion\n",
    "    3) Convex Border filtering\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # after computing final faces, filter for convexity\n",
    "    edges = gap_mesh.edges_sorted.reshape((-1, 6)) #groups all the edges belonging to the corresponding face in one row\n",
    "    final_facets_mean = np.zeros(len(final_facets))\n",
    "    \n",
    "    \n",
    "    #make lookup table for face number to spot in the adjacency edges\n",
    "    face_adjacency_index_lookup = [[] for i in gap_mesh.faces]\n",
    "    for i,faces in enumerate(gap_mesh.face_adjacency):\n",
    "        for f in faces:\n",
    "            face_adjacency_index_lookup[f].append(i)\n",
    "\n",
    "\n",
    "    for j,facet in enumerate(final_facets):\n",
    "        # get the edges for each facet\n",
    "        edges_facet = [edges[i].reshape((-1, 2)) for i in [facet]][0] #stores all the edges belonging to that face\n",
    "\n",
    "        #get the indexes of the boundary edges:\n",
    "        indexes = trimesh.grouping.group_rows(edges_facet, require_count=1)\n",
    "        edge_0 = edges_facet[indexes]\n",
    "\n",
    "        #find the faces that correspond to the boundary edges\n",
    "        edge_0_faces = [facet[int(k/3)] for k in indexes]\n",
    "\n",
    "\n",
    "        #2) Find the indexes of the edges int he face_adajacency_edges and store the projections\n",
    "        adjacency_values = []\n",
    "        for edge,edge_face in zip(edge_0,edge_0_faces):\n",
    "            possible_adj_indexes = face_adjacency_index_lookup[edge_face]\n",
    "\n",
    "            for index in possible_adj_indexes:\n",
    "                if len(set(edge).intersection(set(gap_mesh.face_adjacency_edges[index]))) >= 2:\n",
    "                    #print(f\"adj edge = {e} and boundary edge = {edge}\")\n",
    "                    adjacency_values.append(gap_mesh.face_adjacency_angles[index]) # the metric we actually want to measure\n",
    "                    break\n",
    "\n",
    "\n",
    "        final_facets_mean[j] = np.mean(adjacency_values)\n",
    "        \n",
    "        \n",
    "\n",
    "    #filter the final facets and output them so they can be plotted\n",
    "    \n",
    "    \n",
    "#     #need to make sure they are array of arrays\n",
    "#     thresholld_boolean_results = final_facets_mean > adjacency_threshold\n",
    "#     for fa,individual_facet in  enumerate(final_facets):\n",
    "#         if thresholld_boolean_results[fa] == True:\n",
    "#             np\n",
    "    \n",
    "    final_facets_mean_filtered = np.array(final_facets)[final_facets_mean > adjacency_threshold]\n",
    "    \n",
    "    if len(final_facets_mean_filtered.shape) > 1:\n",
    "        total_array = np.empty(final_facets_mean_filtered.shape[0],object)\n",
    "        total_array[:] = [x for x in final_facets_mean_filtered]\n",
    "        final_facets_mean_filtered = total_array\n",
    "        print(\"Had to restructure the array because was 2D array\")\n",
    "\n",
    "    \n",
    "\n",
    "    #Compute the centers\n",
    "    final_facets_centers = []\n",
    "    \n",
    "    for filt in final_facets_mean_filtered: \n",
    "        #print(\"filt = \" + str(filt))\n",
    "        unique_vertices = gap_mesh.vertices[np.unique(gap_mesh.faces[filt].ravel())].astype(\"float\")\n",
    "        final_facets_centers.append((np.mean(unique_vertices[:,0]),\n",
    "                          np.mean(unique_vertices[:,1]),\n",
    "                          np.mean(unique_vertices[:,2])))\n",
    "    \n",
    "    return final_facets_mean_filtered,final_facets_centers\n",
    "\n",
    "\n",
    "#main function that generates the facets\n",
    "def filter_final_facets_optimized_with_checks(example_mesh,min_len=2,\n",
    "                                  normal_closeness=0.99,\n",
    "                                  first_pass_size_threshold=6000,\n",
    "                                  adjacency_threshold =  0.8\n",
    "                                  ):\n",
    "    \"\"\"\n",
    "    Way of computing facets that uses trimesh grouping function and normals threshold\n",
    "    instead of the pre-built trimesh.facets\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #get the facets for the child mesh\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    #switch out with dot product\n",
    "    \n",
    "    \n",
    "    #Old way of computing the dot product manually\n",
    "    total_normals = example_mesh.face_normals[example_mesh.face_adjacency]\n",
    "    \n",
    "    #using manual method\n",
    "#     print(len(total_normals[:,0]*total_normals[:,1]))\n",
    "    start_normal = time.time()\n",
    "    total_normal_dots = np.sum(total_normals[:,0]*total_normals[:,1],axis=1)\n",
    "    #print(f\"Total normals took {time.time() - start_normal}\")\n",
    "    ''' DONT HAVE TO DO DOT PRODUCT BECAUSE JUST WANT TO MULTIPLY THE ROWS OF EACH COLUMN'''\n",
    "#     print(\"About to do dot product\")\n",
    "#     dot_start = time.time()\n",
    "#     a = total_normals[:,0]\n",
    "#     b = total_normals[:,1].T\n",
    "#     #new way using the dot product\n",
    "#     total_normal_dots = np.dot(a,b)\n",
    "    \n",
    "    \n",
    "    #get the True/False value if face adjacency is within normal closeness\n",
    "    start_normal = time.time()\n",
    "    total_normal_dots_facet_mask = total_normal_dots > normal_closeness\n",
    "    #print(f\"Boolean mask finished: {time.time() - start_normal}\")\n",
    "\n",
    "    #getting the grouping of the faces into facets within the threshold\n",
    "    start_normal = time.time()\n",
    "    components = trimesh.graph.connected_components(example_mesh.face_adjacency[total_normal_dots_facet_mask],\n",
    "                                          nodes=np.arange(len(example_mesh.faces)),\n",
    "                                          min_len=min_len,\n",
    "                                          engine=\"None\")\n",
    "    #print(f\"Grouping took: {time.time() - start_normal}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #print(f\"Lenght of facets BEFORE filtering = {len(components)}\")\n",
    "    \n",
    "    #filter by the size\n",
    "    \n",
    "    start_normal = time.time()\n",
    "    size_filtered_components = [facet_list for facet_list in components \n",
    "                                    if find_polygon_area(example_mesh,facet_list) > first_pass_size_threshold]\n",
    "    print(f\"Filtering edges by size finished: {time.time() - start_normal}, facet # = {len(size_filtered_components)}\")\n",
    "\n",
    "#     if facet_index in checking_list:\n",
    "#         print(\"size_filtered_components = \" + str(size_filtered_components))\n",
    "\n",
    "    #filter by the convexity\n",
    "    #final_facets_mean_filtered,final_facets_centers = filter_final_facets(example_mesh,size_filtered_components)\n",
    "    \n",
    "    start_normal = time.time()\n",
    "    final_facets_mean_filtered,final_facets_centers = filter_final_facets_working(example_mesh,size_filtered_components,adjacency_threshold)\n",
    "    print(f\"Filtering by convexity and getting centers took: {time.time() - start_normal}, facet # = {len(final_facets_mean_filtered)}\")\n",
    "\n",
    "#     if facet_index in checking_list:\n",
    "#         print(\"final_facets_mean_filtered = \" + str(final_facets_mean_filtered))\n",
    "    \n",
    "    #print(f\"Total Time = {time.time() - start_time_total}\")\n",
    "\n",
    "    return final_facets_mean_filtered,final_facets_centers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STITCHING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_bbox_filter(child,min_bb_zone,max_bb_zone):\n",
    "    \"\"\"\n",
    "    Determines if child is withing the bounding box zone\n",
    "    designated by the bounding box corners\n",
    "    \n",
    "    \"\"\"\n",
    "    #get the min and max of the bounding box for the mesh\n",
    "    min_bb = np.array(child.bounding_box.vertices).min(0)\n",
    "    max_bb = np.array(child.bounding_box.vertices).max(0)\n",
    "    \n",
    "    #print(min_bb,max_bb)\n",
    "    #print(min_bb_zone,max_bb_zone)\n",
    "    \n",
    "    #if fails any of these checks then return false, else return True\n",
    "    if min(min_bb[0],max_bb[0])>max_bb_zone[0]:\n",
    "        print(\"returning x greater max\")\n",
    "        return False\n",
    "    \n",
    "    if max(min_bb[0],max_bb[0])<min_bb_zone[0]:\n",
    "        print(\"returning x less min\")\n",
    "        return False\n",
    "    \n",
    "    if min(min_bb[1],max_bb[1])>max_bb_zone[1]:\n",
    "        print(\"returning y greater max\")\n",
    "        return False\n",
    "    \n",
    "    if max(min_bb[1],max_bb[1])<min_bb_zone[1]:\n",
    "        print(\"returning y less min\")\n",
    "        return False\n",
    "        \n",
    "    if min(min_bb[2],max_bb[2])>max_bb_zone[2]:\n",
    "        print(\"returning z greater max\")\n",
    "        return False\n",
    "    \n",
    "    if max(min_bb[2],max_bb[2])<min_bb_zone[2]:\n",
    "        print(\"returning z less mim\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "import math\n",
    "def area(vertices):\n",
    "    \"\"\"\n",
    "    Calculates the area of a 3D triangle from it's coordinates\n",
    "    \"\"\"\n",
    "    side_a = np.linalg.norm(vertices[0]-vertices[1])\n",
    "    side_b = np.linalg.norm(vertices[1]-vertices[2])\n",
    "    side_c = np.linalg.norm(vertices[2]-vertices[0])\n",
    "    s = 0.5 * ( side_a + side_b + side_c)\n",
    "    return math.sqrt(s * (s - side_a) * (s - side_b) * (s - side_c))\n",
    "\n",
    "def find_polygon_area(mesh,list_of_faces):\n",
    "    \"Calculates the area of a 3D polygon that is created from connected traingles\"\n",
    "    return(sum([area(mesh.vertices[mesh.faces[r]]) for r in list_of_faces]))\n",
    "\n",
    "# restitching functions\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "\n",
    "#gets the projection of point p onto line a\n",
    "def ClosestPointOnLine(a, b, p):\n",
    "    ap = p-a\n",
    "    ab = b-a\n",
    "    #base_vector = ab\n",
    "    result = np.dot(ap,ab)/np.dot(ab,ab) # * ab\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "#now have the mesh and the facet faces, can send to function\n",
    "def stitch_mesh_piece_vp4(new_mesh,facet_1,facet_2,\n",
    "                          delete_facets=False,\n",
    "                         return_added_mesh = False,\n",
    "                         fix_normals = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Changed since last version: \n",
    "    1) parameter for deleting facets at end or not\n",
    "    2) parameter for returning added mesh or not \n",
    "    3) Changed normals check to print statement and not exception\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    #how to find the normals of facet groups:\n",
    "    facet_group_1_normal = new_mesh.face_normals[facet_1[0]] #main_mesh_normals\n",
    "    facet_group_2_normal = new_mesh.face_normals[facet_2[0]] #child_mesh_normals\n",
    "\n",
    "\n",
    "    #get the correct version of the normals: (might need to flip them if going in opposite direction)\n",
    "    if np.dot(facet_group_1_normal,facet_group_2_normal) > 0.8:\n",
    "        raise Exception(\"same direction normals\")\n",
    "    elif np.dot(facet_group_1_normal,facet_group_2_normal) < -0.8:\n",
    "        print(\"opposite direction normals\")\n",
    "    else:\n",
    "        print(\"Not correct normals\")\n",
    "        #raise Exception(\"Not correct normals\")\n",
    "\n",
    "    # make each row correspond to a single face \n",
    "    edges = new_mesh.edges_sorted.reshape((-1, 6))\n",
    "    # get the edges for each facet\n",
    "    edges_facet = [edges[i].reshape((-1, 2)) for i in [facet_1,facet_2]]\n",
    "    edges_boundary = np.array([i[trimesh.grouping.group_rows(i, require_count=1)]\n",
    "                               for i in edges_facet])\n",
    "\n",
    "    #the list of boundary edges and unique points in the boundary edges\n",
    "    edge_0 = edges_boundary[0]\n",
    "    edge_1 = edges_boundary[1]\n",
    "\n",
    "    #gets the unique number of points\n",
    "    edge_0_points = np.unique(np.hstack(edge_0))\n",
    "    edge_1_points = np.unique(np.hstack(edge_1))\n",
    "    print(\"Found boundary edges\")\n",
    "    \"\"\"\n",
    "    get the dot product of all the points\n",
    "    \"\"\"\n",
    "\n",
    "    #get any 2 points on the triangle and make that the reference edge\n",
    "    edge_0_anchor_points = new_mesh.vertices[[edge_0_points[0],edge_0_points[1]]]\n",
    "\n",
    "    #gets the starting index for the 1st facet (so that the start of the stitching is close)\n",
    "    max_index = 0\n",
    "    max_magnitude = ClosestPointOnLine(edge_0_anchor_points[0],edge_0_anchor_points[1],new_mesh.vertices[max_index])\n",
    "\n",
    "    for i in range(1,len(edge_0_points)):\n",
    "        current_magnitude = ClosestPointOnLine(edge_0_anchor_points[0],edge_0_anchor_points[1],new_mesh.vertices[edge_0_points[i]])\n",
    "\n",
    "        if current_magnitude > max_magnitude:\n",
    "            max_index = i\n",
    "            max_magnitude = current_magnitude\n",
    "\n",
    "    edge_0_starting_point = edge_0_points[max_index]\n",
    "\n",
    "    #gets the starting index for the 2nd facet (so that the start of the stitching is close)\n",
    "    max_index = 0\n",
    "    max_magnitude = ClosestPointOnLine(edge_0_anchor_points[0],edge_0_anchor_points[1],new_mesh.vertices[max_index])\n",
    "\n",
    "    for i in range(1,len(edge_1_points)):\n",
    "        current_magnitude = ClosestPointOnLine(edge_0_anchor_points[0],edge_0_anchor_points[1],new_mesh.vertices[edge_1_points[i]])\n",
    "        if current_magnitude > max_magnitude:\n",
    "            max_index = i\n",
    "            max_magnitude = current_magnitude\n",
    "\n",
    "    edge_1_starting_point = edge_1_points[max_index]\n",
    "\n",
    "    print(f\"starting edge 1st facet = {edge_0_starting_point}, starting edge 2nd facet= {edge_1_starting_point}, \")\n",
    "    #print(new_mesh.vertices[edge_0_starting_point],new_mesh.vertices[edge_1_starting_point])\n",
    "\n",
    "    \"\"\"\n",
    "    Need to order the points for restitching\n",
    "\n",
    "    Pseudocode: \n",
    "    1) Get starting piont\n",
    "    2) Find the two edges corresponding to that point\n",
    "    3) Need to decide which direction to start....\n",
    "    - go in direction that make the cross of the (1st and last) point in the same direction of the \n",
    "    normal of the first facet\n",
    "    4) loop through and record the orders of the vertices as you traverse along the edges \n",
    "    until you arrive back at the start\n",
    "    5) Error if:\n",
    "        a. You arrive back at the start and haven't processed all the edges\n",
    "        b. Processsed all the edges but haven't arrived back at start\n",
    "\n",
    "    6) Repeat steps 1 through 5 for 2nd facet group\n",
    "    \"\"\"\n",
    "    start_point_list = [edge_0_starting_point,edge_1_starting_point]\n",
    "    edge_list = [edge_0,edge_1]\n",
    "    edge_order_list = []\n",
    "\n",
    "\n",
    "\n",
    "    #loop that organizes the unique boundary points into the correct order\n",
    "    for i,start_point in enumerate(start_point_list):\n",
    "        print(f\"Starting Organizing vertices for side {i}\")\n",
    "        #print(f\"start_point = {start_point}\")\n",
    "        edge_order = [start_point]\n",
    "        processed_edges = []\n",
    "\n",
    "        #find the matching edges to the starting point\n",
    "        starting_edges_indices = np.where(np.logical_or(edge_list[i][:,0] == start_point,edge_list[i][:,1] == start_point) == True)[0]\n",
    "\n",
    "        starting_edges = edge_list[i][starting_edges_indices]\n",
    "        #print(f\"starting edges = {starting_edges}\") #the list of the two possible edges\n",
    "\n",
    "        if starting_edges.size < 4:\n",
    "            raise Exception(\"Not enough edges for 1st facet start point\")\n",
    "\n",
    "        if starting_edges.size > 4:\n",
    "            raise Exception(\"Too many edges for 1st facet start point\") \n",
    "\n",
    "        #np.where(starting_edges[1,:] != start_point)[0][0]\n",
    "        #gets the vectors that will be used for the cross product\n",
    "        #print(\"np.where(starting_edges[0,:] != start_point)[0][0] = \" + str(np.where(starting_edges[0,:] != start_point)[0][0]))\n",
    "        #print(\"np.where(starting_edges[1,:] != start_point)[0][0] = \" + str(np.where(starting_edges[1,:] != start_point)[0][0]))\n",
    "\n",
    "        \"\"\"*************** where pick the starting edge starts ************\n",
    "        The way it works: \n",
    "        1) Gets the two possible starting edges\n",
    "        2) Generates the vectors for the edges where origin is the starting point\n",
    "        3) Gets the cross porduct of both vectors\n",
    "        4) Chooses the cross product that is in the direction of the face normals\n",
    "\n",
    "        Why that doesn't work:\n",
    "        1) they are opposite normals\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Possible other solution:\n",
    "        1) Get the starting points on the child edge\n",
    "        2) Pick the first edge as the default edge\n",
    "\n",
    "        \"\"\"\n",
    "        processed_edges.append(starting_edges_indices[0])\n",
    "        current_vertex = starting_edges[0][np.where(starting_edges[0,:] != start_point)[0][0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #         #gets the possible starting vectors from the two possible edges\n",
    "    #         possible_starting_vector_1 = new_mesh.vertices[starting_edges[0,:][np.where(starting_edges[0,:] != start_point)[0][0]]] - new_mesh.vertices[start_point]\n",
    "    #         #just start with a random edge\n",
    "    #         possible_starting_vector_2 = new_mesh.vertices[starting_edges[1,:][np.where(starting_edges[1,:] != start_point)[0][0]]] - new_mesh.vertices[start_point]\n",
    "\n",
    "\n",
    "    #         #find the cross product of the starting vectors\n",
    "    #         starting_edges_cross = np.cross(possible_starting_vector_1,possible_starting_vector_2)\n",
    "\n",
    "    #         #make sure that the order of the vectors goes so that the cross product is in line with the starting normal\n",
    "    #         #this ensures the the circular direction of the stitching will be the same\n",
    "    #         if np.dot(starting_edges_cross,facet_group_1_normal) > 0:\n",
    "    #             print(\"Edge 1 picked for direction\")\n",
    "    #             processed_edges.append(starting_edges_indices[0])\n",
    "    #             current_vertex = starting_edges[0][np.where(starting_edges[0,:] != start_point)[0][0]]\n",
    "    #         else:\n",
    "    #             print(\"Edge 2 picked for direction\")\n",
    "    #             processed_edges.append(starting_edges_indices[1])\n",
    "    #             #print(\"np.where(starting_edges[1,:] != start_point) = \" + str(np.where(starting_edges[1,:] != start_point)))\n",
    "    #             current_vertex = starting_edges[1][np.where(starting_edges[1,:] != start_point)[0][0]]\n",
    "\n",
    "        #print(f\"current_vertex = {current_vertex}\" )\n",
    "        #print(\"edge_list = \" + str(edge_list))\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"*************** where pick the starting edge ends ************\"\"\"\n",
    "        #now iterate through number of \n",
    "        for z in range(1,edge_list[i][:,0].size):\n",
    "            #print(\"edge_order_temp = \" + str(edge_order))\n",
    "            if current_vertex == start_point:\n",
    "                print(\"Start vertex reached before processed all of edges\")\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "                These should be ok because the extra loops are created from holes inside and this process should always get the outside loop\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                break\n",
    "\n",
    "            #get the next edge\n",
    "            counter = 0\n",
    "            next_vertex = -1\n",
    "            for j,edg in enumerate(edge_list[i]):\n",
    "                #print(\"edg = \" + str(edg))\n",
    "                #print(\"processed_edges = \" + str(processed_edges))\n",
    "                if current_vertex in edg and j not in processed_edges:\n",
    "                    current_edge_index = j\n",
    "                    if edg[0] != current_vertex:\n",
    "                        next_vertex = edg[0]\n",
    "                    else:\n",
    "                        next_vertex = edg[1]\n",
    "\n",
    "\n",
    "                    counter += 1\n",
    "                    if counter >= 2:\n",
    "                        #raise Exception(f\"More than 2 edges possibilities for {current_vertex}\")\n",
    "                        #Don't want to make it an exception anymore put just print out warning\n",
    "                        print(\"More than 2 edges possibilities for {current_vertex}\") # BAC change\n",
    "\n",
    "            #make sure the next vertex was found\n",
    "            if next_vertex <= -1:\n",
    "                raise Exception(f\"No next vertex was found for {current_vertex} \")\n",
    "\n",
    "            #if found next vertex then add the old vertex and edge index\n",
    "            #to the processed edges lists and the order of vertices\n",
    "            processed_edges.append(current_edge_index)\n",
    "            edge_order.append(current_vertex)\n",
    "\n",
    "            current_vertex = next_vertex\n",
    "\n",
    "\n",
    "        edge_order_list.append(edge_order)\n",
    "        print(f\"edge_{i}_order done, len = {len(edge_order)} \")#\"= {edge_order}\")\n",
    "\n",
    "    #     #print the edge orders\n",
    "    #     for e in edge_order_list:\n",
    "    #         print(type(e))\n",
    "    lengths_of_boundaries = [len(x) for x in edge_order_list]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" ************ PROCESS OF ORDERING THE EDGE PATHS SO THAT *********\n",
    "    main mesh loop goes in counter clockwise in reference to the gap\n",
    "    child goes clockwise in reference to gap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    1) Pick the starting point as your P point\n",
    "    2) For each point in ordered list calculate point - P and store that vector\n",
    "    3) Do the cross product of list[0:n-2] x list[1:n-1]\n",
    "    4) Take the sum of these lists of cross products\n",
    "    5) Do the dot product to compare the direction with the normal vector \n",
    "    Result: \n",
    "    - if positive --> conuter-clockwise according to normal\n",
    "    - if negative --> clockwise according to normal\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    starter_point = np.array([0,0,0])\n",
    "\n",
    "    #     list_of_points = [1,2,3,4,0]\n",
    "\n",
    "    #     list_of_points = [list_of_points[len(list_of_points) - x -1] for x in range(0,len(list_of_points))]\n",
    "    #     print(list_of_points)\n",
    "\n",
    "    #print(\"facet_group_2_normal = \" + str(facet_group_2_normal))\n",
    "\n",
    "    for ed,e_loop in enumerate(edge_order_list):\n",
    "\n",
    "        #get the vertices according to the points\n",
    "        vertices_list = new_mesh.vertices[e_loop]\n",
    "\n",
    "        #get the cross product of the offsetted list\n",
    "        #vertices_list[0:len(vertices_list)-1,:]\n",
    "        \"\"\" Wrong earlier previous way\n",
    "\n",
    "        cross_products = np.cross(vertices_list[0:len(vertices_list)-1,:],vertices_list[1:len(vertices_list)])\n",
    "        \"\"\"\n",
    "\n",
    "        cross_products = np.cross(vertices_list[0:len(vertices_list),:],\n",
    "                             np.vstack([vertices_list[1:len(vertices_list),:],vertices_list[0,:]]))\n",
    "\n",
    "        sum_cross_products = np.sum(cross_products,axis=0)\n",
    "\n",
    "        #print(\"cross_products = \" + str(cross_products))\n",
    "        #print(\"sum_cross_products = \" + str(sum_cross_products))\n",
    "\n",
    "        #print(\"Before edge list = \" + str(edge_order_list[ed]))\n",
    "        if ed == 0:\n",
    "            #get the dot product\n",
    "            normals_dot = np.dot(sum_cross_products,facet_group_1_normal)\n",
    "            #print(' normals_dot = ' + str(normals_dot))\n",
    "            if normals_dot > 0:\n",
    "                print(\"Main originally was counter-clockwise --> keeping\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Main originally was clockwise --> flipping\")\n",
    "                edge_order_list[ed] = [e_loop[len(e_loop) - x -1] for x in range(0,len(e_loop))]\n",
    "\n",
    "\n",
    "        else: \n",
    "            # for the children want the cross product to be counter clockwise\n",
    "            normals_dot = np.dot(sum_cross_products,facet_group_2_normal)\n",
    "            #print(' normals_dot = ' + str(normals_dot))\n",
    "            if normals_dot > 0:\n",
    "                print(\"Child originally was counter-clockwise --> flipping\")\n",
    "                edge_order_list[ed] = [e_loop[len(e_loop) - x -1] for x in range(0,len(e_loop))]\n",
    "            else:\n",
    "                print(\"Child originally was clockwise --> keeping\")\n",
    "\n",
    "\n",
    "        #print(\"After edge list = \" + str(edge_order_list[ed]))\n",
    "    \"\"\"  SHOWS THAT DOING THE CROSS PRODUCT THAT WAY WORKS\n",
    "    #do the cross products manually\n",
    "    for jj in range(0,len(vertices_list)-1):\n",
    "        print(np.cross(vertices_list[jj],vertices_list[jj+1]))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #getting which one is the bigger one\n",
    "    bigger = lengths_of_boundaries.index(max(lengths_of_boundaries))\n",
    "    smaller = 1-bigger\n",
    "\n",
    "    if bigger == 0:\n",
    "        starter_face = \"child\" #if the bigger one was the main, then child is the smaller one you start from\n",
    "    else:\n",
    "        starter_face = \"main\" #if the bigger one was the child, then main is the smaller one you start from\n",
    "\n",
    "\n",
    "    print(\"smaller_face = \" + str(starter_face))    \n",
    "\n",
    "    \"\"\" The rules that have to be following in order for normals to be correctly aligned\n",
    "    1) if the smaller is the child (will be traveling in clockwise direction\n",
    "    --> need to stitch points as:\n",
    "    Regular: other_2, other_1,current_point\n",
    "    Neighbor: current,other_1,current-1\n",
    "\n",
    "\n",
    "    1) if the smaller is the main mesh (\n",
    "    --> need to stitch points as:\n",
    "    Regular: current_point,other_1,other_2\n",
    "    Neighbor: current,current-1,other\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #print(f\"index of bigger facets = {bigger}\\nindex of smaller facets = {smaller}\",)\n",
    "\n",
    "    #calculates the number of vertices will be stitched to each vertices on smaller side\n",
    "    dividend = int(lengths_of_boundaries[bigger]/lengths_of_boundaries[smaller])\n",
    "    remainder = lengths_of_boundaries[bigger] - int(lengths_of_boundaries[bigger]/lengths_of_boundaries[smaller])*lengths_of_boundaries[smaller]\n",
    "\n",
    "    print(f\"dividend = {dividend}, remainder = {remainder}\")\n",
    "\n",
    "    #loop that adds the new faces\n",
    "    print(\"About to add faces\")\n",
    "    start_time = time.time()\n",
    "    new_faces = []\n",
    "    current_bigger = 0\n",
    "\n",
    "    for i,current_smaller in enumerate(edge_order_list[smaller]):\n",
    "        #print(\"current_smaller =\" + str(current_smaller))\n",
    "        #print(\"current_bigger=\" + str(edge_order_list[bigger][current_bigger]))\n",
    "\n",
    "        #connecting to the neighbor on the shorter side\n",
    "        \"\"\"\n",
    "        if i == 0:\n",
    "\n",
    "            new_faces.append([current_smaller,edge_order_list[smaller][-1],edge_order_list[bigger][current_bigger]])\n",
    "        else:\n",
    "            new_faces.append([current_smaller,edge_order_list[smaller][i-1],edge_order_list[bigger][current_bigger]])\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if starter_face == \"main\":\n",
    "            new_faces.append([current_smaller,edge_order_list[bigger][current_bigger],edge_order_list[smaller][i-1]])\n",
    "        else:\n",
    "            new_faces.append([current_smaller,edge_order_list[smaller][i-1],edge_order_list[bigger][current_bigger]])\n",
    "\n",
    "\n",
    "        for j in range(0,dividend + int(i<remainder)):\n",
    "            if current_bigger > len(edge_order_list[bigger]):\n",
    "                raise Exception(\"Somehow rapped around too much\")\n",
    "\n",
    "            if current_bigger >= len(edge_order_list[bigger])-1:\n",
    "                next_bigger = 0\n",
    "            else:\n",
    "                next_bigger = current_bigger+1\n",
    "\n",
    "            if starter_face == \"main\":\n",
    "                new_faces.append([edge_order_list[bigger][next_bigger],\n",
    "                                  edge_order_list[bigger][current_bigger],\n",
    "                                  current_smaller,\n",
    "                                ])\n",
    "            else:\n",
    "                new_faces.append([\n",
    "                                current_smaller,\n",
    "                                  edge_order_list[bigger][current_bigger],\n",
    "                                  edge_order_list[bigger][next_bigger],\n",
    "\n",
    "                                ])\n",
    "\n",
    "            current_bigger += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"new_faces = \" + str(new_faces))\n",
    "    print(f\"Finished adding faces: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    print(\"Starting creating stitch mesh\")\n",
    "    start_time = time.time()\n",
    "    stitch_mesh = trimesh.Trimesh()\n",
    "\n",
    "    stitch_mesh.vertices = new_mesh.vertices\n",
    "    stitch_mesh.faces = np.vstack([new_mesh.faces, new_faces])\n",
    "    print(f\"Finished creating stitch mesh: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if delete_facets == True:\n",
    "        #now take away the original facet faces:\n",
    "        total_faces = np.linspace(0,len(stitch_mesh.faces)-1,len(stitch_mesh.faces)).astype(\"int\")\n",
    "        facet_faces = np.hstack([facet_1 ,facet_2])\n",
    "        faces_to_keep = set(total_faces).difference(set(facet_faces))\n",
    "        faces_to_keep\n",
    "\n",
    "        stitch_mesh = stitch_mesh.submesh([list(faces_to_keep)])[0]\n",
    "\n",
    "    if fix_normals == True:\n",
    "        trimesh.repair.fix_inversion(stitch_mesh)\n",
    "        trimesh.repair.fix_winding(stitch_mesh)\n",
    "        trimesh.repair.fix_normals(stitch_mesh)\n",
    "\n",
    "    #print(\"Finished stitching\")\n",
    "\n",
    "    if return_added_mesh == True:\n",
    "        added_mesh = trimesh.Trimesh()\n",
    "        added_mesh.vertices = new_mesh.vertices\n",
    "        added_mesh.faces = new_faces\n",
    "        trimesh.repair.fix_inversion(added_mesh)\n",
    "        trimesh.repair.fix_winding(added_mesh)\n",
    "        trimesh.repair.fix_normals(added_mesh)\n",
    "\n",
    "        return stitch_mesh,added_mesh\n",
    "\n",
    "    else:\n",
    "        return stitch_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITERATIVE STITCHING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATIVE PROCESS THAT STITCHES TOGETHER MESHES\n",
    "\"\"\"\n",
    "\n",
    "Pseudocode for loop at the end that will keep everything going:\n",
    "1) When process a child, will add that index to a list\n",
    "2) Have no_new_children_processed counter set at end of loop \n",
    "    if no children were added to main mesh\n",
    "    --> this will prompt the expansion of the initial parameters\n",
    "3) If this gets too high\n",
    "\n",
    "\n",
    "\n",
    "Things that still need to add:\n",
    "1) Better way of making sure that the normals are good\n",
    "- Can sample one of the neighboring points and flip normals if the dot product is negative\n",
    "\n",
    "\n",
    "Change list: \n",
    "1) Reduced the stitch distance\n",
    "2) Added the copy features that allows this cell to be rerun without rerunning whole notebook\n",
    "3) added the new tie_breaker for childs that want to connect to same parent is just the one with closest facet\n",
    "    Uses the added feature of child_meshes_stitch_distances that is saved along the way\n",
    "4) Iterates through the repeated faces instead of just doing so once which was incorrect before\n",
    "5) Fixed bug that was adding to current_main_mesh but then was using main mesh also in the loop\n",
    "6) Made changes to the stitching mechanism that not error if find a vertices with more than 2 edges\n",
    "--> because did observe some facets with cut out faces along the boundary\n",
    "7) Changed the consider_same_direction normal as False \n",
    "8) Changed the max index error that was used for finding the starting point in stitch meshes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def stitch_iteration(main_mesh,\n",
    "                     main_mesh_facets_centers,\n",
    "                     main_mesh_facets,\n",
    "                     child_meshes,\n",
    "                     child_meshes_facets,\n",
    "                     bounding_box_threshold=4000,\n",
    "                     stitch_distance_threshold=800,\n",
    "                     size_ratio_threshold=0.15,\n",
    "                     normal_closeness= 0.95,\n",
    "                    bbox_expansion_percentage = 0.10,\n",
    "                    stitch_expansion_percentage = 0.20,\n",
    "                    size_ratio_expansion_percentage = 0.10,\n",
    "                    no_new_children_limit = 4,\n",
    "                    consider_same_direction_normals = False\n",
    "                     \n",
    "                     \n",
    "                     \n",
    "    ):\n",
    "    \n",
    "#     print_dict = dict(bounding_box_threshold=bounding_box_threshold,\n",
    "#                     stitch_distance_threshold=stitch_distance_threshold,\n",
    "#                     size_ratio_threshold=size_ratio_threshold,\n",
    "#                     normal_closeness= normal_closeness,\n",
    "#                     bbox_expansion_percentage = bbox_expansion_percentage,\n",
    "#                     stitch_expansion_percentage = stitch_expansion_percentage,\n",
    "#                     size_ratio_expansion_percentage = size_ratio_expansion_percentage,\n",
    "#                     no_new_children_limit = no_new_children_limit,\n",
    "#                     consider_same_direction_normals = consider_same_direction_normals\n",
    "#                                                                           )\n",
    "#     print(print_dict)\n",
    "\n",
    "    no_new_children_multiplier = 0\n",
    "    total_stitch_processing_time = time.time()\n",
    "\n",
    "    children_processed = []\n",
    "\n",
    "    #lists to store the faces that need to be removed later from main mesh\n",
    "    child_faces_to_remove = []\n",
    "    main_faces_to_remove = []\n",
    "\n",
    "    while len(children_processed) < len(child_meshes):\n",
    "        stitch_loop_time = time.time()\n",
    "        if no_new_children_multiplier >= no_new_children_limit:\n",
    "            print(\"The number of times expanding the thresholds has exceed the limit /n Just returning main mesh\")\n",
    "            print(f\"total_stitch_processing_time = {time.time() - total_stitch_processing_time}\")\n",
    "            return main_mesh,children_processed,child_faces_to_remove,main_faces_to_remove\n",
    "\n",
    "        #update the thresholds\n",
    "        bounding_box_threshold = bounding_box_threshold*(1 + bbox_expansion_percentage*no_new_children_multiplier)\n",
    "        stitch_distance_threshold = stitch_distance_threshold*(1 + stitch_expansion_percentage*no_new_children_multiplier)\n",
    "        #reduces the \n",
    "        size_ratio_threshold = size_ratio_threshold*(1 - size_ratio_expansion_percentage*no_new_children_multiplier)\n",
    "\n",
    "        \n",
    "        #get the main mesh facets normals\n",
    "        main_mesh_normals = [main_mesh.face_normals[fac[0]] for fac in main_mesh_facets]\n",
    "        hit_indexes_list= []\n",
    "\n",
    "\n",
    "        #dictionary to save the stitch points\n",
    "        child_meshes_stitch_facets = dict()\n",
    "        child_meshes_stitch_face_ratios = dict()\n",
    "        child_meshes_stitch_distances = dict()\n",
    "        for i,child in enumerate(child_meshes):\n",
    "\n",
    "            if i in children_processed:\n",
    "                #print(f\"Child {i} already processed\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Starting Child {i}\")\n",
    "\n",
    "            #initialize the stitch index\n",
    "            #child_meshes_stitch_facets[i] = [-1,-1]\n",
    "\n",
    "            #two highest points for the bounding box\n",
    "            min_bb = np.array(main_mesh.bounding_box.vertices).min(0)\n",
    "            max_bb = np.array(main_mesh.bounding_box.vertices).max(0)\n",
    "\n",
    "            min_bb_zone = min_bb - bounding_box_threshold\n",
    "            max_bb_zone = max_bb + bounding_box_threshold\n",
    "\n",
    "\n",
    "\n",
    "            #then send mesh to function that decides if with\n",
    "            pass_bbox_filter = apply_bbox_filter(child,min_bb_zone,max_bb_zone)\n",
    "\n",
    "            if not pass_bbox_filter:\n",
    "                print(\"skipped by bounding box filter\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"  NEW WAY OF COMPUTING THE PAIRWISE CALCULATIONS  *************************************************************** \"\"\"\n",
    "\n",
    "            child_facets,child_facets_centers = child_meshes_facets[i]\n",
    "            pairs_start_time = time.time()\n",
    "\n",
    "\n",
    "            if len(child_facets_centers) == 0:\n",
    "                print(\"child_facets_centers for child {i} was 0, so skipping\")\n",
    "                continue\n",
    "\n",
    "            start_time = time.time()\n",
    "            a_New = np.array(child_facets_centers).astype(\"float\")\n",
    "            b_New = np.array(main_mesh_facets_centers).astype(\"float\")\n",
    "\n",
    "            print(len(a_New),len(b_New))\n",
    "    #         print(\"starting distance matrix\")\n",
    "            start_time = time.time()\n",
    "            from scipy.spatial import distance_matrix\n",
    "            a_b_distance = distance_matrix(a_New, b_New)\n",
    "\n",
    "            #print(f\"Time = {time.time() - start_time}\")\n",
    "\n",
    "            #now get the indexes that are within stitch distance\n",
    "\n",
    "    #         print(\"min(a_b_distance) = \" + str(np.amin(a_b_distance.shape)))\n",
    "            indexes_not = np.where(a_b_distance < stitch_distance_threshold)\n",
    "            print(f\"Done distance matrix: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "    #         print(\"(indexes_not) = \" + str((indexes_not)))\n",
    "    #         print(\"starting normals\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    #         #old way to do it\n",
    "    #         child_normals = child.face_normals[indexes_not[0]]\n",
    "    #         main_normals = main_mesh.face_normals[indexes_not[1]]\n",
    "\n",
    "            if not indexes_not[0].any():\n",
    "                print(f\"Child {i} There were no points close enough\")\n",
    "                continue\n",
    "\n",
    "            #old way to do it\n",
    "    #         print(indexes_not[0])\n",
    "    #         print(child_facets[indexes_not[0]])\n",
    "\n",
    "\n",
    "            child_normals = child.face_normals[[p[0] for p in child_facets[indexes_not[0]]]]\n",
    "            main_normals = main_mesh.face_normals[[p[0] for p in main_mesh_facets[indexes_not[1]]]]\n",
    "\n",
    "\n",
    "\n",
    "    #         print(child_facets[indexes_not[0]][0])\n",
    "    #         print('child_normals = ' + str(child_normals))\n",
    "    #         print(main_mesh_facets[indexes_not[1]][0])\n",
    "    #         print('main_normals = ' + str(main_normals))\n",
    "            #start_time = time.time()\n",
    "            #dot_products = np.dot(child_normals,main_normals.T)\n",
    "\n",
    "            total_normal_dots = np.sum(child_normals*main_normals,axis=1)\n",
    "    #         print(\"total_normal_dots = \" + str(total_normal_dots))\n",
    "            total_normal_dots_facet_mask = total_normal_dots < -normal_closeness\n",
    "\n",
    "    #         print(f\"Done Normals generation: {time.time() - start_time}\")\n",
    "    #         print(\"starting pair generation\")\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "            final_pairs = (np.array(indexes_not).T)[total_normal_dots_facet_mask]\n",
    "            #print(final_pairs)\n",
    "\n",
    "\n",
    "            #get the sizes of all the unique ones\n",
    "            face_0_unique_facets = np.unique(final_pairs[:,0])\n",
    "            face_1_unique_facets = np.unique(final_pairs[:,1])\n",
    "            #print(\"final_pairs = \" + str(final_pairs))\n",
    "    #         print(\"face_0_unique_facets = \" + str(face_0_unique_facets))\n",
    "    #         print(\"face_1_unique_facets = \" + str(face_1_unique_facets))\n",
    "\n",
    "\n",
    "            if len(final_pairs) <= 0:\n",
    "                print(f\"Child {i} There was no possible stitch found after stitch distance and face normal filters\")\n",
    "                continue\n",
    "\n",
    "            if len(final_pairs[0]) > 0:\n",
    "                face_0_facet_sizes = dict([(u,find_polygon_area(child,child_facets[u])) for u in face_0_unique_facets])\n",
    "                face_1_facet_sizes = dict([(u,find_polygon_area(main_mesh,main_mesh_facets[u])) for u in face_1_unique_facets])\n",
    "\n",
    "\n",
    "            possible_stitch_pairs = final_pairs\n",
    "            print(f\"Done Generating final pairs: {time.time() - pairs_start_time}\")\n",
    "\n",
    "\n",
    "            \"\"\"  NEW WAY OF COMPUTING THE PAIRWISE  *************************************************************** \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #     print(\"possible_stitch_pairs = \" + str(possible_stitch_pairs))\n",
    "        #     print(\"len(hit_indexes) = \" + str(len(hit_indexes)))\n",
    "        #     print(\"hit_indexes[0].any() = \" + str(hit_indexes[0].any()))\n",
    "        #     print(\"hit_indexes = \" + str(hit_indexes))\n",
    "        #     print(\"hit_indexes[0] = \" + str(hit_indexes[0]))\n",
    "        #     print(\"len(hit_indexes[0]) = \" + str(len(hit_indexes[0])))\n",
    "\n",
    "            #find the sizes of all of them\n",
    "            face_pair_sizes = np.zeros(len(possible_stitch_pairs[:,0]))\n",
    "            face_size_ratios = np.zeros(len(possible_stitch_pairs[:,0]))\n",
    "\n",
    "            #print(\"possible_stitch_pairs = \" + str(possible_stitch_pairs))\n",
    "            for numba,pair in enumerate(possible_stitch_pairs):\n",
    "                #print(\"pair = \" + str(pair))\n",
    "                sizes = [face_0_facet_sizes[pair[0]],face_1_facet_sizes[pair[1]]]\n",
    "                min_area = min(sizes)\n",
    "                max_area = max(sizes)\n",
    "\n",
    "                ratio = min_area/max_area\n",
    "\n",
    "                #print(f\"ratio = {ratio}\")\n",
    "                #print(f\"Total size  = {min_area + max_area}\")\n",
    "                if ratio >= size_ratio_threshold:\n",
    "                    face_pair_sizes[numba] = min_area + max_area\n",
    "                    face_size_ratios[numba] = ratio\n",
    "\n",
    "                    #print(f\"face_pair_sizes[numba] = {face_pair_sizes[numba]}, face_size_ratios[numba] = {face_size_ratios[numba]}\")\n",
    "\n",
    "\n",
    "            #check that made it past stitch ratio threshold\n",
    "\n",
    "            #best possible stitch pair is just the maximum sized matching ones\n",
    "            best_index = np.where(face_pair_sizes == max(face_pair_sizes))\n",
    "            best_stitch_pair = possible_stitch_pairs[best_index][0]\n",
    "            best_stitch_pair_size = face_pair_sizes[best_index][0]\n",
    "            best_stitch_pair_size_ratio = face_size_ratios[best_index][0]\n",
    "\n",
    "            #get the distance of the best_stitch_pair\n",
    "            best_stitch_pair_distance = a_b_distance[possible_stitch_pairs[best_index][0][0],\n",
    "                                                       possible_stitch_pairs[best_index][0][1]]\n",
    "\n",
    "            print(\"best_stitch_pair = \" + str(best_stitch_pair))\n",
    "            print(\"best_stitch_pair_size = \" + str(best_stitch_pair_size))\n",
    "            print(\"best_stitch_pair_distance = \" + str(best_stitch_pair_distance))\n",
    "            print(\"best_stitch_pair_size_ratio = \" + str(best_stitch_pair_size_ratio))\n",
    "\n",
    "            child_meshes_stitch_facets[i] = [best_stitch_pair[0],best_stitch_pair[1]]\n",
    "            child_meshes_stitch_face_ratios[i] = best_stitch_pair_size_ratio\n",
    "            child_meshes_stitch_distances[i] = best_stitch_pair_distance\n",
    "\n",
    "    #         if i == 1:\n",
    "    #             print(\"Just processed piece 1\")\n",
    "    #             raise Exception(\"stoping\")\n",
    "\n",
    "\n",
    "    #     if 1 in child_meshes_stitch_facets.keys():\n",
    "    #         print(\"Just processed piece 1\")\n",
    "    #         raise Exception(\"stoping\")\n",
    "\n",
    "\n",
    "        #if there were no possible stitch points found\n",
    "        if len(child_meshes_stitch_facets.keys()) == 0:\n",
    "            #increment the no children flag multiplier\n",
    "\n",
    "            no_new_children_multiplier += 1\n",
    "            print(f\"no stitch points found IN ALL CHILDREN --> relaxing the parameters time {no_new_children_multiplier}\")\n",
    "            continue\n",
    "\n",
    "        # makes sure that no two child branches try to connect to the same main branch\n",
    "        from collections import Counter\n",
    "        mesh_stitch_counter = Counter(np.array([val for val in child_meshes_stitch_facets.values()])[:,1])\n",
    "\n",
    "        repeat_main_facets = [key for key,val in mesh_stitch_counter.items() if (key != -1 and val > 1)] #gets the main mesh facet with multiples\n",
    "        print(\"repeat_main_facets = \" + str(repeat_main_facets))\n",
    "\n",
    "\n",
    "        #how to fix that some faces are trying to branch to same main facet\n",
    "        #make it iterate through all of the repeats\n",
    "        if len(repeat_main_facets)>0:\n",
    "            for repeat_main in repeat_main_facets:\n",
    "                child_mesh_double_indexes = [key for key,val in child_meshes_stitch_facets.items() if val[1] == repeat_main]\n",
    "                print(\"child_mesh_double_indexes = \" + str(child_mesh_double_indexes))\n",
    "\n",
    "\n",
    "                #decide which one to keep and then ditch all the rest --> pick the CLOSEST ONE, and not the best matching area:\n",
    "\n",
    "                ###### picks the closest area\n",
    "                min_distance = math.inf\n",
    "                min_child = -1\n",
    "\n",
    "\n",
    "                for child_index in child_mesh_double_indexes:\n",
    "                    current_distance = child_meshes_stitch_distances[child_index]\n",
    "\n",
    "                    if current_distance < min_distance:\n",
    "                        min_child = child_index\n",
    "                        min_distance = current_distance\n",
    "\n",
    "                print(f\"min_child = {min_child}, max_ratio = {min_distance}\")\n",
    "\n",
    "        #         ###### picks the maximum area\n",
    "        #         max_ratio = -1\n",
    "        #         max_child = -1\n",
    "\n",
    "\n",
    "        #         for child_index in child_mesh_double_indexes:\n",
    "        #             current_ratio = child_meshes_stitch_face_ratios[child_index]\n",
    "\n",
    "        #             if current_ratio > max_ratio:\n",
    "        #                 max_child = child_index\n",
    "        #                 max_ratio = current_ratio\n",
    "\n",
    "        #         print(f\"max_child = {max_child}, max_ratio = {max_ratio}\")\n",
    "\n",
    "\n",
    "\n",
    "                #remove the others from the stitch facets\n",
    "                for double_index in child_mesh_double_indexes:\n",
    "                    if double_index != min_child:\n",
    "                        del child_meshes_stitch_facets[double_index]\n",
    "\n",
    "        \"\"\"\n",
    "        Pseudocode for stitching:\n",
    "        1) For each pair in the child_meshes_stitch_facets:\n",
    "        a. Get the child mesh for that pair\n",
    "        b. Get the list of faces for the child facet (from the facet number)\n",
    "        c. Get the list of faces for the main facet (from the main number)\n",
    "\n",
    "        d. Get the original number of faces and vertices in the main mesh\n",
    "        d2. Use the orignal number of faces and add to list of faces for child facet to offset them correctly\n",
    "            - Save this number list in a dictionary (to use for later and creating the submesh)\n",
    "        e. Add the two meshes together to get big mesh\n",
    "        f. Send the two meshes and the two facet lists to the restitching function to get a main mesh that is stitched up\n",
    "         - but send it to function that doesnt delete the original facet faces \n",
    "             (because this would remove meshes from original and screw up facet number)\n",
    "        g. reassign the main_mesh to this newly stitched up mesh\n",
    "        h. recompute the facets for the main mesh\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        current_main_mesh = main_mesh.copy()\n",
    "\n",
    "        main_mesh_facet_index_to_delete = []\n",
    "\n",
    "\n",
    "        #if there were no possible stitch points found\n",
    "        if len(child_meshes_stitch_facets.keys()) == 0:\n",
    "            #increment the no children flag multiplier\n",
    "\n",
    "            no_new_children_multiplier += 1\n",
    "            print(f\"no stitch points found IN ALL CHILDREN --> relaxing the parameters time {no_new_children_multiplier}\")\n",
    "            continue\n",
    "        else: #if there were stitch points found\n",
    "            print(\"child_meshes_stitch_facets = \" + str(child_meshes_stitch_facets))\n",
    "            for child_key,pair in child_meshes_stitch_facets.items():\n",
    "                \"\"\"\n",
    "                child_key has the child that is currently being processed\n",
    "\n",
    "                pair has the facet ids that are to be stitched together\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "                child_used_facet_index = pair[0]\n",
    "                main_used_faceet_index = pair[1]\n",
    "\n",
    "                stitch_time = time.time()\n",
    "                print(f\"---Stitching child {child_key} with pair: {pair}---\")\n",
    "                current_child_mesh = child_meshes[child_key]\n",
    "                current_child_facet_faces = child_meshes_facets[child_key][0][pair[0]]\n",
    "\n",
    "\n",
    "                current_main_mesh_facet_faces = main_mesh_facets[pair[1]]\n",
    "\n",
    "                #Get the original number of faces and vertices in the main mesh\n",
    "                original_mesh_faces_len = len(current_main_mesh.faces)\n",
    "                current_child_facet_faces_adjusted = current_child_facet_faces + original_mesh_faces_len\n",
    "\n",
    "                #Save the faces number for deletion later\n",
    "                child_faces_to_remove += current_child_facet_faces_adjusted.tolist()\n",
    "                main_faces_to_remove += current_main_mesh_facet_faces.tolist()\n",
    "\n",
    "                combined_mesh = current_main_mesh + current_child_mesh\n",
    "\n",
    "                #how to stitch up the mesh\n",
    "                start_time = time.time()\n",
    "                current_main_mesh = stitch_mesh_piece_vp4(new_mesh=combined_mesh,\n",
    "                                                               facet_1=current_main_mesh_facet_faces,\n",
    "                                                               facet_2=current_child_facet_faces_adjusted,\n",
    "                                                              delete_facets=False,\n",
    "                                                              return_added_mesh=False,\n",
    "                                                               fix_normals = False)\n",
    "\n",
    "                print(f\"returned from stitch mesh: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "                #Don't need to do any deletion now \n",
    "\n",
    "                #add the child to processed child\n",
    "                children_processed.append(child_key)\n",
    "                print(f\"Finished stitching child {child_key} : {time.time() - stitch_time}\")\n",
    "\n",
    "                \"\"\"\n",
    "                Add all of the child facets that weren't the ones used to the main mesh facets (with adjusted facet numbers)\n",
    "                \"\"\"\n",
    "                original_mesh_faces_len #the original number of faces in the main mesh (including all those stitched before)\n",
    "\n",
    "                #remove certian rows from array of the child facets and cetners\n",
    "                current_child_facet_faces_with_deletion = np.delete(child_meshes_facets[child_key][0],(child_used_facet_index),axis=0)\n",
    "                current_child_facet_centers_with_deletion = np.delete(child_meshes_facets[child_key][1],(child_used_facet_index),axis=0)\n",
    "\n",
    "                #have to adjust the faces of the child_facets list to account for the faces off set\n",
    "                current_child_facet_faces_with_deletion = current_child_facet_faces_with_deletion + original_mesh_faces_len\n",
    "\n",
    "                #append this list to the main mesh list\n",
    "                main_mesh_facets = np.concatenate([main_mesh_facets,current_child_facet_faces_with_deletion])\n",
    "                main_mesh_facets_centers = np.concatenate([main_mesh_facets_centers,current_child_facet_centers_with_deletion])\n",
    "\n",
    "                #save off the facets to delete for the main mesh at the end of the loop\n",
    "                main_mesh_facet_index_to_delete.append(pair[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #reset the no_new_children_multiplier because there were successful stitching\n",
    "            no_new_children_multiplier = 0\n",
    "            \"\"\"recompute the main mesh facets  ###Not going to recompute anymore\n",
    "            #main_mesh_facets,main_mesh_facets_centers = filter_final_facets(main_mesh)\n",
    "\n",
    "            Now we just remove the main mesh facets that are used and reassign\n",
    "            \"\"\"\n",
    "            main_mesh = current_main_mesh\n",
    "\n",
    "\n",
    "            #at the end of the big loop have to delete the facets used from main mesh\n",
    "            #remove certian rows from array\n",
    "\n",
    "            #creating the new facets\n",
    "            main_mesh_facets = np.delete(main_mesh_facets,(main_mesh_facet_index_to_delete),axis=0)\n",
    "            main_mesh_facets_centers = np.delete(main_mesh_facets_centers,(main_mesh_facet_index_to_delete),axis=0)\n",
    "\n",
    "\n",
    "            print(f\"***************Finished big stitching iteration: {time.time() - stitch_loop_time}***************\")\n",
    "\n",
    "    #         if 0 in children_processed and 1 in children_processed:\n",
    "    #             print(\"Exiting before does piece 2\")\n",
    "    #             break\n",
    "\n",
    "            if len(np.unique(children_processed)) == len(child_meshes):\n",
    "                print(\"All children have been processed\")\n",
    "                print(f\"total_stitch_processing_time = {time.time() - total_stitch_processing_time}\")\n",
    "                return main_mesh,children_processed,child_faces_to_remove,main_faces_to_remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stitch_neuron(segment_id,\n",
    "                  vertices,\n",
    "                  faces,\n",
    "                **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Stitches the portions of disconnected mesh pieces of a given mesh back together and applies a pymeshfix\n",
    "    cleaning on the mesh (if the pymeshfix_flag is set). \n",
    "  \n",
    "\n",
    "    Parameters: \n",
    "    segment_id (int): segment id of the mesh to be stitched and cleaned\n",
    "    vertices (np.array): list of vertices for mesh\n",
    "    faces (np.array): list of faces array for the mesh\n",
    "    \n",
    "    Returns:\n",
    "    stitched_vertices (np.array): list of vertices for single largest mesh after stitching\n",
    "    stitched_faces (np.array): list of faces for single largest mesh after stitching\n",
    "    \n",
    "    **** if stitch_and_unstitched_flag is set to True, returns the following instead of stitched_vertices,stitched_faces\n",
    "    stitched_and_unstitched_vertices (np.array): list of vertices for both stitched and unstitched meshes after stitching\n",
    "    stitched_and_unstitched_faces (np.array): list of faces for both stitched and unstitched meshes after stitching\n",
    "    ********************************************************************************************************************\n",
    "    \n",
    "    filtered_inside_percentage (float) : percentage of total mesh that was filtered away becuase inside the main mesh\n",
    "    stitched_percentage (float) : percentage of mesh after inside/outside filtered that was stitched back to the main mesh\n",
    "    unstitched_percentage (float) : percentage of mesh after inside/outside filtered that was left unstitched back to main mesh\n",
    "    \n",
    "    \n",
    "    Optional Parameters:\n",
    "    -- importing parameters --\n",
    "    import_from_off (bool) : if set true then will attempt to import the function mesh a local directory instead of \n",
    "                            using the segment id and datajoint database (default = False)\n",
    "    off_file_path (str) : path to local off file to be loaded (default = \"\")\n",
    "    \n",
    "    -- saving and loading main and child mesh options --\n",
    "    save_file_location (string) : location for saved/cached files to be stored (default = \"./stitch_mesh_saved\")\n",
    "                                    if this does not already exists then creates it\n",
    "                                    \n",
    "    save_file_name (string) : filename for saved main and child meshes (must be an .npz file) \n",
    "                                (default = str(segment_id) + \"_\" + str(outisde_size_threshold) + \" _main_and_child_meshes_array.npz\"\n",
    "  \n",
    "    )\n",
    "    \n",
    "    save_meshes_flag (bool) : if set true, will save off the main and child meshes in the save_file_location with \n",
    "                        the following name (default = False)\n",
    "    \n",
    "    \n",
    "    load_file_location (string) : location for saved/cached files to be loaded from if save_meshes flag set (default = \"./stitch_mesh_saved\")\n",
    "                                   \n",
    "    load_file_name (string) : filename in the save_file_location where a saved copy of the \n",
    "                                            main and child meshes .npz file is, if this is specified then does\n",
    "                                            not do inside/outside filtering but instead uses this saved file\n",
    "                                            (default = str(segment_id) + \"_\" + str(outisde_size_threshold) + \" _main_and_child_meshes_array.npz\"\n",
    "  \n",
    "     \n",
    "    load_meshes_flag (bool) : if set true, will load off the main and child meshes specified in the \n",
    "                                load locations and load file name (default = False)\n",
    "                     \n",
    "                     \n",
    "    \n",
    "    \n",
    "    --Inside/Outisde Mesh Filtering--\n",
    "    \n",
    "    outisde_size_threshold (int): number of faces of a child mesh outside the main piece must be \n",
    "                                   in order to be attempted to be restitched (default = 30)\n",
    "    n_sample_points (int) : The number of points randomly sampled to decide if the child mesh is \n",
    "                            inside or outside the main mesh (default = 3)\n",
    "    \n",
    "                            \n",
    "    -- extracting facets --\n",
    "    length_threshold (int) : size that determines large from small meshes that determines how many minumum faces must be in a facet (default = 60000)\n",
    "    min_len_large (int) :  minimum number of faces to be included in a facet for large meshes (default = 3)\n",
    "    min_len_small (int) :  minimum number of faces to be included in a facet for small meshes (default = 2)\n",
    "    normal_closeness_facets (float) : minimum dot product of normals for adjacent faces to \n",
    "                                be considered in the same facet (default = 0.99)\n",
    "    first_pass_size_threshold (float) : minimum area of facet (default = 6000)\n",
    "    adjacency_threshold (float ) : threshold for average convexity of a facet edge \n",
    "                                in order to be a possible stitch point (default = 0.8)\n",
    "                                \n",
    "                                \n",
    "    -- stitching parameters --\n",
    "    None\n",
    "    \n",
    "    -- finding stitching point parameters --\n",
    "    \n",
    "    bounding_box_threshold (int) : number added to dimensions of main mesh bounding\n",
    "                                    that will help filter the number of child meshes that could possibly be stitched (default=4000)\n",
    "    stitch_distance_threshold (int) : starting distance that is the maximum distance between \n",
    "                                      two possible facets to be stitched together (default=800) \n",
    "    size_ratio_threshold (float) : minimum ratio between sizes of facets in order to be considered possible pair (default=0.15)\n",
    "    normal_closeness: minimum dot product between normals of first faces in each facet to make sure they are\n",
    "                            pointing in the correct directions(default=0.95)\n",
    "    no_new_children_limit (int) : maximum number of iterations that can be done without finding any new stitch points,\n",
    "                                if this number is exceeded then breaks out of loop (default= 4)\n",
    "    consider_same_direction_normals (bool) : flag that if true allows for normals pointing in the same direction\n",
    "                                            to be in consideration for possible stitch points(default = False)\n",
    "    \n",
    "    - paameter expansion values -\n",
    "    \n",
    "    bbox_expansion_percentage (float) : expands bounding box threshold by x percentage after iteration \n",
    "                                        with no stitch points found (default = 0.10 )\n",
    "    stitch_expansion_percentage (float) : expands stitch length threshold by x percentage after iteration \n",
    "                                        with no stitch points found (default = 0.20 )\n",
    "    size_ratio_expansion_percentage (float) : contracts size ratio x percentage after iteration \n",
    "                                        with no stitch points found (default = 0.10 )\n",
    "                                        \n",
    "    -- pymeshfix clean --\n",
    "    \n",
    "    pymeshfix_flag (bool) : if set true, applies a pymeshfix cleaning on stitched mesh at the end\n",
    "                            before returning the mesh (default = False) \n",
    "                            \n",
    "    -- return flags --\n",
    "    stitch_and_unstitched_flag (bool) : if set true returns the stitched portions \n",
    "                                        with the unstitched portions (default = False)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #now get all of the parameters\n",
    "    \n",
    "    #-- importing parameters --\n",
    "    \n",
    "    global_time = time.time()\n",
    "    \n",
    "    import_from_off = kwargs.pop('import_from_off', False)\n",
    "    off_file_path = kwargs.pop('off_file_path', \"\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    #--Inside/Outisde Mesh Filtering-- parameters\n",
    "    outisde_size_threshold = kwargs.pop(\"outisde_size_threshold\",30)\n",
    "    n_sample_points = kwargs.pop(\"n_sample_points\",3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #-- saving and loading main and child mesh options --\n",
    "    save_file_location = kwargs.pop(\"save_file_location\",\"./stitch_mesh_saved\")\n",
    "    save_file_name = kwargs.pop(\"save_file_name\",str(segment_id) + \"_\" + str(outisde_size_threshold) + \"_main_and_child_meshes_array.npz\")\n",
    "    save_meshes_flag = kwargs.pop(\"save_meshes_flag\",False)\n",
    "    \n",
    "    load_file_location = kwargs.pop(\"load_file_location\",\"./stitch_mesh_saved\")\n",
    "    load_file_name = kwargs.pop(\"load_file_name\",str(segment_id) + \"_\" + str(outisde_size_threshold) + \"_main_and_child_meshes_array.npz\")\n",
    "    load_meshes_flag = kwargs.pop(\"load_meshes_flag\",False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-- extracting facets -- parameters\n",
    "    length_threshold = kwargs.pop(\"length_threshold\",60000)\n",
    "    min_len_large = kwargs.pop(\"min_len_large\",3)\n",
    "    min_len_small = kwargs.pop(\"min_len_small\",2)\n",
    "    normal_closeness_facets = kwargs.pop(\"normal_closeness_facets\",0.99)\n",
    "    first_pass_size_threshold = kwargs.pop(\"first_pass_size_threshold\",6000)\n",
    "    adjacency_threshold = kwargs.pop(\"adjacency_threshold\",0.8)\n",
    "    \n",
    "        \n",
    "    #-- finding stitching point parameters --\n",
    "    bounding_box_threshold = kwargs.pop(\"bounding_box_threshold\",4000)\n",
    "    stitch_distance_threshold = kwargs.pop(\"stitch_distance_threshold\",800)\n",
    "    size_ratio_threshold = kwargs.pop(\"size_ratio_threshold\",0.15)\n",
    "    normal_closeness = kwargs.pop(\"normal_closeness\",0.95)\n",
    "    no_new_children_limit = kwargs.pop(\"no_new_children_limit\",4)\n",
    "    consider_same_direction_normals = kwargs.pop(\"consider_same_direction_normals\",False)\n",
    "    \n",
    "    \n",
    "    #-- paameter expansion values -- parameters\n",
    "    bbox_expansion_percentage = kwargs.pop(\"bbox_expansion_percentage\",0.10)\n",
    "    stitch_expansion_percentage = kwargs.pop(\"stitch_expansion_percentage\",0.20)\n",
    "    size_ratio_expansion_percentage = kwargs.pop(\"size_ratio_expansion_percentage\",0.10)\n",
    "    \n",
    "    #    -- pymeshfix clean -- parameters\n",
    "    pymeshfix_flag = kwargs.pop(\"pymeshfix_flag\",False)\n",
    " \n",
    "                            \n",
    "    #-- return parameters --\n",
    "    stitch_and_unstitched_flag = kwargs.pop(\"stitch_and_unstitched_flag\",False)\n",
    "\n",
    "    #######------ finished importing all of the parameters --------\n",
    "    \n",
    "    #making sure there is no more keyword arguments left that you weren't expecting\n",
    "    if kwargs:\n",
    "        raise TypeError('Unexpected **kwargs: %r' % kwargs)\n",
    "    \n",
    "    \n",
    "    ##### STEP 1) IMPORT THE MESH FROM FACES,VERTICES OR FROM AN OFF FILE\n",
    "    \n",
    "    if import_from_off == True:\n",
    "        #load the path\n",
    "        unfiltered_mesh = trimesh.load_mesh(file_location + file_name)\n",
    "    else:\n",
    "        unfiltered_mesh = trimesh.Trimesh()\n",
    "        unfiltered_mesh.vertices = vertices\n",
    "        unfiltered_mesh.faces = faces\n",
    "\n",
    "    \n",
    "    \n",
    "    ##### STEP 2) GENERATE THE CHILD AND MAIN MESH FROM THE WHOLE MESH\n",
    "    start_time = time.time()\n",
    "    if load_meshes_flag == False:\n",
    "        print(\"generating child and main meshes\")\n",
    "\n",
    "\n",
    "        #load the child meshes from a locally saved file\n",
    "        #setting thresholds\n",
    "\n",
    "        #the main mesh is the first mesh in the piece\n",
    "        main_mesh,child_meshes = filter_mesh_significant_outside_pieces(unfiltered_mesh,\n",
    "                                    significance_threshold=outisde_size_threshold,\n",
    "                                        n_sample_points=n_sample_points)\n",
    "\n",
    "    else: #load them from a saved location\n",
    "        print(\"import child and main meshes from\" + str(Path(load_file_location) / Path(load_file_name)))\n",
    "        \n",
    "        if not os.path.isfile(str(Path(load_file_location) / Path(load_file_name))):\n",
    "            raise TypeError(str(Path(load_file_location) / Path(load_file_name)) + \" cannot be found for loading mesh files\")\n",
    "            return None\n",
    "\n",
    "        #load in the file\n",
    "        loaded_meshes = np.load(str(Path(load_file_location) / Path(load_file_name)))\n",
    "\n",
    "        main_mesh_loaded = loaded_meshes[\"main_mesh\"][0]\n",
    "        child_meshes_loaded = loaded_meshes[\"child_meshes\"]\n",
    "\n",
    "        main_mesh = main_mesh_loaded\n",
    "        child_meshes = child_meshes_loaded\n",
    "\n",
    "    \n",
    "    #get the percentage of the largest mesh piece in relation to the whole\n",
    "    largest_piece_perc = len(main_mesh.faces)/len(unfiltered_mesh)\n",
    "    n_pieces = len(child_meshes)\n",
    "    \n",
    "                 \n",
    "    \n",
    "    print(f\"Total time for Mesh Cleansing: {time.time() - start_time}\")\n",
    "    total_faces_retained = len(main_mesh.faces) + sum([len(current_mesh.faces) for current_mesh in child_meshes])\n",
    "    filtered_inside_percentage = total_faces_retained/len(unfiltered_mesh.faces)\n",
    "    print(f\"Number of outside child meshes =  {str(len(child_meshes))}\")\n",
    "    print(f\"Main mesh and outside child meshes make up {filtered_inside_percentage}% of original mesh\")\n",
    "    # get the statistics on the parts of the mesh that was filtered away\n",
    "\n",
    "    \n",
    "    ##### STEP 3) POSSIBLY SAVING THE CHILD AND MAIN MESH\n",
    "\n",
    "    if save_meshes_flag == True:\n",
    "        #check that the save location exists:\n",
    "        try:\n",
    "            os.listdir(save_file_location)\n",
    "        except:\n",
    "            print(f\"{save_file_location} didn't exist so making it now\")\n",
    "            os.mkdir(save_file_location)\n",
    "\n",
    "        #check that the file name is an npz file extension\n",
    "        if save_file_name[-4:] != \".npz\":\n",
    "            raise Exception(str(save_file_name) + \" cannot be saved because isn't a .npz file extension\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        np.savez(str(Path(save_file_location) / Path(save_file_name)),main_mesh=[main_mesh],child_meshes=child_meshes)\n",
    "\n",
    "        print(\"Saved child and main meshes at \" + str(Path(save_file_location) / Path(save_file_name)))\n",
    "    \n",
    "    ##### STEP 4) COMPUTE FACEETS FOR CHILD MESHES AND MAIN MESHES\n",
    "    facet_time = time.time()\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    if len(main_mesh.faces > length_threshold):\n",
    "        print(f\" face length {len(main_mesh.faces)} using optimized facets with 3 neighbors\")\n",
    "        main_mesh_facets,main_mesh_facets_centers = filter_final_facets_optimized_with_checks(example_mesh=main_mesh,\n",
    "                                                                                              min_len=min_len_large,\n",
    "                                                                                              normal_closeness=normal_closeness_facets,\n",
    "                                                                                              first_pass_size_threshold=first_pass_size_threshold,\n",
    "                                                                                              adjacency_threshold=adjacency_threshold)\n",
    "\n",
    "    else:\n",
    "        print(f\" face length {len(main_mesh.faces)} using optimized facets with 2 neighbors\")\n",
    "        main_mesh_facets,main_mesh_facets_centers = filter_final_facets_optimized_with_checks(example_mesh=main_mesh,\n",
    "                                                                                              min_len=min_len_small,\n",
    "                                                                                              normal_closeness=normal_closeness_facets,\n",
    "                                                                                              first_pass_size_threshold=first_pass_size_threshold,\n",
    "                                                                                              adjacency_threshold=adjacency_threshold)\n",
    "    if len(main_mesh_facets) > 10000:\n",
    "            print(f\"Finished facets for main mesh: {time.time() - start_time} with facet length = {len(main_mesh_facets)}\")\n",
    "                                                                                          \n",
    "    \n",
    "\n",
    "    print(f\"Finished { len(main_mesh_facets)} facets for main mesh: {time.time() - start_time}\")\n",
    "    child_meshes_facets= []\n",
    "    for jj,gap_mesh in enumerate(child_meshes):\n",
    "        #print(\"Starting child \" + str(jj))\n",
    "        start_time = time.time()\n",
    "        if len(gap_mesh.faces) > length_threshold:\n",
    "            #print(f\" face length {len(gap_mesh.faces)} using optimized facets 3\")\n",
    "            child_meshes_facets.append(filter_final_facets_optimized_with_checks(example_mesh=gap_mesh,\n",
    "                                                                                  min_len=min_len_large,\n",
    "                                                                                  normal_closeness=normal_closeness_facets,\n",
    "                                                                                  first_pass_size_threshold=first_pass_size_threshold,\n",
    "                                                                                  adjacency_threshold=adjacency_threshold))\n",
    "        else:\n",
    "            #print(f\" face length {len(gap_mesh.faces)} using optimized facets 2\")\n",
    "            child_meshes_facets.append(filter_final_facets_optimized_with_checks(example_mesh=gap_mesh,\n",
    "                                                                                              min_len=min_len_small,\n",
    "                                                                                              normal_closeness=normal_closeness_facets,\n",
    "                                                                                              first_pass_size_threshold=first_pass_size_threshold,\n",
    "                                                                                              adjacency_threshold=adjacency_threshold))\n",
    "\n",
    "\n",
    "        if len(child_meshes_facets[jj][0]) > 10000:\n",
    "            print(f\"Finished facets for child {jj} : {time.time() - start_time} with facet length = {len(child_meshes_facets[jj][0])}\")\n",
    "\n",
    "\n",
    "    print(f\"Total time for facets: {time.time() - facet_time}\")\n",
    "    #child_meshes_facets = [filter_final_facets(gap_mesh) for gap_mesh in child_meshes]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Check the ones that don't have any facets\n",
    "    zero_faced = []\n",
    "\n",
    "    for i,ch in enumerate(child_meshes_facets):\n",
    "        num_facets = len(ch[1])\n",
    "        if num_facets == 0:\n",
    "            zero_faced.append(i)\n",
    "        #print(f\"Child {i} has {num_facets} facets\")\n",
    "    print(\"Zero faceted faces = \" + str(zero_faced))\n",
    "    \n",
    "    ##### STEP 5) STITCHING THE CHILD MESHES TO THE MAIN MESH\n",
    "    restitch_time = time.time()\n",
    "    \n",
    "    original_main_mesh_face_size = len(main_mesh.faces)\n",
    "              \n",
    "    main_mesh,children_processed,child_faces_to_remove,main_faces_to_remove = stitch_iteration(main_mesh=main_mesh,\n",
    "                                                                                               main_mesh_facets_centers=main_mesh_facets_centers,main_mesh_facets=main_mesh_facets,\n",
    "                                                                                               child_meshes=child_meshes,\n",
    "                                                                                                child_meshes_facets=child_meshes_facets,\n",
    "                                                                                                bounding_box_threshold=bounding_box_threshold,\n",
    "                                                                                                stitch_distance_threshold=stitch_distance_threshold,\n",
    "                                                                                                size_ratio_threshold=size_ratio_threshold,\n",
    "                                                                                                normal_closeness= normal_closeness,\n",
    "                                                                                                bbox_expansion_percentage = bbox_expansion_percentage,\n",
    "                                                                                                stitch_expansion_percentage = stitch_expansion_percentage,\n",
    "                                                                                                size_ratio_expansion_percentage = size_ratio_expansion_percentage,\n",
    "                                                                                                no_new_children_limit = no_new_children_limit,\n",
    "                                                                                                consider_same_direction_normals = consider_same_direction_normals\n",
    "                                                                                                                                                      )\n",
    "\n",
    "                     \n",
    "                     \n",
    "    \n",
    "    print(f\"Total time for restitching = {time.time() - restitch_time}\")\n",
    "    \n",
    "    \n",
    "    ##### STEP 6) remove all of the processed facets from the main mesh\n",
    "    #now take away the original facet faces:\n",
    "    total_faces = np.linspace(0,len(main_mesh.faces)-1,len(main_mesh.faces)).astype(\"int\")\n",
    "\n",
    "\n",
    "    #these are the faces that need to be removed\n",
    "    facet_faces = np.hstack([child_faces_to_remove ,main_faces_to_remove])\n",
    "    faces_to_keep = set(total_faces).difference(set(facet_faces))\n",
    "\n",
    "\n",
    "    main_mesh_final = main_mesh.submesh([list(faces_to_keep)])[0]\n",
    "    \n",
    "    ##### STEP 7) calculate final statistics for run\n",
    "    #finds the total children missed and prints them out\n",
    "    total_children = np.linspace(0,len(child_meshes)-1,len(child_meshes))\n",
    "    missed_children = list(set(total_children).difference(set(children_processed)))\n",
    "    print(\"missed_children = \" + str(missed_children))\n",
    "\n",
    "    #calculate the percentage of mesh faces that were added because of stitching\n",
    "    stitched_percentage = (len(main_mesh_final.faces) - original_main_mesh_face_size)/ original_main_mesh_face_size\n",
    "    #calculate the percentage of meshes that still aren't stitched back to the main mesh\n",
    "    non_stitch_total_size = sum([len(current_mesh.faces) for i,current_mesh in enumerate(child_meshes) if i in missed_children])\n",
    "    unstitched_percentage = non_stitch_total_size/ original_main_mesh_face_size\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    stitched_vertices (np.array): list of vertices for single largest mesh after stitching\n",
    "    stitched_faces (np.array): list of faces for single largest mesh after stitching\n",
    "    \n",
    "    **** if stitch_and_unstitched_flag is set to True, returns the following instead of stitched_vertices,stitched_faces\n",
    "    stitched_and_unstitched_vertices (np.array): list of vertices for both stitched and unstitched meshes after stitching\n",
    "    stitched_and_unstitched_faces (np.array): list of faces for both stitched and unstitched meshes after stitching\n",
    "    ********************************************************************************************************************\n",
    "    \n",
    "    filtered_inside_percentage (float) : percentage of total mesh that was filtered away becuase inside the main mesh\n",
    "    stitched_percentage (float) : percentage of mesh after inside/outside filtered that was stitched back to the main mesh\n",
    "    unstitched_percentage (float) : percentage of mesh after inside/outside filtered that was left unstitched back to main mesh\n",
    "    \"\"\"\n",
    "    \n",
    "    if pymeshfix_flag == True:\n",
    "        start_time = time.time()\n",
    "        #pass the vertices and faces to pymeshfix to become watertight\n",
    "        meshfix = pymeshfix.MeshFix(main_mesh_final.vertices,main_mesh_final.vertices)\n",
    "        meshfix.repair(verbose=False,joincomp=True,remove_smallest_components=False)\n",
    "        \n",
    "        print(f\"Pymesh shrinkwrapping: {time.time() - start_time}\")\n",
    "        \n",
    "        main_mesh_final.vertices = meshfix.v\n",
    "        main_mesh_final.faces = meshfix.f\n",
    "    \n",
    "    \n",
    "    print(f\"Whole stitching function complete: {time.time() - global_time}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Goal of what we want it to return to write to datajoint table\n",
    "    \n",
    "    n_vertices           : bigint           # number of vertices in this mesh\n",
    "    n_triangles          : bigint           # number of triangles in this mesh\n",
    "    vertices             : longblob         # x,y,z coordinates of vertices\n",
    "    triangles            : longblob         # triangles (triplets of vertices)\n",
    "    n_pieces             : int              # number of unconnected mesh pieces\n",
    "    largest_piece_perc   : decimal(6,5)     # number of faces percentage of largest mesh piece in respect to total mesh\n",
    "    outside_perc         : decimal(6,5)     # number of faces percentage of mesh outside the biggest mesh piece\n",
    "    n_stitched           : int              # number of mesh pieces stitched back to main mesh\n",
    "    stitched_addon_perc  : decimal(6,5)     # number of faces percentage of pieces that were stitched back in respect to largest mesh piece\n",
    "    n_unstitched         : int              # number of mesh pieces remaining unstitched back to main mesh        \n",
    "    unstitched_perce     : decimal(6,5)     # number of faces percentage of pieces that were not in respect to largest mesh piece\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    n_stitched = len(children_processed)\n",
    "    n_unstitched = len(missed_children)\n",
    "\n",
    "\n",
    "    if stitch_and_unstitched_flag == False:\n",
    "        return (len(main_mesh_final.vertices),\n",
    "                len(main_mesh_final.faces),\n",
    "                main_mesh_final.vertices,\n",
    "                main_mesh_final.faces,\n",
    "                n_pieces,\n",
    "                largest_piece_perc,\n",
    "                filtered_inside_percentage,\n",
    "                n_stitched,\n",
    "                stitched_percentage,\n",
    "                n_unstitched,\n",
    "                unstitched_percentage)\n",
    "        #return main_mesh_final,filtered_inside_percentage,stitched_percentage,unstitched_percentage\n",
    "    else:\n",
    "        for child_id in missed_children:\n",
    "            main_mesh_final = main_mesh_final + child_meshes[child_id]\n",
    "        return (len(main_mesh_final.vertices),\n",
    "                len(main_mesh_final.faces),\n",
    "                main_mesh_final.vertices,\n",
    "                main_mesh_final.faces,\n",
    "                n_pieces,\n",
    "                largest_piece_perc,\n",
    "                filtered_inside_percentage,\n",
    "                n_stitched,\n",
    "                stitched_percentage,\n",
    "                n_unstitched,\n",
    "                unstitched_percentage)\n",
    "    \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "\n",
    "\n",
    "use_datajoint = True\n",
    "segment_id = 648518346349470171\n",
    "\n",
    "if use_datajoint == False:\n",
    "\n",
    "    \n",
    "    file_location = \"./stitch_mesh_saved/\"\n",
    "    file_name =\"1_full_no_alterations_648518346349470171.off\"\n",
    "    off_file_path = file_location + file_name\n",
    "    \n",
    "    stitch_neuron_start = time.time()\n",
    "    [main_mesh_final_vertices,main_mesh_final_faces,\n",
    "         filtered_inside_percentage,\n",
    "         stitched_percentage,\n",
    "         unstitched_percentage]  = stitch_neuron(segment_id=segment_id,\n",
    "                                                  vertices=[],\n",
    "                                                  faces=[],\n",
    "                                                import_from_off=True,\n",
    "                                                 load_meshes_flag = True,\n",
    "                                             save_meshes_flag = True,\n",
    "                                                #pymeshfix_flag = True\n",
    "                                               )\n",
    "else:\n",
    "\n",
    "    #getting data from datajoint\n",
    "    pinky = dj.create_virtual_module(\"pinky\",\"microns_pinky\")\n",
    "    ta3p100 = dj.create_virtual_module(\"ta3p100\",\"microns_ta3p100\")\n",
    "\n",
    "\n",
    "\n",
    "    key = dict(segment_id=segment_id,segmentation=3)\n",
    "    vertices,triangles = (pinky.Mesh & key).fetch(\"vertices\",\"triangles\")\n",
    "\n",
    "    verts = vertices[0]\n",
    "    faces = triangles[0]\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Goal of what we want it to return to write to datajoint table\n",
    "    \n",
    "    n_vertices           : bigint           # number of vertices in this mesh\n",
    "    n_triangles          : bigint           # number of triangles in this mesh\n",
    "    vertices             : longblob         # x,y,z coordinates of vertices\n",
    "    triangles            : longblob         # triangles (triplets of vertices)\n",
    "    n_pieces             : int              # number of unconnected mesh pieces\n",
    "    largest_piece_perc   : decimal(6,5)     # number of faces percentage of largest mesh piece in respect to total mesh\n",
    "    outside_perc         : decimal(6,5)     # number of faces percentage of mesh outside the biggest mesh piece\n",
    "    n_stitched           : int              # number of mesh pieces stitched back to main mesh\n",
    "    stitched_addon_perc  : decimal(6,5)     # number of faces percentage of pieces that were stitched back in respect to largest mesh piece\n",
    "    n_unstitched         : int              # number of mesh pieces remaining unstitched back to main mesh        \n",
    "    unstitched_perce     : decimal(6,5)     # number of faces percentage of pieces that were not in respect to largest mesh piece\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    stitch_neuron_start = time.time()\n",
    "    \n",
    "    [n_vertices,\n",
    "     n_triangles,\n",
    "     vertices,\n",
    "     triangles,\n",
    "     n_pieces,\n",
    "     largest_piece_perc,\n",
    "     outside_perc,\n",
    "     n_stitched,\n",
    "     stitched_addon_perc,\n",
    "     n_unstitched,\n",
    "     unstitched_perce] = stitch_neuron(segment_id=segment_id,\n",
    "                                                  vertices=verts,\n",
    "                                                  faces=faces,\n",
    "                                                import_from_off=False,\n",
    "                                                 load_meshes_flag = True,\n",
    "                                             save_meshes_flag = True,\n",
    "                                                pymeshfix_flag = True\n",
    "                                               )\n",
    "    \n",
    "\n",
    "\n",
    "#     stitch_neuron_start = time.time()\n",
    "#     return_value  = stitch_neuron(segment_id=segment_id,\n",
    "#                                                   vertices=verts,\n",
    "#                                                   faces=faces,\n",
    "#                                                 import_from_off=True,\n",
    "#                                                  load_meshes_flag = True,\n",
    "#                                              save_meshes_flag = True,\n",
    "#                                                 #pymeshfix_flag = True\n",
    "#                                                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total time for stitching function = {time.time() - stitch_neuron_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# num = os.path.getsize(\"./stitch_mesh_saved/final_neuron.npz\")\n",
    "# num/1000/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from print_trimesh import print_trimesh\n",
    "\n",
    "main_mesh_print = trimesh.Trimesh()\n",
    "main_mesh_print.vertices = main_mesh_final_vertices\n",
    "main_mesh_print.faces = main_mesh_final_faces\n",
    "\n",
    "\n",
    "#print_trimesh(main_mesh,\"./test_meshes/local_downloaded_from_server_30_datajoint_mesh.off\" )\n",
    "print_trimesh(main_mesh_print,\"./stitch_mesh_saved/\" + str(segment_id) + \"_stitched_mesh_function.off\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the statistics\n",
    "print(filtered_inside_percentage,\n",
    "         stitched_percentage,\n",
    "         unstitched_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to apply pymeshfix to the returned mesh\n",
    "\n",
    "start_time = time.time()\n",
    "#pass the vertices and faces to pymeshfix to become watertight\n",
    "meshfix = pymeshfix.MeshFix(main_mesh_print.vertices,main_mesh_print.faces)\n",
    "meshfix.repair(verbose=False,joincomp=True,remove_smallest_components=False)\n",
    "\n",
    "print(f\"Pymesh shrinkwrapping: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write this pymesh fix to a file and see if cgal can import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(meshfix.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_filename = \"./stitch_mesh_saved/\" + str(segment_id) + \"_stitched_and_pymeshfix.off\"\n",
    "#meshfix.write(\"./stitch_mesh_saved/\" + str(segment_id) + \"_stitched_and_pymeshfix.off\")\n",
    "write_Whole_Neuron_Off_file(segment_id,meshfix.v,meshfix.f,\"./stitch_mesh_saved/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the output file\n",
    "##write the OFF file for the neuron\n",
    "import pathlib\n",
    "def write_Whole_Neuron_Off_file(neuron_ID,\n",
    "                                vertices=[], \n",
    "                                triangles=[],\n",
    "                                folder=\"pymesh_neurons\"):\n",
    "    #primary_key = dict(segmentation=1, segment_id=segment_id, decimation_ratio=0.35)\n",
    "    #vertices, triangles = (mesh_Table_35 & primary_key).fetch1('vertices', 'triangles')\n",
    "    \n",
    "    num_vertices = (len(vertices))\n",
    "    num_faces = len(triangles)\n",
    "    \n",
    "    #get the current file location\n",
    "    file_loc = pathlib.Path.cwd() / folder\n",
    "    filename = \"neuron_\" + str(neuron_ID)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #print(file_loc)\n",
    "    #print(path_and_filename)\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(num_vertices) + \" \" + str(num_faces) + \" 0\\n\" )\n",
    "    \n",
    "    \n",
    "    #iterate through and write all of the vertices in the file\n",
    "    for verts in vertices:\n",
    "        f.write(str(verts[0]) + \" \" + str(verts[1]) + \" \" + str(verts[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing verts\")\n",
    "        \n",
    "    for faces in triangles:\n",
    "        f.write(\"3 \" + str(faces[0]) + \" \" + str(faces[1]) + \" \" + str(faces[2])+\"\\n\")\n",
    "    \n",
    "    print(\"Done writing OFF file\")\n",
    "    #f.write(\"end\")\n",
    "    \n",
    "    return str(path_and_filename),str(filename),str(file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGAL segmentation: 3429.612897872925\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "csm.cgal_segmentation('/notebooks/21_Reconnecting_Mesh/stitch_mesh_saved/neuron_648518346349470171',\n",
    "                    3,0.20 )\n",
    "\n",
    "print(f\"CGAL segmentation: {time.time() - start_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Things to remove from printing:\n",
    "1) Don't print if single mesh in or out\n",
    "2) Only print facet length if large\n",
    "3) Get rid of a lot of the iteration printing\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

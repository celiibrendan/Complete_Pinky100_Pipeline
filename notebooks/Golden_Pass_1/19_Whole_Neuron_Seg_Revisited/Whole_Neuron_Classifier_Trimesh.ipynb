{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import time\n",
    "import pymeshfix\n",
    "import os\n",
    "import trimesh\n",
    "from pathlib import Path\n",
    "\n",
    "import cgal_Segmentation_Module as csm\n",
    "\n",
    "#for supressing the output\n",
    "import contextlib\n",
    "\n",
    "#for fixing the space issue with the CGAL:readoff\n",
    "import csv\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "ta3p100 = dj.create_virtual_module(\"ta3p100\",\"microns_ta3p100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def segment_Whole_Neuron(clusters=3,smoothness=0.20,filename=\"None\",key=dict()):\n",
    "#     original_start_time = time.time() \n",
    "#     start_time = time.time()\n",
    "#     if filename != \"None\":\n",
    "#         if not os.path.isfile(filename):\n",
    "#             print(\"neuron filename not exist\")\n",
    "#             return \"failure\"\n",
    "\n",
    "\n",
    "#         my_mesh = trimesh.load_mesh(filename)\n",
    "#         verts = my_mesh.vertices\n",
    "#         faces = my_mesh.faces\n",
    "\n",
    "#     elif \"segment_id\" in key.keys():\n",
    "        \n",
    "        \n",
    "#         segment_id = key[\"segment_id\"]\n",
    "#         decimation_ratio = key.pop(\"decimation_ratio\",0.35)\n",
    "#         segmentation = key.pop(\"segmentation\",2)\n",
    "        \n",
    "#         primary_key = dict(segmentation=segmentation,decimation_ratio=decimation_ratio,segment_id=neuron_ID)\n",
    "#         neuron_data = (ta3p100.CleansedMesh & primary_key).fetch1()\n",
    "\n",
    "#         print(neuron_data)\n",
    "#         verts = neuron_data['vertices'].astype(dtype=np.int32)\n",
    "#         faces = neuron_data['triangles'].astype(dtype=np.uint32)\n",
    "#         return verts,faces\n",
    "        \n",
    "\n",
    "\n",
    "#     else:\n",
    "#         print(\"No valid key or filename given\")\n",
    "#         return \"failure\"\n",
    "    \n",
    "    \n",
    "#     #run pymeshfix on the mesh\n",
    "#     #meshfix = pymeshfix.MeshFix(verts,faces)\n",
    "    \n",
    "#     #write the output file to an off file\n",
    "# segment_id=648518346349471500\n",
    "# key = dict(segment_id=segment_id)\n",
    "# verts,faces = segment_Whole_Neuron(key=key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def write_Whole_Neuron_Off_file(neuron_ID,vertices=[], triangles=[]):\n",
    "    #primary_key = dict(segmentation=1, segment_id=segment_id, decimation_ratio=0.35)\n",
    "    #vertices, triangles = (mesh_Table_35 & primary_key).fetch1('vertices', 'triangles')\n",
    "    \n",
    "    num_vertices = (len(vertices))\n",
    "    num_faces = len(triangles)\n",
    "    \n",
    "    #get the current file location\n",
    "    \n",
    "    file_loc = pathlib.Path.cwd() / \"temp\"\n",
    "    filename = \"neuron_\" + str(neuron_ID)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #print(file_loc)\n",
    "    #print(path_and_filename)\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(num_vertices) + \" \" + str(num_faces) + \" 0\\n\" )\n",
    "    \n",
    "    \n",
    "    #iterate through and write all of the vertices in the file\n",
    "    for verts in vertices:\n",
    "        f.write(str(verts[0]) + \" \" + str(verts[1]) + \" \" + str(verts[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing verts\")\n",
    "        \n",
    "    for faces in triangles:\n",
    "        f.write(\"3 \" + str(faces[0]) + \" \" + str(faces[1]) + \" \" + str(faces[2])+\"\\n\")\n",
    "    \n",
    "    print(\"Done writing OFF file\")\n",
    "    #f.write(\"end\")\n",
    "    \n",
    "    return str(path_and_filename),str(filename),str(file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta3p100.CleansedMesh() & \"n_vertices<50000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mesh from datajoint- id: 648518346349472574\n",
      "OrderedDict([('segmentation', 2), ('segment_id', 648518346349472574), ('decimation_ratio', Decimal('0.35')), ('n_vertices', 560132), ('n_triangles', 1134283), ('vertices', array([[308382.71875   , 153810.40625   ,  40942.4765625 ],\n",
      "       [308429.34375   , 153877.75      ,  40947.5234375 ],\n",
      "       [308451.25      , 153913.5625    ,  40888.29296875],\n",
      "       ...,\n",
      "       [475551.8125    , 236005.6875    ,  82891.734375  ],\n",
      "       [475571.625     , 236233.859375  ,  82868.5390625 ],\n",
      "       [475601.75      , 236416.75      ,  82883.3828125 ]])), ('triangles', array([[237021, 237023, 236526],\n",
      "       [238728, 239014, 238941],\n",
      "       [238728, 238941, 238474],\n",
      "       ...,\n",
      "       [511531, 510694, 510702],\n",
      "       [542792, 542786, 542791],\n",
      "       [542034, 542786, 542792]], dtype=uint32))])\n",
      "Done writing OFF file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/notebooks/19_Whole_Neuron_Seg_Revisited/temp/neuron_648518346349472574',\n",
       " 'neuron_648518346349472574',\n",
       " '/notebooks/19_Whole_Neuron_Seg_Revisited/temp')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = dict(segment_id=648518346349472574)\n",
    "#key = dict(segment_id=648518346349472574)\n",
    "print(\"Loading mesh from datajoint- id: \" + str(key[\"segment_id\"]))\n",
    "segment_id = key[\"segment_id\"]\n",
    "decimation_ratio = key.pop(\"decimation_ratio\",0.35)\n",
    "segmentation = key.pop(\"segmentation\",2)\n",
    "\n",
    "primary_key = dict(segmentation=segmentation,decimation_ratio=decimation_ratio,segment_id=segment_id)\n",
    "neuron_data = (ta3p100.CleansedMesh & primary_key).fetch1()\n",
    "\n",
    "print(neuron_data)\n",
    "vertices = neuron_data['vertices']#.astype(dtype=np.float)\n",
    "faces = neuron_data['triangles']#.astype(dtype=np.int)\n",
    "write_Whole_Neuron_Off_file(segment_id,vertices,faces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trimesh.base.Trimesh at 0x7f10b0691208>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimesh.load_mesh(\"temp/neuron_648518346349472574.off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshfix2 = pymeshfix.MeshFix(vertices,faces)\n",
    "meshfix2.repair(verbose=False,joincomp=True,remove_smallest_components=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_Whole_Neuron_Off_file(str(segment_id) + \"_pymesh_fixed\",meshfix2.v,meshfix2.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshfix = pymeshfix.MeshFix(vertices,faces)\n",
    "meshfix.repair(verbose=False,joincomp=True,remove_smallest_components=False)\n",
    "print(f\"Step 2: Pymesh shrinkwrapping: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_Whole_Neuron_Off_file(str(648518346349473044) + \"_fixed\",meshfix.v,meshfix.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshfix.write(\"neuron_648518346349473044_fixed.off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.faces[np.where(classifier.labels_list == 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 1\n",
    "neighbors_list = [13]\n",
    "min_max = {ni:[300000,-300000] for ni in neighbors_list}\n",
    "\n",
    "for i,ll in enumerate(classifier.labels_list):\n",
    "    if ll in neighbors_list:\n",
    "        verts_from_faces = classifier.faces[i]\n",
    "        for vert in verts_from_faces:\n",
    "            real_vert = classifier.vertices[vert]\n",
    "            if real_vert[axis] < min_max[ll][0]:\n",
    "                min_max[ll][0] = real_vert[axis]\n",
    "            if real_vert[axis] > min_max[ll][1]:\n",
    "                min_max[ll][1] = real_vert[axis]\n",
    "print(min_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.mesh.submesh(classifier.faces[np.where(classifier.labels_list == 13)])\n",
    "#classifier.mesh.submesh(np.where(classifier.labels_list == 13))[0].show()\n",
    "np.min(classifier.vertices[classifier.faces[np.where(classifier.labels_list == 13)].ravel()][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(self.faces[np.where(self.labels_list == label)],axis=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class that will handle the whole neuron segmentation:\n",
    "class WholeNeuronClassifier(object):\n",
    "    \n",
    "    \n",
    "    def generate_verts_to_face_dictionary(self,labels_list=[]):\n",
    "        if len(labels_list) <= 1:\n",
    "            print(\"len(labels_list) <= 1\")\n",
    "            labels_list = self.labels_list\n",
    "        \n",
    "        verts_to_Face = {i:[] for i,vertex in enumerate(self.vertices)}\n",
    "        verts_to_Label = {i:[] for i,vertex in enumerate(self.vertices)}\n",
    "\n",
    "\n",
    "        for i,verts in enumerate(self.faces):\n",
    "            #get the vertices\n",
    "\n",
    "            for vertex in verts:\n",
    "                verts_to_Face[vertex].append(i)\n",
    "\n",
    "        #use the verts to face to create the verts to label dictionary\n",
    "        for vert,face_list in verts_to_Face.items():\n",
    "            diff_labels = [labels_list[fc] for fc in face_list]\n",
    "            #print(list(set(diff_labels)))\n",
    "            verts_to_Label[vert] = list(set(diff_labels))\n",
    "            \n",
    "        self.verts_to_Face = verts_to_Face\n",
    "        self.verts_to_Label = verts_to_Label\n",
    "        \n",
    "        print(\"inside generate verts_to_face\")\n",
    "\n",
    "        return\n",
    "    \n",
    "#     def generate_verts_to_face_dictionary(self):\n",
    "#         \"\"\"\n",
    "#         generates the mapping of vertices to the faces that are touching it\n",
    "        \n",
    "#         \"\"\"\n",
    "#         verts_to_Face = {}\n",
    "\n",
    "#         #initialize the lookup dictionary as empty lists\n",
    "#         faces_raw = self.mesh.faces\n",
    "#         verts_raw = self.mesh.vertices\n",
    "        \n",
    "#         for i,pre_vertex in enumerate(verts_raw):\n",
    "#             verts_to_Face[i] = []\n",
    "        \n",
    "\n",
    "#         for i,verts in enumerate(faces_raw):\n",
    "#             #add the index to the list for each of the vertices\n",
    "#             for vertex in verts:\n",
    "#                 verts_to_Face[vertex].append(i)\n",
    "\n",
    "#         return verts_to_Face\n",
    "    \n",
    "    def __init__(self,mesh_file_location=\"\",file_name=\"\",key=dict()):\n",
    "    #import the mesh\n",
    "\n",
    "        full_path = str(Path(mesh_file_location) / Path(file_name))\n",
    "        self.mesh = trimesh.load_mesh(full_path)\n",
    "        \n",
    "        #get the vertices to faces lookup table\n",
    "        \n",
    "        original_start_time = time.time() \n",
    "        start_time = time.time()\n",
    "        if os.path.isfile(full_path):\n",
    "            print(\"Loading mesh from \" + str(full_path))\n",
    "    \n",
    "            my_mesh = trimesh.load_mesh(full_path)\n",
    "            vertices = my_mesh.vertices\n",
    "            faces = my_mesh.faces\n",
    "\n",
    "        elif \"segment_id\" in key.keys():\n",
    "\n",
    "            print(\"Loading mesh from datajoint- id: \" + str(key[\"segment_id\"]))\n",
    "            segment_id = key[\"segment_id\"]\n",
    "            decimation_ratio = key.pop(\"decimation_ratio\",0.35)\n",
    "            segmentation = key.pop(\"segmentation\",2)\n",
    "\n",
    "            primary_key = dict(segmentation=segmentation,decimation_ratio=decimation_ratio,segment_id=segment_id)\n",
    "            neuron_data = (ta3p100.CleansedMesh & primary_key).fetch1()\n",
    "\n",
    "            print(neuron_data)\n",
    "            vertices = neuron_data['vertices']#.astype(dtype=np.int32)\n",
    "            faces = neuron_data['triangles']#.astype(dtype=np.uint32)\n",
    "            \n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"No valid key or filename given\")\n",
    "            return \"failure\"\n",
    "        \n",
    "        self.vertices = vertices\n",
    "        self.faces = faces\n",
    "        #if need to generate verts to face\n",
    "        #self.verts_to_Face = self.generate_verts_to_face_dictionary()\n",
    "        \n",
    "        \"\"\"Ignoring for now so don't have to wait for pymeshfix to run\"\"\"\n",
    "#         start_time = time.time()\n",
    "#         print(\"Starting pymeshfix to clean Mesh\")\n",
    "#         meshfix = pymeshfix.MeshFix(vertices,faces)\n",
    "#         meshfix.repair(verbose=False,joincomp=True,remove_smallest_components=False)\n",
    "#         print(f\"Step 2: Pymesh shrinkwrapping: {time.time() - start_time}\")\n",
    "        \n",
    "#         self.verts = meshfix.v\n",
    "#         self.faces = meshfix.f\n",
    "\n",
    "        trimesh_object = trimesh.Trimesh()\n",
    "        trimesh_object.faces = self.faces\n",
    "        trimesh_object.vertices = self.vertices\n",
    "        self.mesh = trimesh_object\n",
    "        self.mesh_file_location = mesh_file_location\n",
    "        self.file_name = file_name\n",
    "        print(f\"Loading mesh and Pymesh fix: \\\n",
    "        {np.round(time.time() - start_time,5)} seconds\")\n",
    "    \n",
    "    #write the faces and vertices to an off file\n",
    "    def export_self_mesh(self,file_path_and_name):\n",
    "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "            self.mesh.export(file_path_and_name)\n",
    "    \n",
    "        \n",
    "    def load_cgal_segmentation(self,clusters=3,smoothness=0.20):\n",
    "        \"\"\"\n",
    "        Runs the cgal surface mesh segmentation on the mesh object\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #have to write the new mesh to an off file\n",
    "        new_mesh_file_path_and_name = str(Path(self.mesh_file_location) /\n",
    "                                            Path(self.file_name[:-4] + \"_fixed.off\"))\n",
    "        \n",
    "        self.export_self_mesh(new_mesh_file_path_and_name)\n",
    "        #add an extra end line to the off file\n",
    "        with open(new_mesh_file_path_and_name,'a') as fd:\n",
    "            fd.write(\"\\n\")\n",
    "        \n",
    "#         write_Whole_Neuron_Off_file(self.file_name[8:-4],self.mesh.vertices,\n",
    "#                                                            self.mesh.faces)\n",
    "        \n",
    "    \n",
    "        \"\"\"skip the segmentation for now\"\"\"\n",
    "        start_time = time.time()\n",
    "#         print(\"\\nStarting CGAL segmentation\")\n",
    "#         result = csm.cgal_segmentation(new_mesh_file_path_and_name[:-4],clusters,smoothness)\n",
    "#         print(result)\n",
    "        \n",
    "        self.clusters = clusters\n",
    "        self.smoothness = smoothness\n",
    "        self.labels_file = str(Path(self.mesh_file_location) / Path(self.file_name[:-4] + \"_fixed\" + \"-cgal_\" + str(np.round(clusters,2)) + \"_\" + \"{:.2f}\".format(smoothness) + \".csv\" ))  \n",
    "        self.sdf_file = str(Path(self.mesh_file_location) / Path(self.file_name[:-4] + \"_fixed\" + \"-cgal_\" + str(clusters) + \"_\" + \"{:.2f}\".format(smoothness) + \"_sdf.csv\" ))  \n",
    "        print(f\"Step 2: CGAL segmentation total time ---- {np.round(time.time() - start_time,5)} seconds\")\n",
    "\n",
    "        \n",
    "        \n",
    "    #used for when not pulling from datajoint\n",
    "    def get_cgal_data_and_label_local_optomized(self):\n",
    "        \n",
    "        labels_file = self.labels_file\n",
    "        sdf_file = self.sdf_file\n",
    "        \n",
    "        #reads int the cgal labels for all of the faces\n",
    "        triangles_labels = np.zeros(len(self.mesh.faces)).astype(\"int64\")\n",
    "        with open(labels_file) as csvfile:\n",
    "            #print(\"inside labels file\")\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                triangles_labels[i] = int(row[0])\n",
    "\n",
    "\n",
    "        #converts the cgal labels into a list that\n",
    "        # starts at 0\n",
    "        # progresses in order for all unique labels (so no numbers are skipped and don't have corresponding face)\n",
    "        verts_raw = self.mesh.vertices\n",
    "        faces_raw = self.mesh.faces\n",
    "        #gets a list of the unique labels\n",
    "        unique_segments = list(Counter(triangles_labels).keys())\n",
    "        segmentation_length = len(unique_segments) \n",
    "        unique_index_dict = {unique_segments[x]:x for x in range(0,segmentation_length )}\n",
    "        \n",
    "        labels_list = np.zeros(len(triangles_labels)).astype(\"int64\")\n",
    "        for i,tri in enumerate(triangles_labels):\n",
    "\n",
    "            #assembles the label list that represents all of the faces\n",
    "            labels_list[i] = int(unique_index_dict[tri])\n",
    "\n",
    "        #write thses new labels to a file\n",
    "        with open(labels_file[:-4] + \"_revised.csv\",mode=\"w\") as csvfile:\n",
    "            csv_writer = csv.writer(csvfile,delimiter=',')\n",
    "            for i in labels_list:\n",
    "                csv_writer.writerow([i])\n",
    "\n",
    "        #print(\"done with cgal_segmentation\")\n",
    "\n",
    "        #----------------------now return a dictionary of the sdf values like in the older function get_sdf_dictionary\n",
    "        #get the sdf values and store in sdf_labels\n",
    "        sdf_labels = np.zeros(len(labels_list)).astype(\"float\")\n",
    "        with open(sdf_file) as csvfile:\n",
    "\n",
    "            for i,row in enumerate(csv.reader(csvfile)):\n",
    "                sdf_labels[i] = float(row[0])\n",
    "\n",
    "        \n",
    "        sdf_temp_dict = {}\n",
    "        for i in range(0,segmentation_length):\n",
    "            sdf_temp_dict[i] = []\n",
    "        \n",
    "        #print(\"sdf_temp_dict = \" + str(sdf_temp_dict))\n",
    "        #print(\"sdf_labels = \" + str(sdf_labels))\n",
    "        #iterate through the labels_list\n",
    "        for i,label in enumerate(labels_list):\n",
    "            sdf_temp_dict[label].append(sdf_labels[i])\n",
    "        #print(sdf_temp_dict)\n",
    "\n",
    "        #now calculate the stats on the sdf values for each label\n",
    "        sdf_final_dict = {}\n",
    "        \n",
    "        for dict_key,value in sdf_temp_dict.items():\n",
    "\n",
    "            sdf_final_dict[dict_key] = dict(median=np.median(value),mean=np.mean(value),max=np.amax(value))\n",
    "\n",
    "\n",
    "        self.sdf_final_dict = sdf_final_dict\n",
    "        self.labels_list = labels_list\n",
    "        self.labels_list_counter = Counter(labels_list)\n",
    "    \n",
    "        adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "        \n",
    "        self.adjacency_labels_col1, self.adjacency_labels_col2 = adjacency_labels.T\n",
    "        #generate the vertices labels\n",
    "        self.generate_verts_to_face_dictionary(labels_list)\n",
    "        \n",
    "        return \n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_highest_sdf_part(self,size_threshold=3000,exclude_label=None):\n",
    "        \"\"\"\n",
    "        Based ont the sdf data and the labels data,\n",
    "        Finds the label with the highest median,\n",
    "            label with highest max,\n",
    "            label with highest mean sdf value\n",
    "        \n",
    "        *** but only for those that meet the certain threshold ***\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        high_median_val = 0\n",
    "        high_median = -1\n",
    "        high_mean_val = 0\n",
    "        high_mean = -1\n",
    "        high_max_val = 0\n",
    "        high_max = -1\n",
    "\n",
    "\n",
    "\n",
    "        #gets all of the labels\n",
    "        my_list = Counter(self.labels_list)\n",
    "        my_list_keys = list(my_list.keys())\n",
    "        if exclude_label != None:\n",
    "            my_list_keys.remove(exclude_label)\n",
    "\n",
    "        #OPTOMIZE\n",
    "        \n",
    "        for x in my_list_keys:\n",
    "            #print(\"x = \" + str(x))\n",
    "            #print(\"high_median_val = \" + str(high_median_val))\n",
    "            #print('sdf_final_dict[x][\"median\"] = ' + str(sdf_final_dict[x][\"median\"]))\n",
    "            if self.sdf_final_dict[x][\"median\"] > high_median_val and my_list[x] > size_threshold:\n",
    "                high_median = x\n",
    "                high_median_val = self.sdf_final_dict[x][\"median\"]\n",
    "            if self.sdf_final_dict[x][\"mean\"] > high_mean_val  and my_list[x] > size_threshold:\n",
    "                high_mean = x\n",
    "                high_mean_val = self.sdf_final_dict[x][\"mean\"]\n",
    "            if self.sdf_final_dict[x][\"max\"] > high_max_val  and my_list[x] > size_threshold:\n",
    "                high_max = x\n",
    "                high_max_val = self.sdf_final_dict[x][\"max\"]\n",
    "\n",
    "\n",
    "        self.highest_vals= [high_median,high_median_val,high_mean,high_mean_val,high_max,high_max_val]\n",
    "        \n",
    "        self.high_median = self.highest_vals[0]\n",
    "        return \n",
    "    \n",
    "\n",
    "    #getting bad connection data from here\n",
    "    def get_graph_structure(self):\n",
    "        \"\"\"\n",
    "        For each unique label gets:\n",
    "        1) all neighbors\n",
    "        2) number of faces belonging to that label\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        connections = {label_name:[] for label_name in self.labels_list_counter.keys()}\n",
    "        mesh_Number = {label_name:number for label_name,number in self.labels_list_counter.items()}\n",
    "        #label_vert_stats = {label_name:[300000,-300000] for label_name in Counter(labels_list).keys()}\n",
    "\n",
    "        #verts to label curently is the has every vertex and the labels it is toughing in a list\n",
    "        for verts,total_labels in self.verts_to_Label.items():\n",
    "            if len(total_labels) > 1: #if more than one label\n",
    "                for face in total_labels:\n",
    "                    for fc in [v for v in total_labels if v != face]:\n",
    "                        if fc not in connections[face]:\n",
    "                            connections[face].append(fc)\n",
    "\n",
    "        self.connections = connections\n",
    "        self.mesh_Number = mesh_Number\n",
    "        \n",
    "        print(\"INSIDE GET GRAPH STRUCTURE and connections[5] = \" + str(connections[5]))\n",
    "\n",
    "        return \n",
    "    \n",
    "#     def find_max_min_z_vals(self,neighbors_list,axis=1):\n",
    "#         \"\"\"\n",
    "#         for each of the neighbors of the soma and including the soma,\n",
    "#         will find the maximum and minimum value for the axis direction\n",
    "#         \"\"\"\n",
    "#         min_max = {ni:[300000,-300000] for ni in neighbors_list}\n",
    "\n",
    "#         for i,ll in enumerate(self.labels_list):\n",
    "#             if ll in neighbors_list:\n",
    "#                 verts_from_faces = self.faces[i]\n",
    "#                 for vert in verts_from_faces:\n",
    "#                     real_vert = self.vertices[vert]\n",
    "#                     if real_vert[axis] < min_max[ll][0]:\n",
    "#                         min_max[ll][0] = real_vert[axis]\n",
    "#                     if real_vert[axis] > min_max[ll][1]:\n",
    "#                         min_max[ll][1] = real_vert[axis]\n",
    "#         self.min_max = min_max\n",
    "#         return min_max\n",
    "    \n",
    "    \n",
    "   \n",
    "    def find_Soma_Caps(self,soma_index,min_width=0.23,max_faces=6000,max_n_connection=6,large_sized_convex=3):\n",
    "        #get the soma neighbors\n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        \n",
    "        total_soma_caps = []\n",
    "        for i in soma_neighbors:\n",
    "            soma_cap = True\n",
    "            \n",
    "            #collect the mesh of the cap\n",
    "            submesh = self.mesh.submesh(np.where(self.labels_list == i))[0]\n",
    "\n",
    "            mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "            n_faces = len(submesh.faces)\n",
    "            width_data = self.sdf_final_dict[i]\n",
    "            width_data_median = self.sdf_final_dict[i][\"median\"]\n",
    "            n_connections = len(self.connections[i])\n",
    "            \n",
    "#             131 mean_convex = 7.214328769367434, n_faces=483, width_data=0.256008, n_connections=2\n",
    "#             129 mean_convex = 9.830215623946183, n_faces=430, width_data=0.2710475, n_connections=2\n",
    "            \n",
    "#             if i in [129,131]:\n",
    "#                 print(f\" {i} mean_convex = {mean_convex}, n_faces={n_faces}, width_data={width_data_median}, n_connections={n_connections}\")\n",
    "                \n",
    "                \n",
    "            \n",
    "            if width_data[\"median\"] < min_width or n_faces>max_faces or n_connections>max_n_connection: \n",
    "                soma_cap = False\n",
    "            \n",
    "            #use the convex data if size is really big:\n",
    "            if n_faces > 1500:\n",
    "                if mean_convex > 5:\n",
    "                    #print(f\" {i} Doesn't meet second pass\")\n",
    "                    soma_cap = False\n",
    "            \n",
    "            if soma_cap == True:\n",
    "                total_soma_caps.append(i)\n",
    "            \n",
    "#             print(total_soma_caps)\n",
    "        \n",
    "#         print(total_soma_caps)\n",
    "        #for all the soma caps replace the labels list with soma_index and recompute neighbors and connections:\n",
    "        if len(total_soma_caps) > 0:\n",
    "            print(f\"found {len(total_soma_caps)} soma caps and replacing labels\")\n",
    "            start_time = time.time()\n",
    "            self.labels_list[np.where(np.isin(self.labels_list,total_soma_caps))] = soma_index\n",
    "            \n",
    "            #call the functions to recompute the connections/neighbors and others (but don't need to generate SDF labels again)\n",
    "            \n",
    "            #write thses new labels to a file\n",
    "            with open(self.labels_file[:-4] + \"_revised.csv\",mode=\"w\") as csvfile:\n",
    "                csv_writer = csv.writer(csvfile,delimiter=',')\n",
    "                for i in self.labels_list:\n",
    "                    csv_writer.writerow([i])\n",
    "            \n",
    "            self.labels_list_counter = Counter(self.labels_list)\n",
    "\n",
    "            adjacency_labels = self.labels_list[self.mesh.face_adjacency]\n",
    "\n",
    "            self.adjacency_labels_col1, self.adjacency_labels_col2 = adjacency_labels.T\n",
    "            #generate the vertices labels\n",
    "            print(f\"Time to generate new labels_list for soma caps: {time.time() - start_time}\")\n",
    "            start_time = time.time()\n",
    "            self.generate_verts_to_face_dictionary(self.labels_list)\n",
    "            print(f\"Time to generate new dictionaries for soma caps: {time.time() - start_time}\")\n",
    "            \n",
    "            start_time\n",
    "            self.get_graph_structure()\n",
    "            print(\"done creating connections -- %s seconds --\" %(time.time() - start_time))\n",
    "            \n",
    "\n",
    "    def find_Apical(self,soma_index):\n",
    "        \"\"\"Returns the index of the most likely apical \n",
    "        1) calculate the height of 70% up the soma\n",
    "        2) find all the neighbors of the soma using verts_to_Label\n",
    "        3) filter out the neighbors that go below that\n",
    "        4) filter away the neighbors that don't meet minimum number of face, height change and sdf median\n",
    "        5) If multiple, pick the one that has the most number of neighbors\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Soma Index = \" + str(soma_index))\n",
    "        print(\"Soma Connections = \" + str(self.connections[soma_index]))\n",
    "        mesh_Threshold = 2000\n",
    "        height_Threshold =5000\n",
    "        sdf_Threshold = 0.09\n",
    "        #1) calculate the height of 70% up the soma (but have to adjust because the negative direction of y is \n",
    "        #direction of the apical), this new method gets the height of the first 30% of the somae which is actually\n",
    "        # the top 30% of the soma once it is flipped in the right orientation\n",
    "#         neighbor_verts = self.faces[np.where(self.labels_list == soma_index)]\n",
    "#         soma_y_max = np.max(neighbor_verts,axis=0)[1]\n",
    "#         soma_y_min = np.min(neighbor_verts,axis=0)[1]\n",
    "        soma_verts = self.vertices[self.faces[np.where(self.labels_list == soma_index)].ravel()][:,1]\n",
    "        soma_y_min = np.min(soma_verts)\n",
    "        soma_y_max = np.max(soma_verts)\n",
    "        self.soma_y_min = soma_y_min\n",
    "        self.soma_y_max = soma_y_max\n",
    "#         print(\"soma_y_max =\"  + str(soma_y_max))\n",
    "#         print(\"soma_y_min =\"  + str(soma_y_min))\n",
    "        \n",
    "        \n",
    "        soma_70_percent = (soma_y_max - soma_y_min)*0.3 +  soma_y_min\n",
    "        print(\"soma_70_percent = \" + str(soma_70_percent))\n",
    "        #2) find all the neighbors of the soma using verts_to_Label\n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        #3) filter out the neighbors that go below that\n",
    "        \n",
    "        #find the maximum y value for the labels\n",
    "\n",
    "        possible_Axons_filter_1 = [label for label in soma_neighbors \n",
    "                            if np.max(self.vertices[self.faces[np.where(self.labels_list == label)].ravel()][:,1]) < soma_70_percent]\n",
    "\n",
    "        #4) filter away the neighbors that don't meet minimum number of face, height change and sdf median\n",
    "        print(\"possible_Axons_filter_1 = \" + str(possible_Axons_filter_1))\n",
    "        possible_Axons_filter_2 = [lab for lab in possible_Axons_filter_1 if \n",
    "                                        self.mesh_Number[lab] > mesh_Threshold and \n",
    "        np.max(self.vertices[self.faces[np.where(self.labels_list == lab)].ravel()][:,1]) - np.min(self.vertices[self.faces[np.where(self.labels_list == lab)].ravel()][:,1]) > height_Threshold and\n",
    "                                        self.sdf_final_dict[lab][\"median\"] > sdf_Threshold]\n",
    "        print(\"possible_Axons_filter_2 = \" + str(possible_Axons_filter_2))\n",
    "        if len(possible_Axons_filter_2) <= 0:\n",
    "            return \"None\"\n",
    "        elif len(possible_Axons_filter_2) == 1:\n",
    "            return possible_Axons_filter_2[0]\n",
    "        else:\n",
    "            #find the one with the most neighbors \n",
    "            ##### MIGHT WANT TO ADD IN WHERE FINDS THE THICKEST WIDTH !\n",
    "            current_apical = possible_Axons_filter_2[0]\n",
    "            current_apical_neighbors = len(self.connections[possible_Axons_filter_2[0]])\n",
    "            for i in range(1,len(possible_Axons_filter_2)):\n",
    "                if len(self.connections[possible_Axons_filter_2[i]]) > current_apical_neighbors:\n",
    "                    current_apical = possible_Axons_filter_2[i]\n",
    "                    current_apical_neighbors = len(self.connections[possible_Axons_filter_2[i]])\n",
    "\n",
    "            return current_apical\n",
    "    \n",
    "    def classify_whole_neuron(self,possible_Apical,soma_index,threshold=700,axon_std_threshold=69):\n",
    "        #check to see if no soma index\n",
    "        if soma_index < 0:\n",
    "            self.whole_neuron_labels ={lb:\"unsure\" for lb in self.connections.keys()}\n",
    "            return\n",
    "        \n",
    "        #creates dictionary with unique labels whose value will store their final label\n",
    "        whole_neuron_labels ={lb:\"unsure\" for lb in self.connections.keys()}\n",
    "        whole_neuron_labels[soma_index] = \"soma\"\n",
    "\n",
    "        #create a networkx graph based on connections\n",
    "        G=nx.Graph(self.connections)\n",
    "\n",
    "        \n",
    "        #removes the soma from the list of nodes, but not actually remove it from the graph\n",
    "        node_list = list(G.nodes)\n",
    "        if(soma_index in node_list):\n",
    "            node_list.remove(soma_index)\n",
    "        else:\n",
    "            #didn't find soma\n",
    "            return []\n",
    "\n",
    "        \n",
    "        #finds the shortest path from any label to the soma\n",
    "        shortest_paths = {}\n",
    "        for node in node_list:\n",
    "            shortest_paths[node] = [k for k in nx.shortest_path(G,node,soma_index)]\n",
    "\n",
    "        #find the direct neighbors of the soma\n",
    "        soma_branches = dict()\n",
    "        \n",
    "        soma_neighbors = self.connections[soma_index]\n",
    "        \n",
    "        \n",
    "        print(\"soma_neighbors = \" + str(soma_neighbors))\n",
    "        \n",
    "        #assemble each of these compartments into groups\n",
    "        for node,path in shortest_paths.items():\n",
    "            if possible_Apical not in path:\n",
    "                specific_soma_neighbor = (set(path).intersection(set(soma_neighbors))).pop()\n",
    "                \n",
    "                if specific_soma_neighbor not in soma_branches.keys():\n",
    "                    soma_branches[specific_soma_neighbor] = []\n",
    "                soma_branches[specific_soma_neighbor].append(node)\n",
    "        \n",
    "        #return soma_branches\n",
    "        \n",
    "        print(\"soma_branches = \" + str(soma_branches))\n",
    "        #have groups of branches and assmble them into trimesh objects\n",
    "        branches_submeshes = {}\n",
    "        for group,group_list in soma_branches.items():\n",
    "            total_indices = []\n",
    "            for g in group_list:\n",
    "                face_indices = np.where(self.labels_list == g)\n",
    "                total_indices += face_indices\n",
    "            \n",
    "            #create a trimesh submshesh\n",
    "            \n",
    "            branches_submeshes[group] = self.mesh.submesh(total_indices,append=True)\n",
    "        #return branches_submeshes\n",
    "        \n",
    "        #iterate through meshes and assign certain labels to these guys\n",
    "        ## define certain thresholds for determining label\n",
    "        cilia_threshold = 1000\n",
    "        stub_threshold = 200\n",
    "        non_dendrite_convex_threshold = 26.5\n",
    "        \n",
    "        \n",
    "        #Calculate the soma 40%\n",
    "\n",
    "        \n",
    "        soma_height = self.soma_y_max - self.soma_y_min\n",
    "        \n",
    "        soma_lower_30 = self.soma_y_max - 0.3*soma_height\n",
    "        print(\"self.soma_y_max = \" + str(self.soma_y_max))\n",
    "        print(\"self.soma_y_min = \" + str(self.soma_y_min))\n",
    "        print(\"soma_lower_30 = \" + str(soma_lower_30))\n",
    "        \n",
    "        \n",
    "        for neighbor,submesh in branches_submeshes.items():\n",
    "            \n",
    "            #get the number of faces\n",
    "            total_faces = len(submesh.faces)\n",
    "            #print(f\"total_faces  = {total_faces}\")\n",
    "            \n",
    "            if total_faces < stub_threshold:\n",
    "                print(f\"{neighbor} = stub soma\")\n",
    "                for x in soma_branches[neighbor]:\n",
    "                    \n",
    "                    whole_neuron_labels[x] = \"soma\"\n",
    "            else:\n",
    "            \n",
    "                mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "                #print(f\"total_faces  = {mean_convex}\")\n",
    "                if mean_convex > non_dendrite_convex_threshold:\n",
    "                    print(\"neighbor inside cilia check = \" + str(neighbor))\n",
    "                    #classify according to size\n",
    "\n",
    "                    if total_faces < cilia_threshold:\n",
    "                        print(f\"{neighbor} = cilia\")\n",
    "                        for x in soma_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"cilia\"\n",
    "                    else:\n",
    "                        print(f\"{neighbor} = axon\")\n",
    "                        for x in soma_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"error\"\n",
    "                else: #try to see if there is any axon\n",
    "                    #calculate the standard deviation\n",
    "                    print(\"neighbor inside axon check = \" + str(neighbor))\n",
    "                    std_dev_convex = np.std((trimesh.convex.adjacency_projections(submesh)))\n",
    "                    if neighbor == 83:\n",
    "                        print(\"std_dev_convex = \" + str(std_dev_convex))\n",
    "                        print(\"axon_std_threshold = \" + str(axon_std_threshold))\n",
    "                    if std_dev_convex < axon_std_threshold:\n",
    "                        #find the minimum y heght of neighbor\n",
    "                        neighbor_y_min = np.min(self.vertices[self.faces[np.where(self.labels_list == neighbor)].ravel()][:,1])\n",
    "                        neighbor_y_max = np.max(self.vertices[self.faces[np.where(self.labels_list == neighbor)].ravel()][:,1])\n",
    "                        print(\"neighbor_y_min = \" + str(neighbor_y_min))\n",
    "                        print(\"neighbor_y_max = \" + str(neighbor_y_max))\n",
    "                        \n",
    "                        if neighbor_y_min > soma_lower_30:\n",
    "                            #make sure that it doesn't go higher than 40% soma height\n",
    "                            for x in soma_branches[neighbor]:\n",
    "                                whole_neuron_labels[x] = \"axon\"\n",
    "                        else:\n",
    "                            print(f\"MET AXON THRESHOLD CRITERIA but not low enough on soma for neighbor = {neighbor}\")\n",
    "                            \n",
    "        \n",
    "        # checks if apical is present or not, and if not then just labels everything else basal\n",
    "        if possible_Apical == \"None\":\n",
    "            #label everything as basal if don't know\n",
    "            for k,vals in whole_neuron_labels.items():\n",
    "                if k != soma_index and vals == \"unsure\":\n",
    "                    whole_neuron_labels[k] = \"basal\"\n",
    "            self.whole_neuron_labels = whole_neuron_labels\n",
    "            \n",
    "            return \n",
    "                    \n",
    "            \n",
    "        #return branches_submeshes\n",
    "        \n",
    "        \"\"\" 4-29 added edition that will prevent small spines off of apical \n",
    "        from being considered oblique branches\n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        #find the direct neighbors of the soma\n",
    "        apical_branches = dict()\n",
    "        \n",
    "        apical_neighbors = self.connections[possible_Apical]\n",
    "        apical_neighbors.remove(soma_index)\n",
    "        \n",
    "        \n",
    "        #assemble each of these compartments into groups\n",
    "        for node,path in shortest_paths.items():\n",
    "            if possible_Apical in path and node != possible_Apical: #make sure only those obliques and not actual apical\n",
    "                \n",
    "                specific_apical_neighbor = (set(path).intersection(set(apical_neighbors))).pop()\n",
    "                \n",
    "                if specific_apical_neighbor not in apical_branches.keys():\n",
    "                    apical_branches[specific_apical_neighbor] = []\n",
    "                apical_branches[specific_apical_neighbor].append(node)\n",
    "        \n",
    "        #return soma_branches\n",
    "        \n",
    "        print(\"apical_branches = \" + str(apical_branches))\n",
    "        #have groups of branches and assmble them into trimesh objects\n",
    "        branches_submeshes_apical = {}\n",
    "        for group,group_list in apical_branches.items():\n",
    "            total_indices = []\n",
    "            for g in group_list:\n",
    "                face_indices = np.where(self.labels_list == g)\n",
    "                total_indices += face_indices\n",
    "            \n",
    "            #create a trimesh submshesh\n",
    "            \n",
    "            branches_submeshes_apical[group] = self.mesh.submesh(total_indices,append=True)\n",
    "        #return branches_submeshes_apical\n",
    "        \n",
    "        #iterate through meshes and assign certain labels to these guys\n",
    "        ## define certain thresholds for determining label\n",
    "        \n",
    "        stub_threshold_apical = 700\n",
    "        apical_height_threshold = 0.75\n",
    "\n",
    "        for neighbor,submesh in branches_submeshes_apical.items():\n",
    "            \n",
    "            #get the number of faces\n",
    "            total_faces = len(submesh.faces)\n",
    "            #print(f\"total_faces  = {total_faces}\")\n",
    "            \n",
    "            if total_faces < stub_threshold_apical:\n",
    "                print(f\"{neighbor} = stub apical\")\n",
    "                for x in apical_branches[neighbor]:\n",
    "                    \n",
    "                    whole_neuron_labels[x] = \"apical\"\n",
    "            else:\n",
    "            \n",
    "                mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "                #print(f\"total_faces  = {mean_convex}\")\n",
    "                if mean_convex > non_dendrite_convex_threshold:\n",
    "                    #classify according to size\n",
    "\n",
    "                    print(f\"{neighbor} = error\")\n",
    "                    for x in apical_branches[neighbor]:\n",
    "                        whole_neuron_labels[x] = \"error\"\n",
    "                else: #try to see if there is any axon\n",
    "                    #calculate the standard deviation\n",
    "                    std_dev_convex = np.std((trimesh.convex.adjacency_projections(submesh)))\n",
    "                    if std_dev_convex < axon_std_threshold:\n",
    "                        for x in apical_branches[neighbor]:\n",
    "                            whole_neuron_labels[x] = \"error\"\n",
    "\n",
    "\n",
    "        for label_name, path in shortest_paths.items():\n",
    "            if label_name == possible_Apical: #labels the possible apical as apical\n",
    "                whole_neuron_labels[label_name] = \"apical\"\n",
    "            else:\n",
    "                if possible_Apical in path:\n",
    "                    #if has apical on path and not the apical itself, soma or other label --> label oblique\n",
    "                    for jj in path: \n",
    "                        if jj != possible_Apical and jj != soma_index and whole_neuron_labels[jj] == \"unsure\":\n",
    "                            whole_neuron_labels[jj] = \"oblique\" \n",
    "                else:\n",
    "                    #if NO apical on path and not the apical itself, soma or other label --> label oblique\n",
    "                    for jj in path:\n",
    "                        if jj != possible_Apical and jj != soma_index and whole_neuron_labels[jj] == \"unsure\":\n",
    "                            whole_neuron_labels[jj] = \"basal\" \n",
    "\n",
    "        #return the final list of labels:\n",
    "        self.whole_neuron_labels = whole_neuron_labels\n",
    "        return\n",
    "    \n",
    "    def label_whole_neuron(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        iterates through all of faces and labels them accoring\n",
    "        to the labels assigned to the cgal generic labels\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #instead of going to datajoint for labels\n",
    "        #this just have it locally so don't rely on datajoint\n",
    "\n",
    "        apical_index = 2\n",
    "        basal_index = 3\n",
    "        oblique_index = 4\n",
    "        soma_index = 5\n",
    "        cilia_index = 12\n",
    "        error_index = 10 \n",
    "        axon_index=6\n",
    "\n",
    "\n",
    "        self.final_faces_labels_list = np.zeros(len(self.faces))\n",
    "        \n",
    "\n",
    "        unknown_counter = 0\n",
    "\n",
    "        for i,lab in enumerate(self.labels_list):\n",
    "            #get the category according to the dictionary\n",
    "            cat = self.whole_neuron_labels[lab]\n",
    "            if cat == \"apical\":\n",
    "                self.final_faces_labels_list[i] = apical_index\n",
    "            elif cat == \"basal\":\n",
    "                self.final_faces_labels_list[i] = basal_index\n",
    "            elif cat == \"oblique\":\n",
    "                self.final_faces_labels_list[i] = oblique_index\n",
    "            elif cat == \"soma\":\n",
    "                self.final_faces_labels_list[i] = soma_index\n",
    "            elif cat == \"cilia\":\n",
    "                self.final_faces_labels_list[i] = cilia_index\n",
    "            elif cat == \"axon\":\n",
    "                self.final_faces_labels_list[i] = axon_index\n",
    "            elif cat == \"error\":\n",
    "                self.final_faces_labels_list[i] = error_index\n",
    "            else:\n",
    "                #if wasn't labeled anything just assing it a random color based on cgal assignment\n",
    "                self.output_faces_labels_list[i] = 18 + (int(lab))\n",
    "\n",
    "        \n",
    "    def generate_output_lists(self):\n",
    "        \"\"\"\n",
    "        Will generate the final faces and vertices labels for the classification\n",
    "        \"\"\"\n",
    "        #DON'T THINK I NEED THIS\n",
    "#         face_Counter = Counter(self.final_faces_labels_list)\n",
    "#         #print(face_Counter)\n",
    "\n",
    "#         accepted_color_length = 18\n",
    "#         random_labels = {int(l):int(accepted_color_length+i) for i,l in enumerate(face_Counter.keys()) if l >= accepted_color_length}\n",
    "#         color_length = accepted_color_length\n",
    "#         for i in range(0,color_length):\n",
    "#             random_labels[i] = i\n",
    "\n",
    "\n",
    "#         output_faces_list = [random_labels[int(ll)] for ll in final_faces_labels_list]\n",
    "        output_faces_list = self.final_faces_labels_list\n",
    "        \n",
    "\n",
    "        #generate the vertices labels\n",
    "        self.generate_verts_to_face_dictionary(output_faces_list)\n",
    "\n",
    "        output_verts_list = [int(self.verts_to_Label[v][0]) for v in self.verts_to_Label]\n",
    "\n",
    "        self.output_verts_labels_list = output_verts_list\n",
    "        return self.final_faces_labels_list, self.output_verts_labels_list \n",
    "        \n",
    "#     def label_cilia_axons_and_extract_branches(self,split_up_branches=True,soma_mesh=False,stub_threshold=0):\n",
    "#         \"\"\"\n",
    "        \n",
    "#         Process:\n",
    "#         1) Divide the basal parts into seperate parts\n",
    "#         2) Use the convex data and face side to decide what parts are errors, axons and cilia\n",
    "#         a. All of these will have a higher convex number\n",
    "#         b. The stubs will be the ones with face number below stub threshold\n",
    "#         c. The axons will be those above axon threshold\n",
    "#         d. Cillia will be those left over\n",
    "        \n",
    "#         Tricky part\n",
    "#         For each part that needs to be relabeled, find the original face indices and relabel\n",
    "        \n",
    "#         Resplit the mesh after the relabeling\n",
    "        \n",
    "#         According to the parameters set return either the branches individually, as a whole or either with the soma seperately\n",
    "        \n",
    "        \n",
    "#         \"\"\"\n",
    "        \n",
    "#         pass\n",
    "\n",
    "\n",
    "    def return_branches(self,stub_threshold=200,split_up_spines=True,shaft_mesh=False):\n",
    "        apical_index = 2\n",
    "        basal_index = 3\n",
    "        oblique_index = 4\n",
    "        soma_index = 5\n",
    "        cilia_index = 12\n",
    "        error_index = 10 \n",
    "        axon_index=6\n",
    "        \n",
    "        basal_indexes = np.where(self.final_faces_labels_list == basal_index)[0]\n",
    "        oblique_indexes = np.where(self.final_faces_labels_list == oblique_index)[0]\n",
    "        apical_indexes = np.where(self.final_faces_labels_list == apical_index)[0]\n",
    "        #axon_indexes = np.where(self.final_faces_labels_list == axon_index)\n",
    "        spine_indexes = [np.concatenate([basal_indexes,oblique_indexes,apical_indexes])]\n",
    "        \n",
    "        spine_meshes_whole = self.mesh.submesh(spine_indexes,append=True)\n",
    "        \n",
    "        #decides if passing back spines as one whole mesh or seperate meshes\n",
    "        if split_up_spines==True:\n",
    "            individual_spines = []\n",
    "            temp_spines = spine_meshes_whole.split(only_watertight=False)\n",
    "            for spine in temp_spines:\n",
    "                if len(spine.faces) >= stub_threshold:\n",
    "                    individual_spines.append(spine)\n",
    "        else:\n",
    "            individual_spines = spine_meshes_whole\n",
    "        #will also pass back the shaft of the mesh with the extracted spines\n",
    "        if shaft_mesh==False:\n",
    "            return individual_spines\n",
    "        else:\n",
    "            shaft_indexes = np.where(np.array(self.labels_list) == soma_index) \n",
    "            shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "            return individual_spines,shaft_mesh_whole\n",
    "        \n",
    "        #divide into disconnected meshes and return this array\n",
    "        return individual_spines\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     def extract_branches(self,labels_file_location,file_name,clusters,smoothness,\n",
    "#                                            split_up_spines=True,shaft_mesh=False,**kwargs):\n",
    "\n",
    "\n",
    "#             smooth_backbone_parameters = kwargs.pop('smooth_backbone_parameters', dict())\n",
    "#             stub_threshold = kwargs.pop('stub_threshold', 50)\n",
    "\n",
    "\n",
    "#             status = self.get_spine_classification(labels_file_location,file_name,clusters,\n",
    "#                                           smoothness,smooth_backbone_parameters,stub_threshold)\n",
    "\n",
    "#             if status != \"Success\":\n",
    "#                 print(\"spine classification did not execute properly\")\n",
    "#                 return None\n",
    "\n",
    "#             spine_indexes = np.where(np.array(self.labels_list) != -1)\n",
    "#             spine_meshes_whole = self.mesh.submesh(spine_indexes,append=True)\n",
    "\n",
    "#             #decides if passing back spines as one whole mesh or seperate meshes\n",
    "#             if split_up_spines==True:\n",
    "#                 individual_spines = []\n",
    "#                 temp_spines = spine_meshes_wholes_whole.split(only_watertight=False)\n",
    "#                 for spine in temp_spines:\n",
    "#                     if len(spine.faces) >= stub_threshold:\n",
    "#                         individual_spines.append(spine)\n",
    "#             else:\n",
    "#                 individual_spines = spine_meshes_whole\n",
    "\n",
    "#             #will also pass back the shaft of the mesh with the extracted spines\n",
    "#             if shaft_mesh==False:\n",
    "#                 return individual_spines\n",
    "#             else:\n",
    "#                 shaft_indexes = np.where(np.array(self.labels_list) == -1) \n",
    "#                 shaft_mesh_whole = self.mesh.submesh(shaft_indexes,append=True)\n",
    "#                 return individual_spines,shaft_mesh_whole\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         #divide into disconnected meshes and return this array\n",
    "#         return individual_spines\n",
    "            \n",
    "    \n",
    "    def clean_files(self):\n",
    "        #clean the files \n",
    "        \n",
    "        #1) new mesh file\n",
    "        #2) cgal files (sdf and labels)\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment_id = 648518346349471500\n",
    "#segment_id = 648518346349471156\n",
    "global_start = time.time()\n",
    "#classifier = WholeNeuronClassifier(\"./temp\",\"neuron_\" + str(segment_id) + \".off\")\n",
    "segment_id = 648518346349473045\n",
    "segment_id = 648518346349386137\n",
    "segment_id = 648518346349472574 #not correctly identifying the soma\n",
    "#segment_id = 648518346349473045 #one correctly working for\n",
    "\n",
    "classifier = WholeNeuronClassifier(\"./temp\",\"neuron_\" + str(segment_id) + \".off\")\n",
    "classifier.load_cgal_segmentation(clusters=4,smoothness=0.30)\n",
    "#retrieves the cgal data\n",
    "classifier.get_cgal_data_and_label_local_optomized()\n",
    "\n",
    "#get the highest values of sdf\n",
    "start_time = time.time()\n",
    "classifier.get_highest_sdf_part(size_threshold=3000)\n",
    "\n",
    "print(\"got highest part--- %s seconds ---\" % (time.time() - start_time))\n",
    "start_time = time.time()\n",
    "\n",
    "#wants to generate the verts_to_face (have to do this again for the mesh)\n",
    "classifier.generate_verts_to_face_dictionary()\n",
    "print(\"done creating dictionaries -- %s seconds --\" %(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "#create a graph structure and stats for the whole neuron\n",
    "classifier.get_graph_structure()\n",
    "print(\"done creating connections -- %s seconds --\" %(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "soma_index=classifier.high_median\n",
    "# neighbors_list = classifier.connections[soma_index].copy()\n",
    "\n",
    "# neighbors_list.append(soma_index)\n",
    "# # min_max = classifier.find_max_min_z_vals(neighbors_list,axis=1)\n",
    "# # print(\"done FINDING NEIGHBOR STATS -- %s seconds --\" %(time.time() - start_time))\n",
    "# # #print(\"min_max = \" + str(min_max))\n",
    "\n",
    "start_time = time.time()\n",
    "classifier.find_Soma_Caps(soma_index,min_width=0.23,max_faces=6000,max_n_connection=6,large_sized_convex=3)\n",
    "print(\"done FINDING Soma Caps -- %s seconds --\" %(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "#send data to function that will find the Apical\n",
    "possible_Apical = classifier.find_Apical(soma_index)\n",
    "print(\"possible_Apical = \" + str(possible_Apical))\n",
    "print(\"done finding possible apicals -- %s seconds --\" %(time.time() - start_time))\n",
    "\n",
    "#use the apical label and the soma label to classify the rest as basal or oblique and return a dictionary that has the mapping of label to compartment type\n",
    "#but only classifies the cgal labels and not each individual face\n",
    "start_time = time.time()\n",
    "branches = classifier.classify_whole_neuron(possible_Apical,soma_index,threshold=500,axon_std_threshold=69.5)\n",
    "print(\"whole_neuron_labels = \" + str(classifier.whole_neuron_labels))\n",
    "print(\"giving neurons compartment label -- %s seconds --\" %(time.time() - start_time))\n",
    "\n",
    "\n",
    "#label the neurons according to classification\n",
    "#############NEED TO ADD STEP THAT CALCULATES THE LABELS OF THE VERTICES ##################\n",
    "start_time = time.time()\n",
    "classifier.label_whole_neuron()\n",
    "print(\"giving neuron compartment labels to faces -- %s seconds --\" %(time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "#####need to map the final_faces_labels_list to all successive numbers and get vertices\n",
    "start_time = time.time()\n",
    "output_faces_list, output_verts_list = classifier.generate_output_lists()\n",
    "print(\"giving neuron compartment labels to vertices -- %s seconds --\" %(time.time() - start_time))\n",
    "\n",
    "\n",
    "dendritic_branches = classifier.return_branches()\n",
    "\n",
    "print(f\"Total time: {time.time() - global_start }\")\n",
    "\n",
    "#output the faces_labels to a file to test\n",
    "# with open(str(segment_id) + \"_classification.csv\") as file:\n",
    "    \n",
    "#     csv_writer = csv\n",
    "    \n",
    "#             with open(labels_file) as csvfile:\n",
    "#             #print(\"inside labels file\")\n",
    "\n",
    "#             for i,row in enumerate(csv.reader(csvfile)):\n",
    "#                 triangles_labels[i] = int(row[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.labels_list\n",
    "axon_indices = [11,14]\n",
    "np.where(np.isin(classifier.labels_list,axon_indices))\n",
    "classifier.labels_list[np.where(np.isin(classifier.labels_list,axon_indices))] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the faces_labels to a file to test\n",
    "with open(str(segment_id) + \"_classification.csv\",\"w\") as csvfile:\n",
    "    \n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for i in output_faces_list:\n",
    "        csv_writer.writerow([int(i)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEED TO GET RID OF CAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the statistics of the caps:\n",
    "caps_soma = [129,131]\n",
    "caps_big_soma = [17]\n",
    "caps_keep = [98]\n",
    "regular_branches = [57,13,36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Things that will help distinguish caps: \n",
    "1) No more than neighbors\n",
    "2) Certain width\n",
    "3) certain curvature\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#gather all of the caps that we want to get rid of \n",
    "caps_soma_mesh = {}\n",
    "for i in caps_soma + caps_big_soma:\n",
    "    new_mesh = classifier.mesh.submesh(np.where(classifier.labels_list == i))[0]\n",
    "    caps_soma_mesh[i] = new_mesh\n",
    "\n",
    "caps_dendrites_mesh = {}\n",
    "for i in caps_keep + regular_branches:\n",
    "    new_mesh = classifier.mesh.submesh(np.where(classifier.labels_list == i))[0]\n",
    "    caps_dendrites_mesh[i] = new_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_soma_mesh[17].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute statistics on soma group\n",
    "print(\"Want to merge with soma\")\n",
    "for i,submesh in caps_soma_mesh.items():\n",
    "    mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "    n_faces = len(submesh.faces)\n",
    "    width_data = classifier.sdf_final_dict[i]\n",
    "    n_connections = len(classifier.connections[i])\n",
    "    \n",
    "    print(f\"mean_convex = {mean_convex}, n_faces={n_faces}, width_data={width_data}, n_connections={n_connections}\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Those that don't want to merge with soma\")\n",
    "#compute statistics on soma group\n",
    "for i,submesh in caps_dendrites_mesh.items():\n",
    "    mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(submesh)))\n",
    "    n_faces = len(submesh.faces)\n",
    "    width_data = classifier.sdf_final_dict[i]\n",
    "    n_connections = len(classifier.connections[i])\n",
    "    \n",
    "    print(f\" {i} mean_convex = {mean_convex}, n_faces={n_faces}, width_data={width_data}, n_connections={n_connections}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "things learned to help tell them apart:\n",
    "1) maximum threshold for number of connections\n",
    "2) Size threshold\n",
    "3) Number of significant connections (size of the things attached)\n",
    "4) Median width will be greater (0.240)\n",
    "\n",
    "Final rules for identifying soma caps: \n",
    "1) median width > 23\n",
    "2) n_faces < 7000\n",
    "3) n_connections < 6\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the z value of the axon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_index = 6\n",
    "\n",
    "y_max = np.max(classifier.faces[np.where(classifier.labels_list == soma_index)],axis=0)[1]\n",
    "print(y_max)\n",
    "y_min = np.min(classifier.faces[np.where(classifier.labels_list == soma_index)],axis=0)[1]\n",
    "print(y_min)\n",
    "\n",
    "#classifier.faces\n",
    "\n",
    "#mesh.submesh(np.where(classifier.labels_list == i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the Axon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_mesh = dendritic_branches[6]#std_dev_convex = np.std((trimesh.convex.adjacency_projections(axon_mesh)))\n",
    "axon_mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_indices = [83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mesh = classifier.mesh.submesh(np.where(np.isin(classifier.labels_list,axon_indices)))[0]\n",
    "new_mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the soma\n",
    "soma_indices = [6]\n",
    "soma_mesh = classifier.mesh.submesh(np.where(np.isin(classifier.labels_list,soma_indices)))[0]\n",
    "soma_mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_min = np.min(soma_mesh.vertices[:,1])\n",
    "soma_max = np.max(soma_mesh.vertices[:,1])\n",
    "print(soma_min,soma_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_min = np.min(axon_mesh.vertices[:,1])\n",
    "axon_max = np.max(axon_mesh.vertices[:,1])\n",
    "print(axon_min,axon_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_convex = abs(np.std(trimesh.convex.adjacency_projections(new_mesh)))\n",
    "if mean_convex < 69:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"results from the test\n",
    "self.soma_y_max = 222388.59375\n",
    "self.soma_y_min = 202375.6875\n",
    "soma_lower_30 = 216384.721875\n",
    "neighbor_y_max = 204029.984375\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that this object meets the standards of the axon:\n",
    "std_dev_convex = np.std((trimesh.convex.adjacency_projections(new_mesh)))\n",
    "print(std_dev_convex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the max and min of soma and axon mesh\n",
    "axon_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches[list(branches.keys())[10]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop that will do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING THE LABELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing if can distinguish axon branch from rest of the dendrites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notes on differentiation:\n",
    "1) Can't tell the axon apart based on convex methods\n",
    "2) Try sdf values\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying differentiation by curvature\n",
    "for i,current_mesh in enumerate(dendritic_branches):\n",
    "    print(\"branch \" + str(i))\n",
    "    mean_convex = np.mean(trimesh.convex.adjacency_projections(current_mesh))\n",
    "    median_convex = np.median(trimesh.convex.adjacency_projections(current_mesh))\n",
    "    #calculate the absolute values\n",
    "    mean_convex_absolute = np.mean(np.absolute((trimesh.convex.adjacency_projections(current_mesh))))\n",
    "    median_convex_absolute = np.median(np.absolute((trimesh.convex.adjacency_projections(current_mesh))))\n",
    "    std_dev_convex = np.std((trimesh.convex.adjacency_projections(current_mesh)))\n",
    "    std_dev_convex_absolute = np.std(np.absolute(trimesh.convex.adjacency_projections(current_mesh)))\n",
    "    \n",
    "    total_convex = sum(current_mesh.face_adjacency_convex)/len(current_mesh.face_adjacency_convex)\n",
    "    print(f\"mean_convex_absolute = {mean_convex_absolute}, median_convex_absolute = {median_convex_absolute}, total_convex = {total_convex}\")\n",
    "    print(f\"mean_convex = {mean_convex}, median_convex = {median_convex}, total_convex = {total_convex}\")\n",
    "    print(f\" std = {std_dev_convex}, Abs std = {std_dev_convex_absolute}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_submesh = branches[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_convex = abs(np.mean(trimesh.convex.adjacency_projections(axon_submesh)))\n",
    "mean_convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the convex options for identifying axons from cilia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#could be route for getting the indices of the mesh:\n",
    "trimesh.graph.connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches_list[4].face_adjacency_convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.absolute(trimesh.convex.adjacency_projections(branches_list[4]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the projections of faces onto all of its neighboring faces\n",
    "len(trimesh.convex.adjacency_projections(branches_list[4])),len(branches_list[4].face_adjacency),len(branches_list[4].face_adjacency_convex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_faces = [11,12]\n",
    "error_axons_indices = [3,13,14]\n",
    "soma_index = [7]\n",
    "cilia_index = [12]\n",
    "dendrite_index = [i for i in range(len(branches_list)) if i not in (little_faces + error_axons_indices + soma_index + cilia_index) ]\n",
    "dendrite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each group gets some numbers on the convex adjacency\n",
    "for i in error_axons_indices:\n",
    "    current_mesh = branches_list[i]\n",
    "    mean_convex = np.mean(trimesh.convex.adjacency_projections(current_mesh))\n",
    "    median_convex = np.median(trimesh.convex.adjacency_projections(current_mesh))\n",
    "    #calculate the absolute values\n",
    "    mean_convex_absolute = np.mean(np.absolute((trimesh.convex.adjacency_projections(current_mesh))))\n",
    "    median_convex_absolute = np.median(np.absolute((trimesh.convex.adjacency_projections(current_mesh))))\n",
    "    \n",
    "    total_convex = sum(current_mesh.face_adjacency_convex)/len(current_mesh.face_adjacency_convex)\n",
    "    print(f\"mean_convex_absolute = {mean_convex_absolute}, median_convex_absolute = {median_convex_absolute}, total_convex = {total_convex}\")\n",
    "    print(f\"mean_convex = {mean_convex}, median_convex = {median_convex}, total_convex = {total_convex}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each group gets some numbers on the convex adjacency\n",
    "for i in cilia_index:\n",
    "    current_mesh = branches_list[i]\n",
    "    mean_convex = np.mean(trimesh.convex.adjacency_projections(current_mesh))\n",
    "    median_convex = np.median(trimesh.convex.adjacency_projections(current_mesh))\n",
    "    #calculate the absolute values\n",
    "    mean_convex_absolute = np.mean(np.absolute((trimesh.convex.adjacency_projections(current_mesh))))\n",
    "    median_convex_absolute = np.median(np.absolute((trimesh.convex.adjacency_projections(current_mesh))))\n",
    "    \n",
    "    total_convex = sum(current_mesh.face_adjacency_convex)/len(current_mesh.face_adjacency_convex)\n",
    "    print(f\"mean_convex_absolute = {mean_convex_absolute}, median_convex_absolute = {median_convex_absolute}, total_convex = {total_convex}\")\n",
    "    print(f\"mean_convex = {mean_convex}, median_convex = {median_convex}, total_convex = {total_convex}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each group gets some numbers on the convex adjacency\n",
    "for i in dendrite_index:\n",
    "    current_mesh = branches_list[i]\n",
    "    mean_convex = np.mean(trimesh.convex.adjacency_projections(current_mesh))\n",
    "    median_convex = np.median(trimesh.convex.adjacency_projections(current_mesh))\n",
    "    #calculate the absolute values\n",
    "    mean_convex_absolute = np.mean(np.absolute((trimesh.convex.adjacency_projections(current_mesh))))\n",
    "    median_convex_absolute = np.median(np.absolute((trimesh.convex.adjacency_projections(current_mesh))))\n",
    "    \n",
    "    total_convex = sum(current_mesh.face_adjacency_convex)/len(current_mesh.face_adjacency_convex)\n",
    "    print(f\"mean_convex_absolute = {mean_convex_absolute}, median_convex_absolute = {median_convex_absolute}, total_convex = {total_convex}\")\n",
    "    print(f\"mean_convex = {mean_convex}, median_convex = {median_convex}, total_convex = {total_convex}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(classifier.final_faces_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_faces_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_labels = classifier.labels_list\n",
    "verts_labels = np.zeros(len(classifier.verts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #write the neuron to the labels table:\n",
    "\n",
    "\n",
    "# insert_key = dict(segment_id=segment_id,segmentation=2,decimation_ratio=0.35,\n",
    "#                   clusters=classifier.clusters,\n",
    "#                   smoothness=classifier.smoothness,date_time='2019-04-25 19:56:12',\n",
    "#                  vertices = np.zeros(len(classifier.verts)),\n",
    "#                  triangles = classifier.labels_list)\n",
    "\n",
    "\n",
    "# ta3p100.AnnotationRevisited.insert1(insert_key,skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class AnnotationRevisited(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    # creates the labels for the mesh table\n",
    "    -> ta3p100.ComponentAutoSegmentWhole\n",
    "    date_time  : timestamp   #going to keep constant\n",
    "    ---\n",
    "    vertices   : longblob     # label data for the vertices\n",
    "    triangles  : longblob     # label data for the faces\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
